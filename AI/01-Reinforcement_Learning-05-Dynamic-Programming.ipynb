{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Intro to Dynamic Programming and Iterative Policy Evaluation\n",
    "We are now going to start looking at solutions to MDP's. As we saw in the last section, the center piece of the discussion is the **Bellman Equation**:\n",
    "\n",
    "#### <span style=\"color:#0000cc\">$$\\text{Bellman Equation} \\rightarrow V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$</span>\n",
    "\n",
    "In fact, the bellman equation can be used directly to solve for the value function. If you look carefully, you will see that this is actually a set of $S$ equations with $S$ unknowns. In fact, it is a linear equation, meaning it is not too difficult to solve. In addtion, a lot of the matrix entries will be zero, since the state transitions will most likely be sparse. \n",
    "\n",
    "However, this is _not_ the approach we will take. Instead, we will do what is called **iterative policy evaluation**. \n",
    "\n",
    "## 1.1 Iterative Policy Evaluation\n",
    "What exactly is iterative policy evaluation? Well, essentially it means that we apply the bellman equation again and again, and eventually it will just converge. We can write down the algorithm in pseudocode as follows:\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{def iterative_policy_evaluation}(\\pi)\\text{:} \\\\\n",
    "\\hspace{1cm} \\text{initialize V(s) = 0 for all s} \\in \\text{S} \\\\\n",
    "\\hspace{1cm} \\text{while True:} \\\\\n",
    "\\hspace{2cm} \\Delta = 0 \\\\\n",
    "\\hspace{2cm} \\text{for each s} \\in \\text{S:} \\\\\n",
    "\\hspace{3cm} \\text{old_v = V(s)} \\\\\n",
    "\\hspace{3cm} V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}\\\\\n",
    "\\hspace{3cm} \\Delta = \\text{max(} \\Delta \\text{, |V(s) - old_v|)} \\\\\n",
    "\\hspace{2cm} \\text{if} \\Delta \\text{< threshold: break} \\\\\n",
    "\\hspace{1cm} \\text{return V(s)}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "We can see above that the input to iterative policy evaluation is a policy, $\\pi$, and the output is the value function for that particular policy. It works as follows:\n",
    "> * We start by initializing $V(s)$ to 0 for all states. \n",
    "* Then, in an infinite loop, we initialize a variable called $\\Delta$, which represents the maximum change during the current iteration. $\\Delta$ is used to determine when to quit. When it is small enough, that is when we will break out of the loop. \n",
    "* Then, for every state in the state space, we keep a copy of the old $V(s)$, and then we use bellmans equation to update V(s). \n",
    "* We set $\\Delta$ to be the maximum change for $V(s)$ in that iteration. \n",
    "* Once this converges, we return $V(s)$\n",
    "\n",
    "The main point of interest in this algorithm, is of course the part that contains the bellman equation. Notice how strictly speaking, the value at iteration $k+1$ depend only on the values at iteration $k$:\n",
    "\n",
    "$$ V_{k+1}(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_{k}(s') \\Big \\}$$\n",
    "\n",
    "However, this need not be the case. In fact, we can always just use our most up to date versions of the value function for any state. This actually ends up converging faster. \n",
    "\n",
    "## 1.2 Definitions\n",
    "A final note; we generally call the act of finding the value function for a given policy the _**prediction problem**_. Soon, we will learn an algorithm for finding the optimal policy, which is known as the _**control problem**_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Gridworld in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Environment\"\"\"\n",
    "class Grid: \n",
    "  def __init__(self, width, height, start):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.i = start[0] # start is a tuple of 2 integers\n",
    "    self.j = start[1]\n",
    "    \n",
    "  def set(self, rewards, actions):\n",
    "    \"\"\"actions enumerate all possible actions that can take you to new state.\n",
    "       actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "       rewards should be a dict of: (i, j): r (row, col): reward\n",
    "    \"\"\"\n",
    "    self.rewards = rewards\n",
    "    self.actions = actions\n",
    "    \n",
    "  def set_state(self, s):\n",
    "    \"\"\"Useful for various algorithms we will use. For example, iterative policy evaluation\n",
    "    requires looping through all the states, and then doing an action to get to the next\n",
    "    state. In order to know what the next state is, we have to put the agent into that \n",
    "    state, do the action, and then determine the next state. We do not automatically have\n",
    "    a master list of state transitions, we must figure them out by playing the game.\"\"\"\n",
    "    self.i = s[0]\n",
    "    self.j = s[1]\n",
    "    \n",
    "  def current_state(self):\n",
    "    \"Returns current (i,j) position of agent.\"\n",
    "    return (self.i, self.j)\n",
    "  \n",
    "  def is_terminal(self, s):\n",
    "    \"\"\"Returns true if s is terminal state, false if not. Easy way to check this is to see\n",
    "    if the state is in the action dictionary. If you can do an action from the state, then\n",
    "    you can transition to a different state, and hence your state is not terminal.\"\"\"\n",
    "    return s not in self.actions\n",
    "  \n",
    "  def move(self, action):\n",
    "    \"\"\"Checks if action is in actions dictionary. If not, we are not able to do this move,\n",
    "    so we simply stay in same position.\"\"\"\n",
    "    if action in self.actions[(self.i, self.j)]:\n",
    "      if action == 'U':\n",
    "        self.i -= 1\n",
    "      elif action == 'D':\n",
    "        self.i += 1\n",
    "      elif action == 'R':\n",
    "        self.j += 1\n",
    "      elif action == 'L':\n",
    "        self.j -= 1\n",
    "    # Return reward (if any, default is 0)\n",
    "    return self.rewards.get((self.i, self.j), 0)\n",
    "  \n",
    "  def undo_move(self, action):\n",
    "    \"\"\"Pass in the action you just took, and the environment will undo it. \n",
    "    Ex -> Just went up, it will move you back down.\"\"\"\n",
    "    if action == 'U':\n",
    "      self.i += 1\n",
    "    elif action == 'D':\n",
    "      self.i -= 1\n",
    "    elif action == 'R':\n",
    "      self.j -= 1\n",
    "    elif action == 'L':\n",
    "      self.j += 1\n",
    "    # Raise an exception if we arrive somewhere we shouldn't be -> Should never happen\n",
    "    assert(self.current_state() in self.all_states())\n",
    "    \n",
    "  def game_over(self):\n",
    "    \"Returns true if game over, false otherwise. Only need to check if in terminal state.\"\n",
    "    return (self.i, self.j) not in self.actions\n",
    "  \n",
    "  def all_states(self):\n",
    "    \"\"\"We can calculate all of the states simply by enumerating all of the states from \n",
    "    which we can take an action (which don't include terminal states), and all of the \n",
    "    states that return a reward (which do include terminal states). Since there may be\n",
    "    some of the same states in both actions and rewards, we cast it to a set. This also\n",
    "    makes the data structure O(1) for search.\"\"\"\n",
    "    return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "  \n",
    "def standard_grid():\n",
    "  \"\"\"Returns standard grid. This is a grid that has the structure shown in section 4.\n",
    "  All of the actions are defined such that we can move within the grid, but never off\n",
    "  of it. We also cannot walk into the wall, nor out of the terminal state. Upper left\n",
    "  is defined to be (0,0). We define rewards for arriving at each state. The grid looks\n",
    "  like:\n",
    "      .  .  .  1\n",
    "      .  x  . -1\n",
    "      s  .  .  .\n",
    "  * x means you can't go there\n",
    "  * s means start position\n",
    "  * number means reward at that state\n",
    "  \"\"\"\n",
    "  g = Grid(3, 4, (2, 0))\n",
    "  rewards = {(0, 3): 1, (1, 3): -1}\n",
    "  actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "  }\n",
    "  g.set(rewards, actions)\n",
    "  return g\n",
    "\n",
    "def negative_grid(step_cost=-0.1):\n",
    "  \"\"\"Here we want to penalize each move. This is done to prevent a robot from moving \n",
    "  randomly to solve the maze. If you only gave it a reward for solving the maze, then it\n",
    "  would never learn anything beyond a random strategy. We know that we can incentivize\n",
    "  the robot to solve the maze more efficiently by giving it a negative reward for each\n",
    "  step taken. That is what we are doing here-incentivizing the robot to solve the maze \n",
    "  efficiently, rather than moving randomly until it reaches the goal.\"\"\"\n",
    "  g = standard_grid()\n",
    "  g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "  })\n",
    "  return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Iterative Policy Evaluation In Code\n",
    "We are now going to implement iterative policy evaluation in code. To demonstrate how we can find the value function for different policies, we are going to do iterative policy evaluation on two different policies. \n",
    "\n",
    "> The first policy we will look at is a completely random (_uniform_) policy. \n",
    "\n",
    "Remember, there are two probability distributions involved in bellmans equation:\n",
    "\n",
    "$$\\text{Policy probability: }\\hspace{1cm}\\pi(a \\mid s)$$\n",
    "\n",
    "\n",
    "$$\\text{Markov State Transition Probability: }\\hspace{1cm}p(s',r \\mid s,a)$$\n",
    "\n",
    "The probability that is relevant for implementing bellmans equation is $\\pi(a \\mid s)$. This is the probability that we take an action $a$, given that we are in state $s$. For a _uniform random_ policy, this probability will be:\n",
    "\n",
    "$$\\frac{1}{\\mid A(s) \\mid}$$\n",
    "\n",
    "Where $A(s)$ is the set of all possible actions to take from state $s$. In other words, our probability will be 1 divided by the total number of possible actions from state $s$. \n",
    "\n",
    "The other probability, $p(s', r \\mid s, a)$ is only relevant when state transitions themselves are random. That is a scenario when you try to move left, but instead you end up going right. \n",
    "\n",
    "> The second policy we will look at is a completely deterministic policy. \n",
    "\n",
    "From the start position you go directly to the goal state (up, up, right, right, right). However, if you are starting from any other state, the policy is to go directly to the losing state. So, we should expect the values on the upper left path to be positive, and the other values to be negative. \n",
    "\n",
    "Also, as a note/clarification-when performing iterative policy evaluation with random actions, our final value function will differ from the bellman equation as follows; the original bellman equation starts off as:\n",
    "\n",
    "#### <span style=\"color:#0000cc\">$$\\text{Bellman Equation} \\rightarrow V_\\pi(s) = \\sum_a \\pi(a \\mid s) * \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$</span>\n",
    "\n",
    "We can drop the $p(s', r \\mid s, a)$, since as we stated earlier, our state transitions are not currently random. That means our value function looks like:\n",
    "\n",
    "$$V_\\pi(s) = \\sum_a \\pi(a \\mid s) *  \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$\n",
    "\n",
    "Because $\\pi(a \\mid s)$ is a uniform random distribution, it is _not_ dependent on the state and is actually equal to: \n",
    "\n",
    "$$\\pi(a) = \\frac{1}{\\mid A(s) \\mid}$$\n",
    "\n",
    "Hence, we can update our value equation to be:\n",
    "\n",
    "$$V_\\pi(s) = \\sum_a \\frac{1}{\\mid A(s) \\mid} *  \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$\n",
    "\n",
    "In pseudocode, that will look like:\n",
    "\n",
    "```\n",
    "new_v += p_a * (r + gamma * V[grid.current_state()]) \n",
    "```\n",
    "\n",
    "**Key Takeaway**:<br>\n",
    "> Another key thing to keep in mind, is how our value function is actually represented. Generally, when we think of a _function_ we envision an equation of some sort, that maps one domain to another, such as: <br>\n",
    "<br>\n",
    "$$f(x) = x^2 + 4x + 8 $$\n",
    "<br>\n",
    "However, a function can be more generally defined as: _**A rule that relates inputs to outputs in a dataset or system.**_ This allows us to have an alternative way of viewing functions: _As the actual set of input/output pairs it defines, or in other words, as a dataset._ This is the way in which our value function is defined. As we converge on a final value function, we are not determining some final equation, but rather a final mapping of our input domain (our 11 states) to our output domain, the value associated with each state. \n",
    "\n",
    "So remember:\n",
    "> _We can view an equation equivalently in two ways: either via its mathematical expression or as a dataset consisting of a complete listing of the function's input/output pairs._\n",
    "\n",
    "We can now get to the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values for uniformly random actions:\n",
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n",
      "\n",
      "\n",
      "\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "values for fixed policy:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 10e-4 # Threshold for convergence\n",
    "\n",
    "\"\"\"Functions to help us visualize policies and values.\"\"\"\n",
    "def print_values(V, g): \n",
    "  \"\"\"Takes in values dictionary and grid, draws grid, and in each position it prints\n",
    "  the value. \"\"\"\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      v = V.get((i,j), 0)\n",
    "      if v >= 0:\n",
    "        print(\" %.2f|\" % v, end=\"\")\n",
    "      else:\n",
    "        print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "    print(\"\")\n",
    "    \n",
    "def print_policy(P, g):\n",
    "  \"\"\"Takes in policy and grid, draws grid, and in each position it prints\n",
    "  the action from the policy. Note, this will only work for deterministic policies, \n",
    "  since we can't print more than 1 thing per location.\"\"\"\n",
    "  for i in range(g.width):\n",
    "    print(\"---------------------------\")\n",
    "    for j in range(g.height):\n",
    "      a = P.get((i, j), ' ')\n",
    "      print(\"  %s  |\" % a, end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \"\"\"Iterative Policy Evaluation:\n",
    "      * Given a Policy -> find its value function V(s)\n",
    "      * We will do this for both uniform random policy and fixed deterministic policy\n",
    "      * NOTE: There are 2 sources of randomness\n",
    "      * p(a|s)      -> deciding what action to take given the state\n",
    "      * p(s',r|s,a) -> the next state and reward given your action-state pair\n",
    "      * we are only modeling p(a|s) = uniform\"\"\"\n",
    "  grid = standard_grid()\n",
    "  \n",
    "  # States will be positions (i, j). Simpler than tic-tac-toe, because we only have \n",
    "  # 1 game piece that can only be at one position at a time. We get the set of all states\n",
    "  # from the grid, since these will be the keys to the value function dictionary.\n",
    "  states = grid.all_states()\n",
    "  \n",
    "  # ----- Perform Iterative Policy Evaluation for Uniform Random Actions -----\n",
    "  \n",
    "  # Initialize all V(s) to be 0\n",
    "  V = {v: 0 for v in states}\n",
    "  gamma = 1.0 # Discount factor\n",
    "  \n",
    "  # Enter infinite loop, repeat until convergence\n",
    "  while True:\n",
    "    biggest_change = 0\n",
    "    # Loop through all of the states, since we need to find Value function at all states\n",
    "    for s in states: \n",
    "      old_v = V[s] # Keep copy of old V(s), so we can track magnitude of each change\n",
    "      \n",
    "      # Loop through all possible actions from this state. V(s) only has value if it is \n",
    "      # not a terminal state.\n",
    "      if s in grid.actions:\n",
    "        new_v = 0 # we will accumulate the answer\n",
    "        p_a = 1.0 / len(grid.actions[s]) # Each action has equal probability\n",
    "        for a in grid.actions[s]:\n",
    "          # Look ahead action comes into play. Must first set state to s, and then do the\n",
    "          # action, so that we can determine the next state s', since that is required\n",
    "          # to use the bellman equation\n",
    "          grid.set_state(s) \n",
    "          r = grid.move(a)\n",
    "          new_v += p_a * (r + gamma * V[grid.current_state()]) # This is RL portion!\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "      \n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for uniformly random actions:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"\\n\\n\")\n",
    "    \n",
    "  # ----- Perform Iterative Policy Evaluation for Fixed Policy -----\n",
    "  # Print policy so we know what it looks like\n",
    "  policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "  }\n",
    "  print_policy(policy, grid)\n",
    "  \n",
    "  # Reinitialize all V(s) to be 0\n",
    "  V = {v: 0 for v in states}\n",
    "  \n",
    "  # Let's see how V(s) changes as we get further away from the reward\n",
    "  gamma = 0.9 # discount factor\n",
    "  \n",
    "  # Repeat until convergence. Simpler loop, we don't need to loop through any actions, \n",
    "  # because there is only one action per state. Because there is only 1 action per state,\n",
    "  # the probability of that action in that state is 1. \n",
    "  while True:\n",
    "    biggest_change = 0 \n",
    "    for s in states: \n",
    "      old_v = V[s]\n",
    "      \n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        a = policy[s]\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        V[s] = r + gamma * V[grid.current_state()]\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break\n",
    "  print(\"values for fixed policy:\")\n",
    "  print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we look at the results, we can see that most of the value for the uniform random policy are negative. That is because if you are moving randomly, there is a good chance you end up in the losing state. Remember, there are two ways you can end up in the losing state, but only one way you can enter the goal state. The most negative value is for the state right underneath the losing state, which makes sense. \n",
    "\n",
    "Now, for the fixed policy and discount factor 0.9, we see exactly what we expect to see: the further away we get from the terminal state, the more the value decreases, and each time it decreases by exactly 10%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Policy Improvement\n",
    "We can now start discussing the _control problem_, the problem of how to find better policies, eventually leading to the optimal policy. What we know so far is how to find the value function given a fixed policy. Let's look at our recursive definition of $Q_\\pi(s,a)$ again:\n",
    "\n",
    "$$Q_{\\pi}(s,a) = \\sum_{s',r}p(s',r \\mid s,a)\\big[r + \\gamma V_\\pi (s')\\big]$$\n",
    "\n",
    "This tells us the value of doing action $a$, while in state $s$, using the policy $\\pi$. Using the current policy, we simply get the current state value function. \n",
    "\n",
    "$$V_\\pi(s) = Q_\\pi(s, \\pi(s)) = \\sum_{s',r}p(s',r \\mid s,\\pi(s))\\big[r + \\gamma V_\\pi (s')\\big] $$\n",
    "\n",
    "Now let's say that we want to change just one of the actions in the policy; Can we do this? Of course we can! We have a finite set of actions, so all we need to do is just go through each one until we get a better Q, than $\\pi(s)$ does:\n",
    "\n",
    "$$find \\; a \\in A \\; s.t. Q_\\pi(s,a) > Q_\\pi(s, \\pi(s)) $$\n",
    "\n",
    "This is where doing all of the programming exercises we go through becomes _very useful_. This looks like a really abstract equation, but all it's saying is: \n",
    "> _If the policy is currently to go up, let's look at left, right, and down to see if we can get a bigger Q. If it does, let's change our policy for that state to this new action._\n",
    "\n",
    "Formally speaking, what we are doing when we choose a new action for the state is finding a new policy $\\pi'$, that has a bigger value for the state than $\\pi$:\n",
    "\n",
    "$$V_\\pi(s) \\leq V_{\\pi'}(s)$$\n",
    "\n",
    "All we need to do this is to pick an action that gives us maximum $Q$. We can write this in the form where we are using $Q$:\n",
    "\n",
    "$$\\pi'(s) = argmax_a\\big(Q_\\pi(s,a)\\big)$$\n",
    "\n",
    "Or in the form where we are doing a lookahead search on $V$:\n",
    "\n",
    "$$\\pi'(s) = argmax_a \\big(\\sum_{s', r} p(s', r \\mid s,a) \\big[r + \\gamma V_\\pi(s')\\big]\\big)$$\n",
    "\n",
    "## 4.1 Things to notice\n",
    "There are a few things about policy improvement that you should notice:\n",
    "\n",
    "1. First, notice that it is _greedy_. We are never considering globally the value function at all states. We are only looking at the current state, and picking the best action based on the value function at that state. \n",
    "\n",
    "2. Second, notice how it uses an imperfect version of $V_\\pi(s)$. Once we change $\\pi$, $V_\\pi(s)$ also changes. We will see soon why this is not a problem. \n",
    "\n",
    "One question that may arise is: How do we know when we are finished trying to change the policy? When we have found the optimal policy, the policy will not change with respect to the value function. In addition, the value function will no longer improve-it will stay constant. Notice how the inequality we had earlier was less than or equal to:\n",
    "\n",
    "$$V_\\pi(s) \\leq V_{\\pi'}(s)$$\n",
    "\n",
    "In the case where it is less than, we are still improving. In the case where it is equal to, then we have found the optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Policy Iteration\n",
    "We are now going to discuss the algorithm we are going to use to find the optimal policy. It will solve the problem we encountered in the previous lecture, specifically, when we change the policy the value function also changes and becomes out of date. We will now see how we can rectify that problem.\n",
    "\n",
    "So, what do we do when we change the policy and the value funcion becomes out of date? Well, we can simply recalculate the value function. Luckily we already know how to do find $V$ given $\\pi$! We have written the code already, and the algorithm is called _iterative policy evalutation_. At a high level this algorithm is very simple: We just alternate between _policy evaluation_ and _policy improvement_. We keep doing this until the policy doesn't change. Note that what we are _not_ checking for is the value function converging (although it will anyway because once the policy stops changing we only need to do one more iteration of policy evaluation). \n",
    "\n",
    "In pseudcode **policy iteration** looks like:\n",
    "\n",
    "```\n",
    "Step 1. Randomly initialize V(s) and policy pi(s)\n",
    "\n",
    "Step 2. V(s) = iterative_policy_evaluation(pi)\n",
    "\n",
    "Step 3. Policy Improvement\n",
    "policy_changed = False\n",
    "for s in all_states:      # Loop through all states\n",
    "  old_a = policy(s)\n",
    "  policy(s) = argmax[a] { sum[s', r] { p(s',r | s,a) [r + gamma*V(s')] } }\n",
    "  if policy(s) != old_a:\n",
    "    policy_changed = True\n",
    "if policy_changed:\n",
    "  go back to step 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Policy Iteration in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: \n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "Initial policy:\n",
      "---------------------------\n",
      "  U  |  R  |  D  |     |\n",
      "---------------------------\n",
      "  D  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  U  |  D  |  L  |\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# Note: This is deterministic -> All p(s',r|s,a) = 1 or 0. In other words, when you try \n",
    "# to go up, you go up. The transition probability is always 0 or 1.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Negative grid gives reward of -0.1 for every non-terminal state\n",
    "  # We will use this to see if it encourages finding a shorter path to the goal\n",
    "  grid = negative_grid()\n",
    "  \n",
    "  # Print Rewards\n",
    "  print(\"Rewards: \")\n",
    "  print_values(grid.rewards, grid)\n",
    "  \n",
    "  # Create a deterministic random policy. We will randomly chose an action out of the set \n",
    "  # of possible actions for every state. \n",
    "  policy = {key: np.random.choice(ALL_POSSIBLE_ACTIONS) for key in grid.actions.keys()}\n",
    "  \n",
    "  # Initial policy\n",
    "  print(\"Initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "  \n",
    "  # Initialize V(s) randomly -> terminal state initialized to 0\n",
    "  V = {s: np.random.random() if s in grid.actions else 0 for s in states}\n",
    "  \n",
    "  # Repeat until convergence -> will break out when policy does not change. Alternates\n",
    "  # between policy evaluation and policy improvement\n",
    "  while True:\n",
    "    \n",
    "    # ----- Policy evaluation step -----\n",
    "    # Same as before, but simpler version because all of actions, next states, and rewards\n",
    "    # are all deterministic\n",
    "    while True:\n",
    "      biggest_change = 0\n",
    "      for s in states:\n",
    "        old_v = V[s]\n",
    "        \n",
    "        # V(s) only has a value if it's not a terminal state\n",
    "        if s in policy: \n",
    "          a = policy[s]\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          V[s] = r + GAMMA * V[grid.current_state()]\n",
    "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "          \n",
    "      if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "    \n",
    "    # ----- Policy Improvement step -----\n",
    "    is_policy_converged = True\n",
    "    \n",
    "    # Loop through all states\n",
    "    for s in states:\n",
    "      if s in policy:\n",
    "        old_a = policy[s] # Grab existing action and assign to old_a\n",
    "        new_a = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # Loop through all possible actions to find the best lookahead value\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > best_value: \n",
    "            best_value = v\n",
    "            new_a = a # Choose action that gives us best lookahead value\n",
    "        policy[s] = new_a\n",
    "        if new_a != old_a:\n",
    "          is_policy_converged = False # If actions in policy change, we aren't converged\n",
    "    \n",
    "    if is_policy_converged:\n",
    "      break\n",
    "      \n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how if we are in the position below the wall the agent will choose to go right, instead of left and then up around the top left corner. This is because each step has a negative reward associated with it, and future rewards are discounted, so we want to try and get to the goal state as quickly as possible. \n",
    "\n",
    "Again, we should keep in mind the current form of the bellman equation we are utilizing:\n",
    "\n",
    "$$V_\\pi(s) = \\sum_a \\pi(s \\mid a) *  \\Big \\{ r + \\gamma V_\\pi(s') \\Big \\}$$\n",
    "\n",
    "Where are missing the transition probability term:\n",
    "\n",
    "$$p(s',r \\mid s,a)$$\n",
    "\n",
    "Because we have created a deterministic system where if you are at position (0,0) and you try and go right you will. That probability is specifically saying:\n",
    "\n",
    "> _Given you take the action to move the right and you are at position (0,0), what is the probability you get to state s' and have reward r?_\n",
    "\n",
    "Well, because we have created a deterministic system, the probability is just 1, and we can drop that term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Policy Iteration in Windy Grid World\n",
    "We are now going to go over a modification of our environment called _windy grid world_. Recall that we have two probability distributions to deal with: \n",
    "> 1. Probability of doing an action $a$ when in state $s$: \n",
    "$$\\pi(a \\mid s)$$\n",
    "2. State Transition Probability: \n",
    "$$p(s',r \\mid s,a)$$\n",
    "\n",
    "So far our state transition probability has been deterministic! When we do an action $a$ in state $s$ we always end up in state $s'$. This is also tied to the reward, since gridworld gives you a reward based on the state that you landed in. We will now consider the case where this probability distribution is _not_ deterministic. In other words, the probability will have a value between 0 and 1. \n",
    "\n",
    "## 7.1 Windy Gridworld\n",
    "Imaginge you are walking along a windy street. You try to walk straight, but the wind pushes you to the side or backwards. This is what happens in windy gridworld. If the agent tries to go up, it will do so with probability 0.5. But it can also go left, down, or right with probability 0.5/3. \n",
    "\n",
    "Let's dig into the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards:\n",
      "---------------------------\n",
      "-1.00|-1.00|-1.00| 1.00|\n",
      "---------------------------\n",
      "-1.00| 0.00|-1.00|-1.00|\n",
      "---------------------------\n",
      "-1.00|-1.00|-1.00|-1.00|\n",
      "values:\n",
      "---------------------------\n",
      "-4.52|-2.95|-0.86| 0.00|\n",
      "---------------------------\n",
      "-5.57| 0.00|-1.94| 0.00|\n",
      "---------------------------\n",
      "-5.76|-4.88|-3.44|-2.17|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  R  |  R  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Next state and reward will now have some randomness. The agent will go in the desired\n",
    "direction with probability 0.5. The agent will go in a random direction with probability\n",
    "0.5/3\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \"\"\"Change of step_cost. It was -0.1, now it is -1.0. Because the goal only gives +1 \n",
    "  reward, and the losing state gives -1 reward, the agent is going to want to end the \n",
    "  game as soon as possible, even if this means ending up in the losing state. For example,\n",
    "  if the agent is 3 steps away from the goal, they will get -3 reward before they can even\n",
    "  reach the goal. But, if they are only 1 step away from the losing state, then they only\n",
    "  get -1 reward. Remember, the goal of the agent is not to get to what we have defined as \n",
    "  the winning or losing states; all it does is try and maximize its reward. \"\"\"\n",
    "  grid = negative_grid(step_cost=-1.0)\n",
    "  \n",
    "  # print rewards\n",
    "  print(\"rewards:\")\n",
    "  print_values(grid.rewards, grid)\n",
    "  \n",
    "  # Create a deterministic random policy. We will randomly chose an action out of the set \n",
    "  # of possible actions for every state. \n",
    "  policy = {key: np.random.choice(ALL_POSSIBLE_ACTIONS) for key in grid.actions.keys()}\n",
    "  \n",
    "  # Initialize V(s) randomly -> terminal state initialized to 0\n",
    "  V = {s: np.random.random() if s in grid.actions else 0 for s in states}\n",
    "  \n",
    "  # Repeat until convergence - Will break when policy does not change\n",
    "  while True:\n",
    "    \n",
    "    # ----- Policy Evaluation step -----\n",
    "    while True:\n",
    "      biggest_change = 0\n",
    "      for s in states:\n",
    "        old_v = V[s]\n",
    "        \n",
    "        # V(s) only has value if it's not a terminal state\n",
    "        new_v = 0\n",
    "        if s in policy:\n",
    "          for a in ALL_POSSIBLE_ACTIONS: \n",
    "            # Adding in state transition probability. Policy itself is deterministic.\n",
    "            # There are two probabilities we can play with, but we are only playing with the \n",
    "            # state transitions this time. \n",
    "            if a == policy[s]:             \n",
    "              p = 0.5\n",
    "            else:\n",
    "              p = 0.5 / 3\n",
    "            grid.set_state(s)\n",
    "            r = grid.move(a)\n",
    "            new_v += p*(r + GAMMA * V[grid.current_state()])\n",
    "          V[s] = new_v\n",
    "          biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "          \n",
    "      if biggest_change < SMALL_ENOUGH:\n",
    "        break\n",
    "        \n",
    "    # ----- Policy Improvement step -----\n",
    "    is_policy_converged = True\n",
    "    for s in states:\n",
    "      if s in policy:\n",
    "        old_a = policy[s]\n",
    "        new_a = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS: # chosen action\n",
    "          v = 0\n",
    "          for a2 in ALL_POSSIBLE_ACTIONS: # resulting action\n",
    "            if a == a2:\n",
    "              p = 0.5\n",
    "            else:\n",
    "              p = 0.5/3\n",
    "            grid.set_state(s)\n",
    "            r = grid.move(a2)\n",
    "            v += p*(r + GAMMA * V[grid.current_state()])\n",
    "          if v > best_value:\n",
    "            best_value = v\n",
    "            new_a = a\n",
    "        policy[s] = new_a\n",
    "        if new_a != old_a:\n",
    "          is_policy_converged = False\n",
    "\n",
    "    if is_policy_converged:\n",
    "      break\n",
    "      \n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)\n",
    "  # result: every move is as bad as losing, so lose as quickly as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 8. Value Iteration\n",
    "We are now going to talk about an alternative technique to solve the _**control problem**_. This new technique is called **value iteration**. Recall that the previous technique that we just went over was called _policy iteration_. It is easy to get all of these confused, because all of these look and sound the same. So, coding these will definitely help getting used to the variety of solutions that we have for MDP's. \n",
    "\n",
    "One of the disadvantages of policy iteration is that it is an iterative algorithm, and hence we have to wait for it to converge. At the same time the main loop contains two steps, and the first step itself is also an iterative algorithm. This leaves us with one iterative algorithm inside of another. The question arises: is there a more efficient way to go about solving this problem? \n",
    "\n",
    "Recall that the policy evaluation algorithm ends when $V$ converges. It is reasonable to ask: Is there a point before $V$ converges, such that the resulting greedy policy wouldn't change? In fact, studies have shown that after only a few iterations of policy evaluation the resulting greedy policy is constant. What this tells us is that we don't actually need to wait for _policy evaluation_ to converge when we are doing _policy iteration_. We can just do a few steps and then break, because the policy improvement step would find the same policy given additional iterations of policy evaluation. \n",
    "\n",
    "## 8.1 Value Iteration Algorithm\n",
    "However, the algorithm that we are studying in this lecture takes this one step further. It is called **value iteration**. What is does is combines the policy evaluation and policy improvement into one step:\n",
    "\n",
    "$$V_{k+1}(s) = max_a \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\big\\{ r + \\gamma V_k(s') \\big\\}$$ \n",
    "\n",
    "What is interesting about this is that it looks almost exactly like the equations that we have already seen. In fact, if you weren't paying attention it would look identical. So this equation is very similar to the policy evaluation equation, except we are taking the max over all possible actions. \n",
    "\n",
    "It is also iterative, as we can see via the k index. Similar to policy evaluation, we don't actually need to wait for the entire kth iteration of $V$ to finish before calculating the (k+1)th iteration of $V$. Notice how this does both policy evaluation and policy improvement in one step. This is because _policy improvement_ occurs by taking the _argmax_ of the expression on the right. So, by just taking the _max_, we are doing the policy evaluation step on the policy we would have chosen had we taken the argmax. \n",
    "\n",
    "## 8.2 Value Iteration Pseudocode\n",
    "We can now write this out in pseudocode:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{initialize V(s) = 0 for all s}\\in\\text{S} \\\\\n",
    "\\text{while True:} \\\\\n",
    "\\hspace{2cm} \\Delta = 0 \\\\\n",
    "\\hspace{2cm} \\text{for each s} \\in \\text{S:} \\\\\n",
    "\\hspace{3cm} \\text{old_v = V(s)} \\\\\n",
    "\\hspace{3cm} V(s) = max_a \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\big\\{ r + \\gamma V(s') \\big\\}\\\\\n",
    "\\hspace{3cm} \\Delta = \\text{max(} \\Delta \\text{, |V(s) - old_v|)} \\\\\n",
    "\\hspace{2cm} \\text{if} \\Delta \\text{< threshold: break} \\\\\n",
    "\\text{for each s} \\in \\text{S:} \\\\\n",
    "\\hspace{2cm} \\pi(s) = argmax_A \\sum_{s'}\\sum_r p(s',r \\mid s,a) \\big\\{ r + \\gamma V(s') \\big\\}\n",
    "$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 9. Value Iteration in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: \n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "Initial policy:\n",
      "---------------------------\n",
      "  U  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  D  |     |  D  |     |\n",
      "---------------------------\n",
      "  L  |  L  |  D  |  L  |\n",
      "values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# Note: This is deterministic -> All p(s',r|s,a) = 1 or 0. In other words, when you try \n",
    "# to go up, you go up. The transition probability is always 0 or 1.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Negative grid gives reward of -0.1 for every non-terminal state\n",
    "  # We will use this to see if it encourages finding a shorter path to the goal\n",
    "  grid = negative_grid()\n",
    "  \n",
    "  # Print Rewards\n",
    "  print(\"Rewards: \")\n",
    "  print_values(grid.rewards, grid)\n",
    "  \n",
    "  # Create a deterministic random policy. We will randomly chose an action out of the set \n",
    "  # of possible actions for every state. \n",
    "  policy = {key: np.random.choice(ALL_POSSIBLE_ACTIONS) for key in grid.actions.keys()}\n",
    "  \n",
    "  # Initial policy\n",
    "  print(\"Initial policy:\")\n",
    "  print_policy(policy, grid)\n",
    "  \n",
    "  # Initialize V(s) randomly -> terminal state initialized to 0\n",
    "  V = {s: np.random.random() if s in grid.actions else 0 for s in states}\n",
    "  \n",
    "  # Repeat until convergence. \n",
    "  # ---- Value iteration Loop. ----\n",
    "  # Looks very similar to policy evaluation, except now we are looping through all \n",
    "  # actions, and taking the maximum value. We break when the biggest change is below the\n",
    "  # SMALL_ENOUGH threshold\n",
    "  while True:\n",
    "    biggest_change = 0 \n",
    "    for s in states:\n",
    "      old_v = V[s]\n",
    "      \n",
    "      # V(s) only has value if it's not a terminal state\n",
    "      if s in policy:\n",
    "        new_v = float('-inf')\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "          grid.set_state(s)\n",
    "          r = grid.move(a)\n",
    "          v = r + GAMMA * V[grid.current_state()]\n",
    "          if v > new_v:\n",
    "            new_v = v\n",
    "        V[s] = new_v\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "    \n",
    "    if biggest_change < SMALL_ENOUGH:\n",
    "      break \n",
    "      \n",
    "    # Take our optimal value function, find optimal policy. Recall this is just argmax. \n",
    "    # We loop through all of the actions, and we find the action that gives us the best \n",
    "    # future reward.\n",
    "    for s in policy.keys():\n",
    "      best_a = None\n",
    "      best_value = float('-inf')\n",
    "      # Loop through all possible actions to find the best current action\n",
    "      for a in ALL_POSSIBLE_ACTIONS:\n",
    "        grid.set_state(s)\n",
    "        r = grid.move(a)\n",
    "        v = r + GAMMA * V[grid.current_state()]\n",
    "        if v > best_value:\n",
    "          best_value = v\n",
    "          best_a = a\n",
    "      policy[s] = best_a\n",
    "      \n",
    "  # our goal here is to verify that we get the same answer as with policy iteration\n",
    "  print(\"values:\")\n",
    "  print_values(V, grid)\n",
    "  print(\"policy:\")\n",
    "  print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 10. Dynamic Programming Summary\n",
    "In the last section, we defined the **Markov Decision Process**. In this section we looked at one method for finding solutions to the MDP: **Dynamic Programming**. In particular we looked at:\n",
    "\n",
    "> * **Prediciton Problem:** Find the _value function_ given a _policy_. The algorithm we looked at to solve this was **iterative policy evaluation**.\n",
    "* **Control Problem:** Find the optimal policy and optimal value function. We looked at two algorithms that could do this. The first was called **policy iteration**, and it involved an iterative algorithm inside of another iterative algorithm, which could be inefficient. We then looked at **value iteration**, which truncates the policy evaluation step, and combines both _policy evaluation_ and _policy iteration_ into one step. \n",
    "\n",
    "## 10.1 Asynchronous Dynamic Programming\n",
    "Notice how all of the algorithms we have looked at so far have involved looping through the entire set of states. As we have discussed before, the state space in many realistic games can be incredibly large. Thus, even one iteration of value iteration can take a very long time. Also, recall that one way to speed up value iteration is to do the update \"in place\", meaning that we don't exclusively use the values from the previous iteration to update the values in the current iteration. \n",
    "\n",
    "We can take that a step further with what is called _**asynchronous dynamic programming**_. The modification is simple: instead of looping through the entire state set, we loop through just a few or even just one of the states per iteration. We can chose these states randomly, or based on which states are more likely to be visited. You can learn this information by having the agent play a few rounds of the game. \n",
    "\n",
    "## 10.2 Generalized Policy Iteration \n",
    "Although we introduced policy iteration as a dynamic programming algorithm in this section, we will see that the main idea behind policy iteration will appear again and again through the course. \n",
    "\n",
    "This main concept is: \n",
    "\n",
    "> We iteratively perform 2 steps: policy evaluation and policy improvement, and we alternate between the two until we reach convergence. The only way that we can reach convergence is when Bellman's equation has come true. In other words, the value has converged for the given policy, and the value has stabilized with respect to greedy selection on the value function. \n",
    "\n",
    "A way to visualize this can be seen below: \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1TyYQtTzC8CYTUDkX4W3KzSdFTFd5ucy8\">\n",
    "\n",
    "Initially the policy and value function can be highly suboptimal. But, by requiring the policy to be greedy with respect to the value function, and the value function to be consistent with the policy, we can converge to the optimal policy and optimal value function. \n",
    "\n",
    "## 10.3 Efficiency of Dynamic Programming\n",
    "Let's consider how much better dynamic programming is than brute force search. Well, let's call the number of states $N$ and the number of possible actions $M$. If we assume that the agent can go from the start state to the goal state in $O(N)$ time, then we want a sequence of actions of length $O(N)$. The number of possible actions is then:\n",
    "\n",
    "$$M*M*M*M...*M \\rightarrow O(M^N)$$\n",
    "\n",
    "Hence the number of possible permutations is $O(M^N)$. How you would solve this practically, is you would a list of all the possible permutations of state sequences and then do policy evaluation on all of them. You would then keep the policy that gives you the maximum value function. This is exponential in the number of states, so we can see how DP is a much more efficient solution. \n",
    "\n",
    "## 10.4 Model-based vs. Model-free\n",
    "Lastly, the DP solutions require a full of the environment. In particular, the state transition probabilities $p(s',r \\mid s,a)$. In the real world, these may be hard to measure, especially if $|S|$ is large. In the remaining sections we are going to look at methods with _don't_ require such a model of the environment. These are called _**model-free methods**_. \n",
    "\n",
    "You also may have noticed, that these iterative methods require an initial estimate. In policy iteration and value iteration, we are essentially making estimates from other estimates; going back and forth between the value function and the policy. Making that initial estimate is called _**bootstrapping**_. You will see that the next method we look at, **Monte Carlo** does not require bootstrapping, while the method after that, **Temporal Difference Learning**, does require bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
