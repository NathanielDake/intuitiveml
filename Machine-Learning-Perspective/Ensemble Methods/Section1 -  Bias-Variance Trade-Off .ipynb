{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bias-Variance Trade-Off\n",
    "As we get started with ensemble methods, we will being by looking at the **bias-variance trade-off**. There are three key terms in particular that we are going to look at:\n",
    "> 1. **Bias**\n",
    "2. **Variance**\n",
    "3. **Irreducible Error**\n",
    "\n",
    "We can start be looking into irreducible error...\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "# 1.1 Irreducible Error\n",
    "When we say the term irreducible error, what exactly do we mean? Well, this term comes from the fact that data generating processes are **noisy**. By definition, noise is *random* (aka not deterministic). This in turn mean that we cannot predict the exact values that we are going to get, but rather it's statistics, like **mean** and **variance**. This can be best illustrated with an equation. Say we know that the *true function* that is generating our response data, $Y$ is $f(x)$. Well, the full equation for $Y$ would look like:\n",
    "\n",
    "#### $$Y = f(X) + \\epsilon$$\n",
    "\n",
    "Where $\\epsilon$ is the **irreducible error** term, which can be thought of as **noise**. Visually this can be seen below:\n",
    "\n",
    "<img src=\"images/one-input-variable.png\">\n",
    "\n",
    "<img src=\"images/two-input-variables.png\">\n",
    "\n",
    "The black line in the 2d plot, and the blue surface in the 3d plot represent our **true target function**, $f(x)$. This is the function that is actually used to generate all of the red points. However, we can see that the red points don't perfectly map to $f(x)$. This is due to the irreducible error, or the inherent noise in the system that we cannot get rid of. \n",
    "\n",
    "From a machine learning perspective, let's say that we are in charge of the data-generating process, and our exact function is:\n",
    "\n",
    "#### $$f(x) = 2x + 1$$\n",
    "\n",
    "We are performing linear regression in this case. Now if a machine learning researcher was working with the data and we gave him this exact function, his work would already be done! Because he has the **true function** there is nothing more he can do to make things more accurate. However, again we cannot forget about the irreducible error, which makes our equation look like:\n",
    "\n",
    "#### $$f(x) = ax + b + \\epsilon$$\n",
    "\n",
    "We know that the noise is random, and we generally assume that is is **gaussian distributed** with 0 mean. \n",
    "#### $$\\epsilon \\approx N(0, \\sigma^2) $$\n",
    "\n",
    "In other words, we are starting with a fixed pattern this is based on some underlying function, and then we add noise to it (this noise is the irreducible error): \n",
    "\n",
    "<img src=\"images/i-error-1.png\">\n",
    "\n",
    "<img src=\"images/i-error-2.png\">\n",
    "\n",
    "So, even when *we* are responsible for the data generating process, we cannot predict what the noise will be! So even though we know the *exact* function that the data came from, our own predictions won't be perfect, because there is noise. This is irreducible error. \n",
    "\n",
    "#### $$\\hat{f}(x) = 2x + 1 $$\n",
    "\n",
    "Will not achieve 0 error on:\n",
    "\n",
    "#### $$f(x) = y = 2x + 1 + \\epsilon$$\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "# 1.2 Bias\n",
    "Now let's talk about the **bias**. This is a slightly weird term since we also use the term bias when talking about the intercept in linear transformations. That is not the bias that we are talking about here. The bias we are discussing actually refers to the error in your model. In other words, it is **how far off your prediction is from the target**. \n",
    "\n",
    "#### $$bias = E\\Big[f(x) - \\hat{f}(x)\\Big]$$\n",
    "\n",
    "We can think of it as the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Below, we can see an example of this in regression:\n",
    "\n",
    "<img src=\"images/large-bias.png\">\n",
    "\n",
    "The fitted function clearly does not represent the pattern in the data, so bias tells us how far off we are from the target. In this case we tried to approximate a convex function (complex) with a linear function (simple), and there was a bias error introduced. And below we can see a picture with small bias, however, you may get the feeling that we are overfitting (since it fits the data perfectly, but it is not following the main trend): \n",
    "\n",
    "<img src=\"images/small-bias.png\">\n",
    "\n",
    "And we can also have the same issue arise in classification:\n",
    "\n",
    "<img src=\"images/large-bias-classification.png\">\n",
    "\n",
    "Again, it is a very simple model (not complex) and it does not fit the pattern well. We can also see a small bias below for classification. Again it is a complex decision boundary, which allows us to fit the data better, but we may suspect that it is overfitting:\n",
    "\n",
    "<img src=\"images/small-bias-classification.png\">\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "# 1.3 Variance\n",
    "Now that we have looked at bias, we can now look at variance. We already know about variance in terms of statistics:\n",
    "> How much a random variable deviates from its mean in squared units.\n",
    "\n",
    "However, in the context of the **bias-variance** trade off, we mean something much more specific. Variance in this context tells us:\n",
    "> The statistical variance of our predictor over all possible training sets that are drawn from this particular data generating process. \n",
    "\n",
    "**Variance** refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\\hat{f}$. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in $\\hat{f}$. In general, more flexible statistical methods have higher variance.\n",
    "\n",
    "For example, if you have a model that overfits, in the sense that it can get perfect accuracy on any data set that it trains on - remember that these data sets are all drawn from the same process that we are trying to model. Well if each model is perfect for each training set, then each model will probably be very different from the others! In other words, these different models trained on different data sets from the same process will vary, and we are **measuring that variance**. \n",
    "\n",
    "Let's try and word that differently just one more time. Variance has nothing to do with accuracy! Variance just measures how **inconsistent** a predictor is over different training sets. Remember that our actual goal is not to achieve the lowest possible error rate. Our actual goal is to find the true $f(x)$. Being close to the training points is just a proxy solution. \n",
    "\n",
    "---\n",
    "<br></br>\n",
    "# 1.4 Model Complexity \n",
    "Variance is often used as a proxy for model complexity. Complexity is a malleable term and can mean different things for different classifiers. For example, a decision tree that is very deep can be considered very complex, whereas a shallow decision tree is not complex. For K-Nearest Neighbor, K=1 would be complex, K=50 would not. \n",
    "\n",
    "For linear models you can really see how this concept becomes malleable. For linear models you may initially think that each model has the same complexity because they are all linear. One line is no more complex than any other line. However, in terms of variance there is a difference. Recall that we often use regularization to prevent overfitting. Regularization encourages the weights of a model to be small or 0, which decreases the variance, and hence decreases the complexity as well. \n",
    "\n",
    "Another thing about linear models is that you may assume that they are not complex because they are linear, while nonlinear models are more \"expressive\". For example, you may look at something like decision trees and conclude that because decision trees can find nonlinear decision boundary that they are more complex. But linear doesn't necessarily mean not complex. A large dimensionality linear model can be more complex than a nonlinear model with just a few inputs. So its not a universal measure; it means different things depending on the context. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "# 2. Bias-Variance Trade-Off\n",
    "In machine learning, we are always striving to minimize our error. We just saw that the best we can do is strive to get our error that it is so small that it is just equal to the irreducible error-this is when we know the true underlying function that generates the data. In this situation, since the only error is the irreducible error, then everything else (meaning the reducible error) is 0. \n",
    "\n",
    "We will go over the derivation soon, but the overall error is a combination of:\n",
    "> 1. Bias\n",
    "2. Variance\n",
    "3. Irreducible Error\n",
    "\n",
    "As a data scientist, the goal is clear: we want to make the bias and variance as small as possible! There is a problem however, which is known as the **bias-variance trade-off**. This tells us that we are trying to find a balance between bias and variance. So while we would love low bias and variance, what we find is that when we try and lower the bias, the variance increases, and vice-versa.\n",
    "\n",
    "This has been seen before in the context of overfitting. When we overfit our training data our bias goes down, but our variance goes up. When we underfit the training data out bias increases but our variance goes down. The visualization below clearly demonstrates this. \n",
    "\n",
    "<br></br>\n",
    "<img src=\"images/bias-variance-tradeoff.png\">\n",
    "\n",
    "The **sum** of bias and variance means that there is a minimum somewhere in the middle! Which also coincides approximately with the best generalization error. Another way of visualizing this can be seen below: \n",
    "\n",
    "<br></br>\n",
    "<img src=\"images/bullseye.png\">\n",
    "\n",
    "Now, an important question to ask is: we talk about bias-variance being a tradeoff, but is it really a tradeoff? Is it possible to achieve lower bias, and lower variance at the same time? The trade-off occurs in the context of looking at the same model while we alter the complexity of that model. But what if we could somehow combine models, such that the overall result achieves better accuracy on the training set, and better generalization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "# 3. Bias-Variance Decomposition \n",
    "We are now going to go through the math that shows the expected error is:\n",
    "#### $$E[error] = bias^2 + variance + irreducible \\; error$$\n",
    "Note that people usually use mean-squared error function for both regression and classification scenarios, when talking about bias-variance decomposition. This is despite the fact that we don't use **MSE** when optimizing a classification model. You can try and derive this using another kind of error, but **MSE** is usually sufficient for getting the idea across. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
