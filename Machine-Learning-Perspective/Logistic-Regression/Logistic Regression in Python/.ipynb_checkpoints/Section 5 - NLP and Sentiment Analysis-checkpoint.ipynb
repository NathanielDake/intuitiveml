{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer \n",
    "* sentiment = how positive or negative some text is \n",
    "* this can be seen in yelp, amazon, etc...\n",
    "* We are going to be looking at a data set from: https://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "* This data has been labeled, and we can use that knowledge to our advantage \n",
    "\n",
    "# Data set\n",
    "* These files are XML files, so we are going to need an XML parser to work with them \n",
    "\n",
    "---\n",
    "\n",
    "# Outline of Sentiment Analyzer\n",
    "### What are we trying to predict? \n",
    "* we could use 5 star targets to do regression, but lets just do classification since they are already marked **positive** and **negative**\n",
    "\n",
    "### What category are we looking at?\n",
    "* The electronics category\n",
    "\n",
    "### XML parser\n",
    "* BeautifulSoup\n",
    "* only going to look at key \"review_text\"\n",
    "\n",
    "### Feature vector creation\n",
    "* going to count number of occurences of each word, and divide it by the total number of words \n",
    "* For that to work we will have to do 2 passes through the data\n",
    "* one to collect the total number of distinct words, so that we know the size of our feature vector (and possibily remove stop words like this, is, I, to, etc, to reduce vocabulary size\n",
    "* This is so we know the index of each token\n",
    "* then on the second pass we will be able to assign values to each data vector\n",
    "\n",
    "### Classifier Creation\n",
    "* After that it is simply a matter of creating a classifier using SKLearn\n",
    "* by using a model like logistic regression we can look at the weights of the learned model to get a score for each individual input word\n",
    "* that will tell us if see a word like horrible and it has a score of -1, it is associated with negative reviews\n",
    "\n",
    "# Implementation Details\n",
    "Two main things we want to do in order to pass our training data to the classifier:\n",
    "* X = one-hot encoded bag of words\n",
    "* Y = 1/0 (positive/negative)\n",
    "\n",
    "# Lets get to the code!\n",
    "### Imports\n",
    "* Start with our imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lemmatization\n",
    "Next lets create an instance of the WordNetLemmatizer. To learn more about lemmatization, check this link out: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Nexts lets grab a list of stop words, found here: http://www.lextek.com/manuals/onix/stopwords1.html, and then create a set.\n",
    "\n",
    "More information on stop words can be found here: https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(w.rstrip() for w in open('data/stopwords.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Reviews\n",
    "Now lets load the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews = BeautifulSoup(open('Data/electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "# there are 1000 total\n",
    "len(positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews = BeautifulSoup(open('Data/electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "len(negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Classes\n",
    "If there were more positive reviews compared to negative then we would want to take a random sample so we had balanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize using NLTK's Tokenizer\n",
    "Lets try this first, and see what some of the disadvantages may be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'recommend',\n",
       " 'these',\n",
       " 'speakers',\n",
       " '-',\n",
       " 'have',\n",
       " 'had',\n",
       " 'them',\n",
       " 'for',\n",
       " '3',\n",
       " 'months',\n",
       " 'now',\n",
       " 'and',\n",
       " 'they',\n",
       " 'are',\n",
       " 'great',\n",
       " '-',\n",
       " 'sound',\n",
       " 'quality',\n",
       " 'is',\n",
       " 'great',\n",
       " '.',\n",
       " 'Power',\n",
       " 'converter',\n",
       " 'gets',\n",
       " 'pretty',\n",
       " 'hot',\n",
       " 'though',\n",
       " 'so',\n",
       " 'i',\n",
       " 'suggest',\n",
       " 'putting',\n",
       " 'it',\n",
       " 'on',\n",
       " 'something',\n",
       " 'non',\n",
       " 'flammable',\n",
       " '-',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'think',\n",
       " 'it',\n",
       " 'would',\n",
       " 'catch',\n",
       " 'anything',\n",
       " 'on',\n",
       " 'fire',\n",
       " 'but',\n",
       " 'just',\n",
       " 'to',\n",
       " 'be',\n",
       " 'safe',\n",
       " '.',\n",
       " 'Speakers',\n",
       " 'are',\n",
       " 'not',\n",
       " 'super',\n",
       " 'loud',\n",
       " 'but',\n",
       " 'are',\n",
       " 'great',\n",
       " 'if',\n",
       " 'you',\n",
       " 'just',\n",
       " 'want',\n",
       " 'quality',\n",
       " 'sound',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = positive_reviews[0]\n",
    "nltk.tokenize.word_tokenize(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this does not downcase, so It != it. Not only that, but do we really want to include 'a', 'and' or 'of', etc, anyways? Those words most likely wouldn't be any more common in a positive vs. a negative review, and hence may only add noise to our model. \n",
    "\n",
    "### Preprocessing Function\n",
    "Instead of using the above approach, lets write a function that does our preprocessing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    # downcase all of characters\n",
    "    s = s.lower() \n",
    "    \n",
    "    # split string into words (tokens)\n",
    "    tokens = nltk.tokenize.word_tokenize(s) \n",
    "    \n",
    "    # remove short words, they're probably not useful \n",
    "    tokens = [t for t in tokens if len(t) > 2] \n",
    "    \n",
    "    # put words into base form\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-to-Index map\n",
    "Lets now create a word to index map so that we can create our word-frequency vectors later. We can also save the tokenized versions so we don't have to tokenize again later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index_map['epson']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input Matrices\n",
    "The input matrices are going to be made up of the words from our word index map. \n",
    "\n",
    "> * The function `tokens_to_vector`  takes in an array of tokens. \n",
    "* It then loops through the array, and then will add 1 to the x training example in the column of any word that occurs\n",
    "* example: if the array contains the word \"horrible\", the x training vector that is being created will have a +1 added to the index that represents \"horrible\"\n",
    "* this will continue onward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now are going to look through the both the positive and negative token arrays. What is really happening is any time a word is encountered, the input matrix has a +1 added to its specific column for that training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle our data\n",
    "Lets now shuffle our data and create traing test splits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "# the last 100 rows will be test data\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Logistic Regression Model and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.7\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at the weights for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend 0.721731694164\n",
      "speaker 0.840179660187\n",
      "month -0.763958577277\n",
      "sound 1.18229711024\n",
      "quality 1.39155588211\n",
      "pretty 0.669390842765\n",
      "n't -2.24120843936\n",
      "you 0.922648398124\n",
      "price 2.7031713505\n",
      "excellent 1.39295020663\n",
      "wa -1.66334493742\n",
      "laptop 0.546463190134\n",
      "perfect 1.06620527026\n",
      "ha 0.698991284566\n",
      "cable 0.686356713833\n",
      "bit 0.569320252056\n",
      "little 0.861271185304\n",
      "fit 0.546196149522\n",
      "buy -0.916600516047\n",
      "try -0.658400265073\n",
      "doe -1.18328921761\n",
      "lot 0.63453293964\n",
      "using 0.683173546191\n",
      "easy 1.75884199088\n",
      "bad -0.747977767389\n",
      "company -0.570860437642\n",
      "time -0.747340714474\n",
      "then -1.03498636974\n",
      "software -0.533526038642\n",
      "video 0.547596533954\n",
      "value 0.536045087478\n",
      "money -1.09037737947\n",
      "'ve 0.838069726274\n",
      "poor -0.782166454075\n",
      "home 0.567139496652\n",
      "tried -0.78750377086\n",
      "love 1.23219324718\n",
      "unit -0.649458604082\n",
      "highly 1.00506284325\n",
      "fast 0.914970773823\n",
      "expected 0.520602586276\n",
      "week -0.669957616405\n",
      "happy 0.646355913785\n",
      "comfortable 0.638802425366\n",
      "returned -0.784685888305\n",
      "hour -0.582630032301\n",
      "support -0.930850313177\n",
      "item -1.00533265381\n",
      "stopped -0.537994595622\n",
      "warranty -0.628424939133\n",
      "space 0.567797037674\n",
      "memory 0.897681697891\n",
      "waste -1.00953783905\n",
      "customer -0.670575134858\n",
      "return -1.16371982739\n",
      "paper 0.647347608366\n",
      "refund -0.600855433733\n",
      "terrible -0.528637467009\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results\n",
    "We can see from the above results what words the model found to be the most useful in predicting whether it was a positive or negative review! Pretty cool stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
