{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sick or Healthy\n",
    "We will now look at a model that examines our state of healthiness vs. being sick. Keep in mind that this is very much like something you could do in real life. If you wanted to model a certain situation or environment, we could take some data that we have gathered, build a maximum likelihood model on it, and do things like study the properties that emerge from the model, or make predictions from the model, or generate the next most likely state. \n",
    "\n",
    "Let's say we have 2 states: **sick** and **healthy**. We know that we spend most of our time in a healthy state, so the probability of transitioning from healthy to sick is very low:\n",
    "\n",
    "#### $$p(sick \\; | \\; healthy) = 0.005$$\n",
    "\n",
    "Hence, the probability of going from healthy to healthy is:\n",
    "\n",
    "#### $$p(healthy \\; | \\; healthy) = 0.995$$\n",
    "\n",
    "Now, on the other hand the probability of going from sick to sick is also very high. This is because if you just got sick yesterday then you are very likely to be sick tomorrow.\n",
    "\n",
    "#### $$p(sick \\; | \\; sick) = 0.8$$\n",
    "\n",
    "However, the probability of transitioning from sick to healthy should be higher than the reverse, because you probaably won't stay sick for as long as you would stay healthy:\n",
    "\n",
    "#### $$p(healthy \\; | \\; sick) = 0.02$$\n",
    "\n",
    "We have now fully defined our state transition matrix, and we can now do some calculations. \n",
    "\n",
    "## 1.1 Example Calculations\n",
    "### 1.1.1 \n",
    "What is the probability of being healthy for 10 days in a row, given that we already start out as healthy? Well that is:\n",
    "\n",
    "#### $$p(healthy \\; 10 \\; days \\; in \\; a \\; row \\; | \\; healthy \\; at \\; t=0) = 0.995^9 = 95.6 \\%$$\n",
    "\n",
    "How about the probability of being healthy for 100 days in a row? \n",
    "\n",
    "#### $$p(healthy \\; 100 \\; days \\; in \\; a \\; row \\; | \\; healthy \\; at \\; t=0) = 0.995^99 = 60.9 \\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 2. Expected Number of Continuously Sick Days\n",
    "We can now look at the expected number of days that you would remain in the same state (e.g. how many days would you expect to stay sick given the model?). This is a bit more difficult than the last problem, but completely doable, only involving the mathematics of <a href=\"https://en.wikipedia.org/wiki/Geometric_series\">infinite sums</a>.\n",
    "\n",
    "First, we can look at the probability of being in state $i$, and going to state $i$ in the next state. That is just $A(i,i)$:\n",
    "\n",
    "#### $$p \\big(s(t)=i \\; | \\; s(t-1)=i \\big) = A(i, i)$$\n",
    "\n",
    "Now, what is the probability distribution that we actually want to calculate? How about we calculate the probability that we stay in state $i$ for $n$ transitions, at which point we move to another state:\n",
    "\n",
    "#### $$p \\big(s(t)!=i \\; | \\; s(t-1)=i \\big) = 1 - A(i, i)$$\n",
    "\n",
    "So, the joint probability that we are trying to model is:\n",
    "\n",
    "#### $$p\\big(s(1)=i, s(2)=i,...,s(n)=i, s(n+1) != i\\big) = A(i,i)^{n-1}\\big(1-A(i,i)\\big)$$\n",
    "\n",
    "In english this means that we are multiplying the transition probability of staying in the same state, $A(i,i)$, times the number of times we stayed in the same state, $n$, times $1 - A(i,i)$, the probability of transitioning from that state. This leaves us with an expected value for $n$ of:\n",
    "\n",
    "#### $$E(n) = \\sum np(n) = \\sum_{n=1..\\infty} nA(i,i)^{n-1}(1-A(i,i))$$\n",
    "\n",
    "Note, in the above equation $p(n)$ is the probability that we will see state $i$ $n-1$ times after starting from $i$ and then see a state that is not $i$. Also, we know that the expected value of $n$ should be the sum of all possible values of $n$ times $p(n)$. \n",
    "\n",
    "\n",
    "## 2.1 Expected $n$\n",
    "So, we can now expand this function and calculate the two sums separately. \n",
    "\n",
    "\n",
    "\n",
    "#### $$E(n) = \\sum_{n=1..\\infty}nA(i,i)^{n-1}(1 - A(i,i)) = \\sum nA(i, i)^{n-1} - \\sum nA(i,i)^n$$\n",
    "\n",
    "**First Sum**<br>\n",
    "With our first sum, we can say that:\n",
    "\n",
    "#### $$S = \\sum nA(i, i)^{n-1}$$\n",
    "\n",
    "#### $$S = 1 + 2A + 3A^2 + 4A^3+ ...$$\n",
    "\n",
    "And we can then multiply that sum, $S$, by $A$, to get:\n",
    "\n",
    "#### $$AS = a + 2A^2 + 3A^3 + 4A^4+...$$\n",
    "\n",
    "And then we can subtract $AS$ from $S$:\n",
    "\n",
    "#### $$S - AS = S'= 1 + A + A^2 + A^3+...$$\n",
    "\n",
    "This $S'$ is another infinite sum, but it is one that is much easier to solve! \n",
    "\n",
    "#### $$S'= 1 + A + A^2 + A^3+...$$\n",
    "\n",
    "And then $AS'$ is:\n",
    "\n",
    "#### $$AS' = A + A^2 + A^3+ + A^4 + ...$$\n",
    "\n",
    "Which, when we then do $S' - AS'$, we end up with:\n",
    "\n",
    "#### $$S' - AS' = 1$$\n",
    "#### $$S' = \\frac{1}{1 - A}$$\n",
    "\n",
    "And if we then substitute that value in for $S'$ above:\n",
    "\n",
    "#### $$S - AS = S'= 1 + A + A^2 + A^3+... = \\frac{1}{1 - A}$$\n",
    "#### $$S - AS = \\frac{1}{1 - A}$$\n",
    "#### $$S = \\frac{1}{(1 - A)^2}$$\n",
    "\n",
    "\n",
    "**Second Sum**<br>\n",
    "We can now look at our second sum:\n",
    "\n",
    "#### $$S = \\sum nA(i,i)^n$$\n",
    "\n",
    "#### $$S = 1A + 2A^2 + 3A^3 +...$$\n",
    "#### $$SA = 1A^2 + 2A^3 +...$$\n",
    "#### $$S - AS = S' = A + A^2 + A^3 + ...$$\n",
    "#### $$AS' = A^2 + A^3 + A^4 +...$$\n",
    "#### $$S' - AS' = A$$\n",
    "#### $$S' = \\frac{A}{1 - A}$$\n",
    "And we can plug back in $S'$ to get:\n",
    "\n",
    "#### $$S - AS = \\frac{A}{1 - A}$$\n",
    "#### $$S = \\frac{A}{(1 - A)^2}$$\n",
    "\n",
    "**Combine** <br>\n",
    "We can now combine these two sums as follows:\n",
    "\n",
    "#### $$E(n) = \\frac{1}{(1 - a)^2} - \\frac{a}{(1-a)^2}$$\n",
    "\n",
    "#### $$E(n) = \\frac{1}{1-a}$$\n",
    "\n",
    "**Calculate Number of Sick Days**<br>\n",
    "So, how do we calculate the correct number of sick days? That is just:\n",
    "\n",
    "#### $$\\frac{1}{1 - 0.8} = 5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 3. SEO and Bounce Rate Optimization \n",
    "We are now going to look at SEO and Bounch Rate Optimization. This is a problem that every developer and website owner can relate to. You have a website and obviously you would like to increase traffic, increase conversions, and avoid a high bounce rate (which could lead to google assigning your page a low ranking). What would a good way of modeling this data be? Without even looking at any code we can look at some examples of things that we want to know, and how they relate to markov models. \n",
    "\n",
    "## 3.1 Arrival\n",
    "First and foremost, how do people arrive on your page? Is it your home page? Your landing page? Well, this is just the very first page of what is hopefully a sequence of pages. So, the markov analogy here is that this is just the initial state distribution or $\\pi$. So, once we have our markov model, the $\\pi$ vector will tell us which of our pages a user is most likely to start on. \n",
    "\n",
    "## 3.2 Sequences of Pages \n",
    "What about sequences of pages? Well, if you think people are getting to your landing page, hitting the buy button, checking out, and then closing the browser window, you can test the validity of that assumption by calculating the probability of that sequence. Of course, the probability of any sequence is probability going to be much less than 1. This is because for a longer sequence, we have more multiplication, and hence smaller final numbers. We do have two alternatives however:\n",
    "\n",
    "> * 1) You can compare the probability of two different sequences. So, are people going through the entire checkout process? Or is it more probable that they are just bouncing? \n",
    "* 2) Another option is to just find the transition probabilities themselves. These are conditional probabilities instead of joint probabilities. You want to know, once they have made it to the landing page, what is the probability of hitting buy. Then, once they have hit buy, what is the probability of them completing the checkout. \n",
    "\n",
    "## 3.3 Bounce Rate\n",
    "This is hard to measure, unless you are google and hence have analytics on nearly every page on the web. This is because once a user has left your site, you can no longer run code on their computer or track what they are doing. However, let's pretend that we can determine this information. Once we have done this, we can measure which page has the highest bounce rate. At this point we can manually analyze that page and ask our marketing people \"what is different about this page that people don't find it useful/want to leave?\" We can then address that problem, and the hopefully later analysis shows that the fixed page no longer has a high bounce right. In the markov model, we can just represents this as the null state. \n",
    "\n",
    "## 3.4 Data\n",
    "So, the data we are going to be working with has two columns: `last_page_id` and `next_page_id`. This can be interpreted as the current page and the next page. The site has 10 pages with the id's 0-9. We can represent start pages by making the current page -1, and the next page the actual page. We can represent the end of the page with two different codes, `B`(bounce) or `C` (close). In the case of bounce, the user saw the page and then immediately bounced. In the case of close, the user saw the page stayed and potentially saw some useful information, and then closed the window. So, you can imagine that our engineer may use time as a factor in determining if it is a bounce or a close.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state distribution\n",
      "8 0.10152591025834719\n",
      "2 0.09507982071813466\n",
      "5 0.09779926474291183\n",
      "9 0.10384247368686106\n",
      "0 0.10298635241980159\n",
      "6 0.09800070504104345\n",
      "7 0.09971294757516241\n",
      "1 0.10348995316513068\n",
      "4 0.10243239159993957\n",
      "3 0.09513018079266758\n",
      "Bounce rate for 1: 0.125939617991374\n",
      "Bounce rate for 2: 0.12649551345962112\n",
      "Bounce rate for 8: 0.12529550827423167\n",
      "Bounce rate for 6: 0.1208153180975911\n",
      "Bounce rate for 7: 0.12371650388179314\n",
      "Bounce rate for 3: 0.12743384922616077\n",
      "Bounce rate for 4: 0.1255756067205974\n",
      "Bounce rate for 5: 0.12369559684398065\n",
      "Bounce rate for 0: 0.1279673590504451\n",
      "Bounce rate for 9: 0.13176232104396302\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Goal here is to store start page and end page, and the count how many times that happens. After that \n",
    "we are going to turn it into a probability distribution. We can divide all transitions that start with specific\n",
    "start state, by row_sum\"\"\"\n",
    "transitions = {} # getting all specific transitions from start pg to end pg, tallying up # of times each occurs\n",
    "row_sums = {} # start date as key -> getting number of times each starting pg occurs\n",
    "\n",
    "# Collect our counts\n",
    "for line in open('../../../data/site/site_data.csv'):\n",
    "  s, e = line.rstrip().split(',') # get start and end page \n",
    "  transitions[(s, e)] = transitions.get((s, e), 0.) + 1\n",
    "  row_sums[s] = row_sums.get(s, 0.) + 1 \n",
    "  \n",
    "# Normalize the counts so they become real probability distributions \n",
    "for k, v in transitions.items():\n",
    "  s, e = k\n",
    "  transitions[k] = v / row_sums[s]\n",
    "  \n",
    "# Calculate initial state distribution\n",
    "print('Initial state distribution')\n",
    "for k, v in transitions.items():\n",
    "  s, e = k\n",
    "  if s == '-1': # this means it is the start of the sequence. \n",
    "    print (e, v)\n",
    "      \n",
    "# Which page has the highest bounce rate?\n",
    "for k, v in transitions.items():\n",
    "  s, e = k\n",
    "  if e == 'B':\n",
    "    print(f'Bounce rate for {s}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that page with `id` 9 has the highest value in the initial state distribution, so we are most likely to start on that page. We can then see that the page with highest bounce rate is also at page `id` 9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 4. Build a 2nd-order language model and generate phrases\n",
    "So, we are now going to work with non first order markov chains for a little bit. In this example we are going to try and create a language model. So we are going to first train a model on some data to determine the distribution of a word given the previous two words. We can then use this model to generate new phrases. Note that another step of this model would be to calculate the probability of a phrase.\n",
    "\n",
    "So the data that we are going to look at is just a collection of Robert Frost Poems. It is just a text file with all of the poems concatenated together. So, the first thing we are going to want to do is tokenize each sentence, and remove punctuation. It will look similar to this:\n",
    "\n",
    "```\n",
    "def remove_punctuation(s):\n",
    "    return s.translate(None, string.punctuation)\n",
    "    \n",
    "tokens = [t for t in remove_puncuation(line.rstrip().lower()).split()]\n",
    "```\n",
    "\n",
    "Once we have tokenized each line, we want to perform various counts in addition to the second order model counts. We need to measure the initial distribution of words, or stated another way the distribution of the first word of a sentence. We also want to know the distribution of the second word of a sentence. Both of these do not have two previous words, so they are not second order. We could technically include them in the second order measurement by using `None` in place of the previous words, but we won't do that here. We also want to keep track of how to end the sentence (end of sentence distribution, will look similar to (w(t-2), w(t-1) -> END)), so we will include a special token for that too. \n",
    "\n",
    "When we do this counting, what we first want to do is create an array of all possibilities. So, for example if we had two sentences:\n",
    "\n",
    "```\n",
    "I love dogs\n",
    "I love cats\n",
    "```\n",
    "\n",
    "Then we could have a dictionary where the key was `(I, love)` and the value was an array `[dogs, cats]`. If \"I love\" was also a stand alone sentence, then the value would be `[dogs, cats, END]`. The function below can help us with this, since we first need to check if there is any value for the key, create an array if not, otherwise just append to the array. \n",
    "\n",
    "```\n",
    "def add2dict(d, k, v):\n",
    "    if k not in d:\n",
    "        d[k] = []\n",
    "    else:\n",
    "        d[k].append(v)\n",
    "```\n",
    "\n",
    "One we have collected all of these arrays of possible next words, we need to turn them into **probability distributions**. For example, the array `[cat, cat, dog]` would become the dictionary `{\"cat\": 2/3, \"dog\": 1/3}`. Here is a function that can do this:\n",
    "\n",
    "```\n",
    "def list2pdict(ts):\n",
    "    d = {}\n",
    "    n = len(ts)\n",
    "    for t in ts:\n",
    "        d[t] = d.get(t, 0.) + 1 \n",
    "    for t, c in d.items():\n",
    "        d[t] = c / n\n",
    "    return d\n",
    "```\n",
    "\n",
    "Next, we will need a function that can sample from this dictionary. To do this we will need to generate a random number between 0 and 1, and then use the distribution of the words to sample a word given a random number. Here is a function that can do that:\n",
    "\n",
    "```\n",
    "def sample_word(d):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for t, p in d.items():\n",
    "        cumulative += p\n",
    "        if p0 < cumulative:\n",
    "            return t\n",
    "    assert(False) # should never get here\n",
    "```\n",
    "\n",
    "Because all of our distributions are structured as dictionaries, we can use the same function for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that had been for just such meeting\n",
      "and sit before you in that grave one\n",
      "you neednt talk to you to cool the eyes\n",
      "how could that be i thought i saw the strange position of his eyes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"3 dicts. 1st store pdist for the start of a phrase, then a second word dict which stores the distributions\n",
    "for the 2nd word of a sentence, and then we are going to have a dict for all second order transitions\"\"\"\n",
    "initial = {}\n",
    "second_word = {}\n",
    "transitions = {}\n",
    "\n",
    "def remove_punctuation(s):\n",
    "  return s.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def add2dict(d, k, v): \n",
    "  \"\"\"Parameters: Dictionary, Key, Value\"\"\"\n",
    "  if k not in d:\n",
    "    d[k] = []\n",
    "  d[k].append(v)\n",
    "  \n",
    "# Loop through file of poems\n",
    "for line in open('../../../data/poems/robert_frost.txt'):\n",
    "  tokens = remove_punctuation(line.rstrip().lower()).split() # Get all tokens for specific line we are looping over\n",
    "  \n",
    "  T = len(tokens) # Length of sequence\n",
    "  for i in range(T): # Loop through every token in sequence\n",
    "    t = tokens[i] \n",
    "    if i == 0: # We are looking at first word\n",
    "      initial[t] = initial.get(t, 0.) + 1 \n",
    "    else:\n",
    "      t_1 = tokens[i - 1]\n",
    "      if i == T - 1: # Looking at last word\n",
    "        add2dict(transitions, (t_1, t), 'END')\n",
    "      if i == 1: # second word of sentence, hence only 1 previous word\n",
    "        add2dict(second_word, t_1, t)\n",
    "      else: \n",
    "        t_2 = tokens[i - 2] # Get second previous word\n",
    "        add2dict(transitions, (t_2, t_1), t) # add previous and 2nd previous word as key, and current word as val\n",
    "        \n",
    "# Normalize the distributions\n",
    "initial_total = sum(initial.values())\n",
    "for t, c in initial.items():\n",
    "  initial[t] = c / initial_total\n",
    "\n",
    "# Take our list and turn it into a dictionary of probabilities\n",
    "def list2pdict(ts):\n",
    "  d = {}\n",
    "  n = len(ts) # get total number of values\n",
    "  for t in ts: # look at each token\n",
    "    d[t] = d.get(t, 0.) + 1\n",
    "  for t, c in d.items(): # go through dictionary, divide frequency by sum\n",
    "    d[t] = c / n\n",
    "  return d\n",
    "\n",
    "for t_1, ts in second_word.items():\n",
    "  second_word[t_1] = list2pdict(ts)\n",
    "\n",
    "for k, ts in transitions.items():\n",
    "  transitions[k] = list2pdict(ts)\n",
    "  \n",
    "def sample_word(d):\n",
    "  p0 = np.random.random() # Generate random number from 0 to 1 \n",
    "  cumulative = 0 # cumulative count for all probabilities seen so far\n",
    "  for t, p in d.items():\n",
    "    cumulative += p\n",
    "    if p0 < cumulative:\n",
    "      return t\n",
    "  assert(False) # should never hit this\n",
    "  \n",
    "\"\"\"Function to generate a poem\"\"\"\n",
    "def generate():\n",
    "  for i in range(4):\n",
    "    sentence = []\n",
    "    \n",
    "    # initial word\n",
    "    w0 = sample_word(initial)\n",
    "    sentence.append(w0)\n",
    "    \n",
    "    # sample second word\n",
    "    w1 = sample_word(second_word[w0])\n",
    "    sentence.append(w1)\n",
    "    \n",
    "    # second-order transitions until END -> enter infinite loop\n",
    "    while True:\n",
    "      w2 = sample_word(transitions[(w0, w1)]) # sample next word given previous two words\n",
    "      if w2 == 'END': \n",
    "        break\n",
    "      sentence.append(w2)\n",
    "      w0 = w1\n",
    "      w1 = w2\n",
    "    print(' '.join(sentence))\n",
    "    \n",
    "generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 5. Google's PageRank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
