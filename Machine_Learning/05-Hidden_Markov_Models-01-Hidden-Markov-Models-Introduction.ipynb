{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hidden Markov Models Introduction\n",
    "This notebook is going to cover **hidden markov models**, which are used for modeling sequences of data. Sequences appear everywhere, from stock prices, to language, credit scoring, webpage visits.\n",
    "\n",
    "Often, we may be dealing with sequences in machine learning and we don't even realize it; or we may even ignore the fact that it came from a sequence. For instance, the following sentence: \n",
    "\n",
    "> \"Like and cats dogs I\"\n",
    "\n",
    "Clearly, this model does not make any sense. This is what happens when you use a model such as bag of words. The fact that it becomes much harder to tell what a sentence means when you take away the time aspect, tells you that there is a lot of information carried there. The original sentence was:\n",
    "\n",
    "> \"I like cats and dogs\"\n",
    "\n",
    "This may be relatively easy to decode on your own, but you can imagine that this gets much harder as the sentence gets longer. \n",
    "\n",
    "## 1.1 Outline\n",
    "\n",
    "1. We will start by looking at the most **basic markov model**, with no hidden portion. These are very useful for modeling sequences as we will see. We will talk about the mathematical properties of the markov model, and go through a ton of examples so we can see how they are used. Google's PageRank algorithm is based on markov models. So, despite being based on old technology, markov models are still very useful and relevant today. \n",
    "\n",
    "2. We will also talk about how to model language, and how to analyze web visitor data, so you can fix problems like high bounce rate. \n",
    "\n",
    "3. Next, we will look at the **hidden markov model**. This will be very complex mathematically, but the first section should prepare you. We will look at the three basic problems in hidden markov modeling:\n",
    "    * Predicting the probability of a sequence\n",
    "    * Predicting the most likely sequence of hidden states given an observed sequence\n",
    "    * How to train a hidden markov model \n",
    "    * We will even go further and look at how this relates to deep learning by using gradient descent to train our HMM. Typically, the expectation maximization algorithm is used. We will do this too, but we will see how gradient descent makes this much easier. \n",
    "4. We will finally look at Hidden Markov Models for real-valued data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# Unsupervised or Supervised?\n",
    "We can now discuss where HMM's fit in the spectrum of machine learning techniques. Hidden markov models are for modeling sequences. If you think of a sequence by itself, that could look like:\n",
    "\n",
    "> ```\n",
    "x(1), x(2),...,x(t),...,x(T)\n",
    "```\n",
    "\n",
    "We can see that there is no label there, so HMM's just model the distribution of a sequence; this means that it is unsupervised. \n",
    "\n",
    "## Classification\n",
    "However, we often see HMMs being used for classifiers as well. For instance, we could train an HMM to model a male voice and a female voice. Then we could predict, given a new voice sample, whether the voice is male or female. \n",
    "\n",
    "How can we do this, given that we only have a model for the probability of the data, $p(X)$? The key idea here is bayes rule. What we actually modeled was $p(X \\; | \\; male)$ and $p(X \\; | \\; female)$. We know that bayes rule helps us reverse the conditional, leaving us with:\n",
    "\n",
    "$$p(male \\; | \\; x) \\; and \\; p(female \\; | \\; x)$$\n",
    "\n",
    "Now we can find the most probable class, and our prediction becomes whatever the most probable class is. From bayes rule we know that:\n",
    "\n",
    "$$p(male \\; | \\; x) = \\frac{p(X \\; | \\; male) p(male)}{p(X)}$$\n",
    "\n",
    "$$p(female \\; | \\; x) = \\frac{p(X \\; | \\; female) p(female)}{p(X)}$$\n",
    "\n",
    "And in general:\n",
    "$$posterior = \\frac{likelihood * prior}{normalization \\; constant}$$\n",
    "\n",
    "We do not care about the actual probability, $p(X \\; | \\; C)$, just which one is great. It should be noted that while we can model it with an HMM, but also with Naive Bayes:\n",
    "\n",
    "$$P(X \\; | \\; C) = P(x(1,1) \\; | \\; C)*P(x(1,2) \\; | \\; C)*...*P(x(T,D) \\; | \\; C)$$\n",
    "\n",
    "With Naive Bayes, we make the independence assumption, meaning that each sample is independent. So, we take the probability of each given feature, and multiply them together to get the final $P(X \\; | \\; C)$. What we can even do is extend this idea behind hidden markov models and model the data using more general concepts like Bayesian Belief Networks. \n",
    "\n",
    "## Conclusion\n",
    "At its core, HMMs are unsupervised. However, it can easily be used for classification just by creating a separate model for each class, and then making the prediction based on which model gives you the maximum posterior probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
