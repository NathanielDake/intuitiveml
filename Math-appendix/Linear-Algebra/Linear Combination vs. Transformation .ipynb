{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Background\n",
    "As we progress in our understanding of the math surrounding machine learning, AI, and DS, there will be a host of linear algebra concepts that we are forced to reckon with. From PCA and it's utilization of eigenvalues and eigenvectors, to neural networks reliance on linear combinations and matrix multiplication, the list goes on and on. Having a very solid grasph on linear algebra is crucial to realizing _how_ and _why_ these algorithms work. \n",
    "\n",
    "This notebook in particular is going to focus on the connection between the following:\n",
    "\n",
    "> * **Linear Combinations**\n",
    "* **Linear Transformations**\n",
    "* **The Dot Product**\n",
    "* **Functions**\n",
    "\n",
    "These concepts are incredibly prevelant and linked to each other in beautiful ways, however, this link is generally missing in the way linear algebra is taught-particularly when studying machine learning. Before moving I recommend reviewing my notebook concerning vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Linear Combination \n",
    "If you go to wikipedia, you can find the following definition regarding a **linear combination**:\n",
    "\n",
    "> A linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results. For example, a linear combination of $x$ and $y$ would be any expression of the form $ax + by$, where a and b are constants.\n",
    "\n",
    "Now, this can be defined slightly more formally in regards to vectors as follows:\n",
    "\n",
    "> If $v_1,...,v_n$ is a set of vectors, and $a_1,...,a_n$ is a set of scalars, then their linear combination would take the form:<br>\n",
    "$$a_1\\vec{v_1} + a_2\\vec{v_2}+...+a_n\\vec{v_n}$$\n",
    "Where, it should be noted that all $\\vec{v}$'s are vectors. Hence, it can be expanded as:<br>\n",
    "<br>\n",
    "$$a_1\\begin{bmatrix}\n",
    "    v_1^1 \\\\\n",
    "    v_1^2 \\\\\n",
    "    .     \\\\\n",
    "    .     \\\\\n",
    "    v_1^m\n",
    "\\end{bmatrix} \n",
    "+\n",
    "a_2\\begin{bmatrix}\n",
    "    v_2^1 \\\\\n",
    "    v_2^2 \\\\\n",
    "    .     \\\\\n",
    "    .     \\\\\n",
    "    v_2^m\n",
    "\\end{bmatrix} \n",
    "+\n",
    "a_n\\begin{bmatrix}\n",
    "    v_n^1 \\\\\n",
    "    v_n^2 \\\\\n",
    "    .     \\\\\n",
    "    .     \\\\\n",
    "    v_n^m\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "where for generality we have defined $\\vec{v}$ to be an $m$ dimensional vector. Notice that the final result is a single $m$ dimensional vector. So, for instance, in a simple case, we could have:<br>\n",
    "<br>\n",
    "$$a_1\\begin{bmatrix}\n",
    "    v_1^1 \n",
    "\\end{bmatrix} \n",
    "+\n",
    "a_2\\begin{bmatrix}\n",
    "    v_2^1\n",
    "\\end{bmatrix} \n",
    "+\n",
    "a_n\\begin{bmatrix}\n",
    "    v_n^1\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "<br>\n",
    "$$a_1\\begin{bmatrix}\n",
    "    v_1\n",
    "\\end{bmatrix} \n",
    "+\n",
    "a_2\\begin{bmatrix}\n",
    "    v_2\n",
    "\\end{bmatrix} \n",
    "+\n",
    "a_n\\begin{bmatrix}\n",
    "    v_n\n",
    "\\end{bmatrix} \n",
    "$$<br>\n",
    "$$a_1v_1+a_2v_2+a_nv_n$$\n",
    "And end up with a 1 dimensional vector, often just viewed as a scalar. \n",
    "\n",
    "Now, this definition is good to have in mind, however we can make it a bit more concrete by expanding visually. For instance, if you have a pair of numbers that is meant to describe, a vector, such as:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    3 \\\\\n",
    "    -2\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "<img src=\"images/linear-comb-1.png\" width=\"300\">\n",
    "\n",
    "We can think of each coordinate as a scalar (how does it stretch or squish vectors?). In linear algebra, there are two very important vectors, commonly known as $\\hat{i}$ and $\\hat{j}$:\n",
    "\n",
    "<img src=\"images/linear-comb-2.png\" width=\"300\">\n",
    "\n",
    "Now, we can think of the coordinates of our vector as stretching $\\hat{i}$ and $\\hat{j}$:\n",
    "\n",
    "<img src=\"images/linear-comb-3.png\" width=\"300\">\n",
    "\n",
    "In this sense, the vector that these coordinates describe is the sum of two scaled vectors:\n",
    "\n",
    "#### $$(3)\\hat{i} + (-2)\\hat{j}$$\n",
    "\n",
    "Note that $\\hat{i}$ and $\\hat{j}$ have a special name; they are refered to as the _basis vectors_ of the _xy_ coordinate system. This means that when you think about vector coordinates as scalars, the basis vectors are what those coordinates are actually scaling. \n",
    "\n",
    "Now, this brings us to our first definiton:\n",
    "\n",
    "> **Linear Combination:** Any time you are scaling two vectors and then adding them together, you have a linear combination. For example: <br>\n",
    "$$(3)\\hat{i} + (-2)\\hat{j}$$<br>\n",
    "Or, more generally:\n",
    "$$a\\vec{v} + b \\vec{w}$$<br>\n",
    "Where, above both $a$ and $b$ are scalars. \n",
    "\n",
    "This can be seen visually: \n",
    "\n",
    "<img src=\"images/linear-comb-4.png\" width=\"300\">\n",
    "\n",
    "And we can see that as we scale $\\vec{v}$ and $\\vec{w}$ we can create many different linear combinations:\n",
    "\n",
    "<img src=\"images/linear-comb-5.png\" width=\"400\">\n",
    "\n",
    "This brings up another definition, _span_. \n",
    "\n",
    "> **Span**: The set of all possible vectors that you can reach with a linear combination of a given pair of vectors is known as the _span_ of those two vectors. \n",
    "\n",
    "So, the span of most 2-d vectors is all of space, however, if they line up then they span is a specific line. When two vectors do happen to line up we can say that they are _linearly dependent_, and one can be expressed in terms of the other. On the other hand, if they do line up, they are said to be _linearly independent_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Linear Transformations and Matrices\n",
    "Linear transformations are absolutely fundamental in order to understand matrix vector multiplication (well, unless you want to rely on memorization). To start, let's just parse the term \"Linear Transformation\". \n",
    "\n",
    "Transformation is essentially just another way of saying _function_. This is where the first bit of confusion can arise though if you are being particularly thoughtful about the process's-what exactly is a function? It is helpful to define it before moving forward.\n",
    "\n",
    "**Function**<br>\n",
    "Generally, in mathematics we view a function as a process that take in an input and returns an output (this coincides nicely with the computer science view as well). It can be viewed as:\n",
    "\n",
    "#### $$x \\rightarrow f(x) \\rightarrow y$$\n",
    "Or, expanded as:\n",
    "#### $$x_1, x_2, ... , x_n \\rightarrow f(x_1, x_2, ... , x_n) \\rightarrow y$$\n",
    "\n",
    "This is how it is _generally_ encountered, where anywhere from one to several inputs are taken in, and a single output is produced. However, let's define a function more rigorously:\n",
    "\n",
    "> A function is a _process_ or a relation that associates each element $x$ of a set $X$, the _domain_ of the function, to a single element $y$ of another set $Y$ (possibly the same set), the codomain of the function.\n",
    "\n",
    "The important point to recognize from the above definition is that, while it is common for a function to map elements from a set $X$ to a different set $Y$, the two sets can be _same_. Hence, although it is not encountered quite as often in ML, a function can map $X \\rightarrow X$. \n",
    "\n",
    "Now, back to our term transformation; it is something that takes in inputs, and spits out an output for each one. In the context of linear algebra, we like to think about transformations that take in some vector, and spit out another vector:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    7\n",
    "\\end{bmatrix} \\rightarrow \n",
    "L(\\vec{v})\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is where we can see an example of a function that does not map to a different space necessarily, but potentially to itself. In other words, generally if we have a function that takes in two inputs, we end up with one output:\n",
    "\n",
    "#### $$f(x,y) = z$$\n",
    "\n",
    "However, we can clearly see here that we take in two inputs (coordinates of the vector) and end up with two outputs (coordinates of the transformed vector). \n",
    "\n",
    "So, why use the word transformation instead of function if they essentially mean the same thing? Well, it is to be suggestive of _movement_! The way to think about functions of vectors is to use movement. If a transformation takes some input vector to some output vector, we image that input vector moving over to the output vector:\n",
    "\n",
    "<img src=\"images/linear-trans-1.png\" width=\"400\">\n",
    "\n",
    "And in order to think about the transformation as a whole, we can think about _every possible input vector_ moving over to its corresponding _output vector_. \n",
    "\n",
    "**Key Point**<br>\n",
    "> We are now seeing that our general interpretation of a function can be expanded (via its full definition), to not simply taking in one or several inputs and producing a single output, but taking in one to several inputs, and producing one to several outputs. This expansion is key to recognizing the relationship between functions, linear transformations, and dot products. \n",
    "\n",
    "Now let's pose the following question: If we were given the coordinates of a vector, and we then were trying to determine the coordinates of where that vector landed after being linearly transformed, how would we represent this?\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    x_{in} \\\\\n",
    "    y_{in}\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "????\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "    x_{out} \\\\\n",
    "    y_{out}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Well, it turns out that you actually only need to record where the two basis vectors land and everything else will follow from that! For example, let's consider the vector:\n",
    "\n",
    "#### $$ \\vec{v} = \n",
    "\\begin{bmatrix}\n",
    "    -1 \\\\\n",
    "    2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "<img src=\"images/linear-trans-2.png\" width=\"250\">\n",
    "\n",
    "Where $\\vec{v}$ can also be written as:\n",
    "\n",
    "#### $$\\vec{v} = -1 \\hat{i} + 2 \\hat{j}$$\n",
    "\n",
    "If we then perform some transformation and watch where all of the vectors go, the property (of linear transformations) that all gridlines remain parallel and evenly spaced has a really important consequence!  \n",
    "\n",
    "<img src=\"images/linear-trans-3.png\" width=\"400\">\n",
    "\n",
    "The place where $\\vec{v}$ lands will be -1 times the vector where $\\hat{i}$ landed, and 2 times the vector where $\\hat{j}$ landed.\n",
    "\n",
    "#### $$\\text{Transformed } \\vec{v} = -1 (\\text{Transformed }\\hat{i}) + 2 (\\text{Transformed }\\hat{j})$$\n",
    "\n",
    "In other words, it started off as a certain _linear combination_ of $\\hat{i}$ and $\\hat{j}$, and it ended up as that _same linear combination_ of where those two vectors landed! This means that we can determine where $\\vec{v}$ must go, based only on where the two basis vectors land!\n",
    "\n",
    "Because we have a copy of our original gridlines in the background, we can see where the vectors landed:\n",
    "\n",
    "<img src=\"images/linear-trans-4.png\" width=\"500\">\n",
    "\n",
    "We see that our transformed $\\hat{i}$ and $\\hat{j}$ landed at:\n",
    "\n",
    "#### $$ \\text{Transformed }\\hat{i} = \n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    -2\n",
    "\\end{bmatrix} \\hspace{1cm}  \\hspace{1cm}\n",
    "\\text{Transformed }\\hat{j} = \n",
    "\\begin{bmatrix}\n",
    "    3 \\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Meaning that our transformed $\\vec{v}$ is:\n",
    "\n",
    "#### $$\\text{Transformed } \\vec{v} = -1 \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    -2\n",
    "\\end{bmatrix} + 2 \\begin{bmatrix}\n",
    "    3 \\\\\n",
    "    0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "#### $$\\text{Transformed } \\vec{v} = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Now, the cool thing about this is that we have just discovered a way to determine where _any_ transformed vector will land, only by knowing where $\\hat{i}$ and $\\hat{j}$ land, without needing to watch the transformation itself! We can write a vector with more general coordinates:\n",
    "\n",
    "#### $$\\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And it will land on $x$ times the vector where $\\hat{i}$ lands, and $y$ times the vector where $\\hat{j}$ lands:\n",
    "\n",
    "#### $$ \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y\n",
    "\\end{bmatrix} \\rightarrow x \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    -2\n",
    "\\end{bmatrix} + y \\begin{bmatrix}\n",
    "    3 \\\\\n",
    "    0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    1x + 3y\\\\\n",
    "    -2x + 0y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can now get to a very key point:\n",
    "\n",
    "> This is all to say that a 2-dimensional linear transformation is completely described by _just four numbers_: the 2 coordinates for where $\\hat{i}$ lands, and the 2 coordinates for where $\\hat{j}$ lands. It is common to package these coordinates in a 2x2 grid of numbers, commonly referred to as a 2x2 _**matrix**_:<br>\n",
    "<br>\n",
    "$$\\begin{bmatrix}\n",
    "    1  & 3\\\\\n",
    "    -2  & 0\n",
    "\\end{bmatrix}$$<br>\n",
    "Here you can interprete the columns as where $\\hat{i}$ lands and where $\\hat{j}$ lands!\n",
    "\n",
    "So, for just one more example, let's say we are given the 2x2 matrix representing a linear transformation:\n",
    "\n",
    "#### $$\n",
    "\\begin{bmatrix}\n",
    "    3  & 2\\\\\n",
    "    -2  & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And we are then given a random vector:\n",
    "\n",
    "#### $$\\begin{bmatrix}\n",
    "    5\\\\\n",
    "    7 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "If we want to determine where that linear transformation takes that vector, we can take the coordinates of the vector, multiply them by the corresponding columns of the matrix, and then add together what you get:\n",
    "\n",
    "#### $$\n",
    "5\\begin{bmatrix}\n",
    "    3 \\\\\n",
    "    -2\n",
    "\\end{bmatrix} +\n",
    "7\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## 2.1 The General Case\n",
    "Let's now try and generalize this as much as possible. Assume our 2x2 matrix (the typographical representation of our linear transformation of the vector space-in other words, it is a convenient way to package the information needed to describe a linear transformation), is:\n",
    "\n",
    "#### $$\n",
    "\\begin{bmatrix}\n",
    "    a  & b\\\\\n",
    "    c  & d\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "If we apply this transformation to some vector:\n",
    "\n",
    "#### $$ \\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "What do we end up with? Well it will be:\n",
    "\n",
    "#### $$\n",
    "x\\begin{bmatrix}\n",
    "    a \\\\\n",
    "    c\n",
    "\\end{bmatrix} +\n",
    "y\\begin{bmatrix}\n",
    "    b \\\\\n",
    "    d\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    ax + by\\\\\n",
    "    cx + dy\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Now, what we just did-which has been intuitive and clear (or I should at least hope)-is frequently taught _with no intuition whatsoever_. It is referred to as _**matrix vector multiplication**_, and would written as follows:\n",
    "\n",
    "#### $$\\begin{bmatrix}\n",
    "    a  & b\\\\\n",
    "    c  & d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y\n",
    "\\end{bmatrix} = \n",
    "x\\begin{bmatrix}\n",
    "    a \\\\\n",
    "    c\n",
    "\\end{bmatrix} +\n",
    "y\\begin{bmatrix}\n",
    "    b \\\\\n",
    "    d\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    ax + by\\\\\n",
    "    cx + dy\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Where our matrix (linear transformation, a form of a _function_) is on the left, and the vector is on the right. This, when taught without the necessary background is incredibly confusing and leaves us to simply utilize rote memorization of symbol manipulation processes. When we view this transformation as a type of function that takes in a vector as input and yields an output:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    7\n",
    "\\end{bmatrix} \\rightarrow \n",
    "L(\\vec{v})\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It begins to become far more intuitive _why the order matters_, and _why the top number in the vector is multiplied by the first column of the matrix_, and subsequently _why the bottom number in the vector is multiplied by the second column in the matrix_. It is simply due to:\n",
    "\n",
    "> 1. The way that vectors are, by convention, packaged and represented.\n",
    "2. The way that matrices are, by convention, packaged and represented. \n",
    "\n",
    "In order for the process to perform isomorphically to the geometric interpretation, we must follow the conventions that have been laid out. However, if we _only new the conventions_, while we did not have the underlying _intuition_, then there would be very little _meaning_ in what was going on. \n",
    "\n",
    "## 2.2 Matrix Multiplication\n",
    "We won't dig into it here, but as a quick note I wanted to touch on matrix multiplication.  If we had the following two matrices:\n",
    "\n",
    "#### $$\\begin{bmatrix}\n",
    "    a  & b\\\\\n",
    "    c  & d\n",
    "\\end{bmatrix}$$\n",
    "#### $$\\begin{bmatrix}\n",
    "    e  & f\\\\\n",
    "    g  & h\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And we wanted to the result of applying one, then the other, to a given vector:\n",
    "\n",
    "#### $$\\begin{bmatrix}\n",
    "    a  & b\\\\\n",
    "    c  & d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    e  & f\\\\\n",
    "    g  & h\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x  \\\\\n",
    "    y \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We can first thinking about this a a _**composition of functions**_. What this would look like from a standard function perspective is:\n",
    "\n",
    "#### $$f(g(x))$$\n",
    "\n",
    "Where we first run $x$ through function $g$, and then the result is run through function $f$. In our case, where the function is a matrix (linear transformation), we can say that the vector is first _transformed_ via the matrix:\n",
    "\n",
    "#### $$\n",
    "\\begin{bmatrix}\n",
    "    e  & f\\\\\n",
    "    g  & h\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Which would look like:\n",
    "\n",
    "#### $$\n",
    "\\begin{bmatrix}\n",
    "    e  & f\\\\\n",
    "    g  & h\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    x  \\\\\n",
    "    y \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    ex + fy\\\\\n",
    "    gx + hy\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And that resulting 2-d vector is then transformed via the second _linear transformation_:\n",
    "\n",
    "#### $$\n",
    "\\begin{bmatrix}\n",
    "    a  & b\\\\\n",
    "    c  & d\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    ex + fy\\\\\n",
    "    gx + hy\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    a(ex + fy) + b(gx + hy)\\\\\n",
    "    c(ex + fy) + d(gx + hy)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Resulting in a final 2-d vector! We can now easily understand what the following property of matrices exists:\n",
    "\n",
    "#### $$M_1M_2 \\neq M_2M_1$$\n",
    "\n",
    "We know that because a matrix represents a _transformation_ or _function_, that switching the order changes the order in which they are applied. In a more familiar case, that would mean:\n",
    "\n",
    "#### $$f(g(x)) \\rightarrow g(f(x))$$\n",
    "\n",
    "We know that the above is not true, and it follows that it is not true for matrices either! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 3. Nonsquare Matrices $\\rightarrow$ Transformations between dimensions\n",
    "Up until this point we have been dealing with transformations from 2-d vectors to other 2-d vectors, by the means of a 2x2 matrix. This is a point that is very crucial to understand, especially when dealing with machine learning (specifically deep neural networks). More importantly, it should help bring our generalization around to include some of the functions we are more familiar with. \n",
    "\n",
    "As we go through this, recall what was mentioned at the start of this notebook. There are functions that are dependent on several inputs, such as:\n",
    "\n",
    "#### $$f(x,y) = z$$\n",
    "Or:\n",
    "#### $$f(x_1,x_2, ...,x_n) = z$$\n",
    "\n",
    "Now, what is interesting about functions of that sort is that we start with _multiple dimensions_, and the result of our function process is a _one dimensional output_. Keep that in mind as we move forward. \n",
    "\n",
    "Okay, so we can start by saying that it is perfectly reasonable to talk about transformations between dimensions, such as:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    7\n",
    "\\end{bmatrix} \\rightarrow \n",
    "L(\\vec{v})\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "   1 \\\\\n",
    "   8 \\\\\n",
    "   2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Above, we have a 2-d input, and a 3-d output. In order to determine the transformation $L$, we just look at the coordinates of where the basis vectors land! For instance, it may be (left column is $\\hat{i}$, right column is $\\hat{j}$):\n",
    "\n",
    "#### $$L = \n",
    "\\begin{bmatrix}\n",
    "   2 & 0\\\\\n",
    "   -1 & 1 \\\\\n",
    "   -2 & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Now, the matrix above which encodes our transformation has 3 rows and 2 columns, making it a 3x2 matrix. We can intuitively say that a 3x2 matrix has a geometric interpretation of mapping 2 dimensions to 3 dimensions. This is because the 2 columns mean that the input space has 2 basis vectors, and the 3 rows indicate that the landing spots for each of those 3 basis vectors is described with 3 separate coordinates. \n",
    "\n",
    "Likewise, if we see a 2x3 matrix (2 rows, 3 columns):\n",
    "\n",
    "#### $$\\begin{bmatrix}\n",
    "   3 & 1 & 4\\\\\n",
    "   1 & 5 & 9 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We know that the 3 columns indicate that we are starting in a space that has 3 basis vectors, and the 2 rows indicate that the landing spot for each of those 3 basis vectors is described with only 2 coordinates. \n",
    "\n",
    "Now, we can complete the bridge from linear transformations to our standard functions. We have come across functions before of the form:\n",
    "\n",
    "#### $$f(x,y) = z$$\n",
    "\n",
    "Where we take a 2 dimensional input, and produce a single 1 dimensional output. Well, linear transformations are capable of the same thing! Below, we take a 2-d input, and produce a 1-d output:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    2 \\\\\n",
    "    7\n",
    "\\end{bmatrix} \\rightarrow \n",
    "L(\\vec{v})\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "   1.8\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "1-dimensional space is just the number line, so a transformation such as the one above essentially just takes in 2-d vectors, and spits out a single number. A transformation such as this is encoded as a 1x2 matrix, where each column has a single entry:\n",
    "\n",
    "#### $$L = \n",
    "\\begin{bmatrix}\n",
    "   1 & 2\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The 2 columns represent where each of the basis vectors land, and each column requires just one number; the number where that basis vector landed on. This is a very interesting concept, with ties to the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. The Dot Product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
