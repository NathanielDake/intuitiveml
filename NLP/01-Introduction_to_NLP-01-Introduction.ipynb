{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Natural Language Processing\n",
    "Natural Language Processing is certainly one of the most fascinating and exciting areas to be involved with at this point in time. It is a wonderful intersection of computer science, artificial intelligence, machine learning and linguistics. With the (somewhat) recent rise of Deep Learning, Natural Language Processing currently has a great deal of buzz surrounding it, and for good reason. The goal of this post is to do three things:\n",
    "\n",
    "1. Inspire the reader with the beauty of the problem of NLP\n",
    "2. Explain how machine learning techniques (i.e. something as simple as Logistic Regression) can be applied to text data.\n",
    "3. Prepare the reader for the next sections surrounding Deep Learning as it is applied to NLP.\n",
    "\n",
    "Before we dive in, I would like to share the poem _Jabberwocky_ by Lewis Carrol, and an accompanying excerpt from the book \"_Godel, Escher, Bach_\", by Douglas Hofstadter.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ROLVf2p6xYyTqQ3fmeky0eSD6ZCdfJ3M\" width=\"300\">\n",
    "\n",
    "And now, the corresponding excerpt, _**Translations of Jabberwocky**_. \n",
    "\n",
    "> ### Translations of Jabberwocky<br>\n",
    "Douglas R. Hofstadter\n",
    "Imagine native speakers of English, French, and German, all of whom have excellent command of their respective native languages, and all of whom enjoy wordplay in their own language. Would their symbol networks be similar on a local level, or on a global level? Or is it meaningful to ask such a question? The question becomes concrete when you look at the preceding translations of Lewis Carroll's famous \"Jabberwocky\".\n",
    "<br>\n",
    "<br>\n",
    "[The \"preceding translations\" were \"Jabberwocky\" (English, original), by Lewis Carroll, \"Le Jaseroque\", (French), by Frank L. Warrin, and \"Der Jammerwoch\" (German), by Robert Scott. --kl]\n",
    "<br>\n",
    "<br>\n",
    "I chose this example because it demonstrates, perhaps better than an example in ordinary prose, the problem of trying to find \"the same node\" in two different networks which are, on some level of analysis, extremely nonisomorphic. In ordinary language, the task of translation is more straightforward, since to each word or phrase in the original language, there can usually be found a corresponding word or phrase in the new language. By contrast, in a poem of this type, many \"words\" do not carry ordinary meaning, but act purely as exciters of nearby symbols. However, what is nearby in one language may be remote in another.\n",
    "<br>\n",
    "<br>\n",
    "Thus, in the brain of a native speaker of English, \"slithy\" probably activates such symbols as \"slimy\", \"slither\", \"slippery\", \"lithe\", and \"sly\", to varying extents. Does \"lubricilleux\" do the corresponding thing in the brain of a Frenchman? What indeed would be \"the corresponding thing\"? Would it be to activate symbols which are the ordinary translations of those words? What if there is no word, real or fabricated, which will accomplish that? Or what if a word does exist, but it is very intellectual-sounding and Latinate (\"lubricilleux\"), rather than earthy and Anglo-Saxon (\"slithy\")? Perhaps \"huilasse\" would be better than \"lubricilleux\"? Or does the Latin origin of the word \"lubricilleux\" not make itself felt to a speaker of French in the way that it would if it were an English word (\"lubricilious\", perhaps)?\n",
    "<br>\n",
    "<br>\n",
    "An interesting feature of the translation into French is the transposition into the present tense. To keep it in the past would make some unnatural turns of phrase necessary, and the present tense has a much fresher flavour in French than in the past. The translator sensed that this would be \"more appropriate\"--in some ill-defined yet compelling sense--and made the switch. Who can say whether remaining faithful to the English tense would have been better?\n",
    "<br>\n",
    "<br>\n",
    "In the German version, the droll phrase \"er an-zu-denken-fing\" occurs; it does not correspond to any English original. It is a playful reversal of words, whose flavour vaguely resembles that of the English phrase \"he out-to-ponder set\", if I may hazard a reverse translation. Most likely this funny turnabout of words was inspired by the similar playful reversal in the English of one line earlier: \"So rested he by the Tumtum tree\". It corresponds, yet doesn't correspond.\n",
    "<br>\n",
    "<br>\n",
    "Incidentally, why did the Tumtum tree get changed into an \"arbre Té-té\" in French? Figure it out for yourself.\n",
    "<br>\n",
    "<br>\n",
    "The word \"manxome\" in the original, whose \"x\" imbues it with many rich overtones, is weakly rendered in German by \"manchsam\", which back-translates into English as \"maniful\". The French \"manscant\" also lacks the manifold overtones of \"manxome\". There is no end to the interest of this kind of translation task.\n",
    "<br>\n",
    "<br>\n",
    "When confronted with such an example, one realizes that it is utterly impossible to make an exact translation. Yet even in this pathologically difficult case of translation, there seems to be some rough equivalence obtainable. Why is this so, if there really is no isomorphism between the brains of people who will read the different versions? The answer is that there is a kind of rough isomorphism, partly global, partly local, between the brains of all the readers of these three poems.\n",
    "\n",
    "\n",
    "Now, the purpose of sharing the above is because if you are reading these posts (and are anything like me), you may very well spend a large chunk of your time studying mathematics, computer science, machine learning, writing code, and so on. But, if you are new to NLP the appreciation for the beauty and deeper meaning surrounding language may not be on the forefront of your mind-that is understandable! But hopefully the passage and commentary above ignited some interest in the wonderfully complex and worthwhile problem of Natural Language Processing and Understanding.\n",
    "\n",
    "## 2. Spam Detection\n",
    "Now, especially at first, I don't want to dive into phonemes, morphemes, syntactical structure, and the like. We will leave those linguistic concepts for later on. The goal here is to quickly allow someone with an understanding of basic machine learning algorithms and techniques to implement them in the domain of NLP. \n",
    "\n",
    "We will see that, at least at first, a lot of NLP deals with preprocessing data, which allows us to use algorithms that we already know. The question that most definitely arises is: How do we take a bunch of documents which are basically a bunch of text, and feed them into other machine learning algorithms where the input is usually a vector of numbers? \n",
    "\n",
    "Well, before we even get to that, let's take a preprocessed data set from the [uci archive](https://archive.ics.uci.edu/ml/datasets/Spambase) and perform a simple classification on it. The data has been processed in such a way that we can consider columns 1-48 to the be the input, and column 49 to the be label (1 = spam, 0 = not spam). \n",
    "\n",
    "The input columns are considered the input, and they are a **word frequency measure**. This measure can be calculated via:\n",
    "\n",
    "$$\\text{Word Frequency Measure} = \\frac{\\text{# of times word appears in a document}}{\\text{Number of words in document}} * 100$$\n",
    "\n",
    "This will result in a **Document Term matrix**, which is a matrix where _terms_ (words that appeared in the document) go along the columns, and _documents_ (emails in this case) go along the rows:\n",
    "\n",
    "|       |word 1|word 2|word 3|word 4|word 5|word 6|word 7|word 8|\n",
    "|-------|------|------|------|------|------|------|------|------|\n",
    "|Email 1|||||||||\n",
    "|Email 2|||||||||\n",
    "|Email 3|||||||||\n",
    "|Email 4|||||||||\n",
    "|Email 5|||||||||\n",
    "\n",
    "### 2.1 Implementation in Code\n",
    "We will now use `Scikit Learn` to show that we can use _any_ model on NLP data, as long as it has been preprocessed correctly. First, let's use scikit learns `NaiveBayes` classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.40</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.44</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6 ...  0.40  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00 ...  0.00   \n",
       "\n",
       "    0.41  0.42  0.778   0.43   0.44  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/nlp/spambase.data')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifcation Rate for NB:  0.86\n"
     ]
    }
   ],
   "source": [
    "data = data.values\n",
    "np.random.shuffle(data)    # randomly split data into train and test sets\n",
    "\n",
    "X = data[:, :48]\n",
    "Y = data[:, -1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print (\"Classifcation Rate for NB: \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, a classification rate of 92%! Let's now look utilize `AdaBoost`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifcation Rate for Adaboost:  0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print (\"Classifcation Rate for Adaboost: \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, a nice improvement, but more importantly, we have shown that we can take text data and that via correct preprocessing we are able to utilize it with standard machine learning API's. The next step is to dig into _how_ basic preprocessing is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Sentiment Analysis\n",
    "To go through the basic preprocessing steps that are frequently used when performing machine learning on text data (often referred to an NLP pipeline) we are going to want to work on the problem of **sentiment analysis**. Sentiment is a measure of how positive or negative something is, and we are going to build a very simple sentiment analyzer to predict the sentiment of Amazon reviews. These are reviews, so they come with 5 star ratings, and we are going to look at the electronics category in particular. These are XML files, so we will need an XML parser. \n",
    "\n",
    "### 3.1 NLP Terminology \n",
    "Before we begin, I would just like to quickly go over some basic NLP terminology that will come up frequently throughout this post.\n",
    "* **Corpus**: Collection of text\n",
    "* **Tokens**: Words and punctuation that make up the corpus. \n",
    "* **Type**: a distinct token. Ex. \"Run, Lola Run\" has four tokens (comma counts as one) and 3 types.\n",
    "* **Vocabulary**: The set of all types. \n",
    "* The google corpus (collection of text) has 1 trillion tokens, and only 13 million types. English only has 1 million dictionary words, but the google corpus includes types such as \"www.facebook.com\". \n",
    "\n",
    "### 3.2 Problem Overview\n",
    "Now, we are just going to be looking at the electronics category. We could use the 5 star targets to do regression, but instead we will just do classification since they are already marked \"positive\" and \"negative\". As I mentioned, we are going to be working with XML data, so we will need an XML parser, for which we will use `BeautifulSoup`. We will only look at the `review_text` attribute. To create our feature vector, we will count up the number of occurences of each word, and divided it by the total number of words. However, for that to work we will need two passes through the data:\n",
    "\n",
    "1. One to collect the total number of distinct words, so that we know the size of our feature vector, in other words the vocabulary size, and possibly remove stop words like \"this\", \"is\", \"I\", \"to\", etc, to decrease the vocabulary size. The goal here is to know the index of each token\n",
    "2. On the second pass, we will be able to assign values to each data vector whose index corresponds to which words, and one to create data vectors \n",
    "\n",
    "Once we have that, it is simply a matter of creating a classifier like the one we did for our spam detector! Here, we will use logistic regression, so we can intepret the weights! For example, if you see a word like horrible and it has a weight of minus 1, it is associated with negative reviews. With that started, let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sentiment Analysis in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()                                # this turns words into their base form \n",
    "\n",
    "stopwords = set(w.rstrip() for w in open('../../data/nlp/stopwords.txt'))         # grab stop words \n",
    "\n",
    "# get pos reviews\n",
    "# only want rev text\n",
    "positive_reviews = BeautifulSoup(open('../../data/nlp/electronics/positive.review').read(), \"lxml\") \n",
    "positive_reviews = positive_reviews.findAll('review_text')                                  \n",
    "\n",
    "negative_reviews = BeautifulSoup(open('../../data/nlp/electronics/negative.review').read(), \"lxml\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Class Imbalance\n",
    "There are more positive than negative reviews, so we are going to shuffle the positive reviews and then cut off any extra that we may have so that they are both the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Tokenizer function\n",
    "Lets now create a tokenizer function that can be used on our specific reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)                        # essentially string.split()\n",
    "    tokens = [t for t in tokens if len(t) > 2]                     # get rid of short words\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]     # get words to base form\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Index each word\n",
    "We now need to create an index for each of the words, so that each word has an index in the final data vector. However, to able able to do that we need to know the size of the final data vector, and to be able to know that we need to know how big the vocabulary is. Remember, the **vocabulary** is just the set of all types!\n",
    "\n",
    "We are essentially going to look at every individual review, tokenize them, and then add those tokens 1 by 1 to the map if they do not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index_map = {}                            # our vocabulary - dictionary that will map words to dictionaries\n",
    "current_index = 0                              # counter increases whenever we see a new word\n",
    "\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "# --------- loop through positive reviews ---------\n",
    "for review in positive_reviews:              \n",
    "    tokens = my_tokenizer(review.text)          # converts single review into array of tokens (split function)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:                        # loops through array of tokens for specific review\n",
    "        if token not in word_index_map:                        # if the token is not in the map, add it\n",
    "            word_index_map[token] = current_index          \n",
    "            current_index += 1                                 # increment current index\n",
    "                \n",
    "# --------- loop through negative reviews ---------\n",
    "for review in negative_reviews:              \n",
    "    tokens = my_tokenizer(review.text)          \n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:                       \n",
    "        if token not in word_index_map:                        \n",
    "            word_index_map[token] = current_index          \n",
    "            current_index += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can actually take a look at the contents of `word_index_map` by making use of the `random` module (part of the Python Standard Library):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'law': 8207, 'carpal': 5952, '550+': 7956, 'esmartbuy': 5764, '88.3': 10919, 'excellence': 4350, 'plaster': 7215, '200mhz': 2520, 'videocamera': 5487, 'teamed': 1116, 'grievance': 10138, 'pc155': 5284, 'attenuated': 9677, 'roller': 5527, 'ping': 9778, 'steal': 4852, '512mb': 5169, 'begun': 6407, 'dlink': 8946, 'n70': 5452}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(dict(random.sample(word_index_map.items(), 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size 11088\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size', len(word_index_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Convert tokens into vector\n",
    "Now that we have our tokens and vocabulary, we need to convert our tokens into a vector. Because we are going to shuffle our train and test sets again, we are going to want to put labels and vector into same array for now since it makes it easier to shuffle. \n",
    "\n",
    "Note, this function operates on **one** review. So the +1 is creating our label, and this function is basically designed to take our input vector from an english form to a numeric vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    xy_data = np.zeros(len(word_index_map) + 1)          # equal to the vocab size + 1 for the label \n",
    "    for t in tokens:                                     # loop through every token\n",
    "        i = word_index_map[t]                            # get index from word index map\n",
    "        xy_data[i] += 1                                  # increment data at that index \n",
    "    xy_data = xy_data / xy_data.sum()                    # divide entire array by total, so they add to 1\n",
    "    xy_data[-1] = label                                  # set last element to label\n",
    "    return xy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to actually assign these tokens to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)               # total number of examples \n",
    "data = np.zeros((N, len(word_index_map) + 1))                       # N examples x vocab size + 1 for label\n",
    "i = 0                                                               # counter to keep track of sample\n",
    "\n",
    "for tokens in positive_tokenized:                                   # loop through postive tokenized reviews\n",
    "    xy = tokens_to_vector(tokens, 1)                                # passing in 1 because these are pos reviews\n",
    "    data[i,:] = xy                                                  # set data row to that of the input vector\n",
    "    i += 1                                                          # increment 1\n",
    "    \n",
    "for tokens in negative_tokenized:                                   \n",
    "    xy = tokens_to_vector(tokens, 0)                                \n",
    "    data[i,:] = xy                                                 \n",
    "    i += 1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 11089)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now 1000 rows of positively labeled reviews, followed by 1000 rows of negatively labeled reviews. We have `11089` columns, which is one more than our vocabulary size because we have a column for the label (positive or negative). Lets shuffle before getting our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Rate:  0.71\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification Rate: \", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Classification Rate\n",
    "We end up with a classification rate of 0.71, which is not ideal, but it is better than random guessing. \n",
    "\n",
    "### 3.3.6 Sentiment Analysis\n",
    "Something interesting that we can do is look at the weights of each word, to see if that word has positive or negative sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('price', 2.6933335314234945), ('easy', 1.6607386675647273), ('quality', 1.506317971058838), ('excellent', 1.3251060670385901), ('love', 1.1709408142202362), ('sound', 1.0464461627535124), ('perfect', 1.0428539601558817), ('memory', 0.9481894070592741), ('highly', 0.9284127485038839), ('fast', 0.9039820340656025), ('speaker', 0.9002561834234093), ('ha', 0.8647998524063103), ('little', 0.8627906327108551), ('you', 0.853071637748681), ('cable', 0.8026349629008833), ('lot', 0.745602002367273), ('pretty', 0.7353695361233098), ('week', -0.7039072766822804), ('month', -0.7151217604117455), ('time', -0.7403733187986121), ('tried', -0.7425915021555822), ('bad', -0.7762820930505366), ('support', -0.7928126295579041), ('returned', -0.8027859990541623), ('poor', -0.8200114239785323), ('buy', -0.8295009740538636), ('waste', -0.9238493092609887), ('item', -0.9985140274134781), ('then', -1.1357154577664963), ('money', -1.1944408261067214), ('return', -1.1957524522249907), ('doe', -1.34741851003303), ('wa', -1.5901863019516307), (\"n't\", -1.976871969101383)]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.7 \n",
    "large_magnitude_weights = []\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "      large_magnitude_weights.append((word, weight))\n",
    "\n",
    "def sort_by_magnitude(sentiment_dict):\n",
    "    return sentiment_dict[1]\n",
    "  \n",
    "large_magnitude_weights.sort(reverse=True, key=sort_by_magnitude)\n",
    "print(large_magnitude_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the above list is not perfect, _but_ it should give some insight on what is possible for us already. The logistic regression model was able to pick out `easy`, `quality`, and `excellent` as words that correlate to a positive response, and it was able to find `poor`, `returned`, and `waste` as words the correlate to a negative response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. NLTK Exploration \n",
    "Before we move on any further, I wanted to take a minute to go over a few of the most useful tools for the `nltk` (Natural Language Toolkit) library. This library will encapsulate many NLP tasks for us.\n",
    "\n",
    "### 4.1 Parts of Speech (POS) Tagging\n",
    "Parts of speech tagging is meant to do just what it sound like: tag each word with a given part of speech within a document. For example, in the following sentence:\n",
    "\n",
    "> \"Bob is great.\"\n",
    "\n",
    "`Bob` is a noun, `is` is a verb, and `great` is an adjective. We can utilize `nltk`'s POS tagger on that sentence and see the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bob', 'NNP'), ('is', 'VBZ'), ('great', 'JJ')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.pos_tag(\"Bob is great\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN'), ('learning', 'NN'), ('is', 'VBZ'), ('great', 'JJ')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"Machine learning is great\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second entry in the above tuples `NN`, `VBZ`, etc, represents the determined tag of the word. For a description of each tag, check out [this link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Stemming and Lemmatization\n",
    "Both the process of **stemming** and **lemmatization** are used in reducing words to a \"base\" form. This is very useful because a vocabulary can get very large, while certain words tend to have the same meaning. For example _dog_ and _dogs_, and _jump_ and _jumping_ both have similar meanings. The main difference between stemming and lemmatization is that stemming is a bit more basic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "print(porter_stemmer.stem('dogs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolv\n"
     ]
    }
   ],
   "source": [
    "print(porter_stemmer.stem('wolves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('dogs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('wolves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the stemmer and lemmatizer managed to get `dogs` correct, but only the lemmatizer managed to correctly convert `wolves` to base form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Named Entity Recognition \n",
    "Finally there is **Named Entity** recognition. Entities refer to nouns such as:\n",
    "* \"Albert Einstein\" - a person\n",
    "* \"Apple\" - an organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Albert', 'NNP'),\n",
       " ('Einstein', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('March', 'NNP'),\n",
       " ('14,', 'CD'),\n",
       " ('1879', 'CD')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Albert Einstein was born on March 14, 1879\"\n",
    "tags = nltk.pos_tag(s.split())\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Albert', 'NNP')]), Tree('PERSON', [('Einstein', 'NNP')]), ('was', 'VBD'), ('born', 'VBN'), ('on', 'IN'), ('March', 'NNP'), ('14,', 'CD'), ('1879', 'CD')])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Steve', 'NNP'), ('Jobs', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('CEO', 'NNP'), ('of', 'IN'), ('Apple', 'NNP'), ('Corp.', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "s = \"Steve Jobs was the CEO of Apple Corp.\"\n",
    "tags = nltk.pos_tag(s.split())\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Steve', 'NNP')]), Tree('PERSON', [('Jobs', 'NNP')]), ('was', 'VBD'), ('the', 'DT'), Tree('ORGANIZATION', [('CEO', 'NNP')]), ('of', 'IN'), Tree('ORGANIZATION', [('Apple', 'NNP')]), ('Corp.', 'NNP')])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Latent Semantic Analysis\n",
    "We will now take a moment to extend our semantic analysis example from before, instead now performing **Latent Semantic Analysis**. Latent semantic analysis is utilized to deal with the reality that we will often have _multiple_ words with the _same_ meaning, or on the other hand, _one_ word with _multiple_ meanings. These are referred to as _synonomy_ and _polysemy_ respectively. \n",
    "\n",
    "In the case of synonyms here are a few basic examples:\n",
    "* \"Buy\" and \"Purchase\"\n",
    "* \"Big\" and \"Large\"\n",
    "* \"Quick\" and \"Speedy\"\n",
    "\n",
    "And in the case of polysemes:\n",
    "* \"Man\" (man as in human, and man as in a male opposed to a female)\n",
    "* \"Milk\" (can be a noun or a verb)\n",
    "\n",
    "In order to solve this problem, we will need to introduce _Latent Variables_.\n",
    "\n",
    "## 5.1 Latent Variables\n",
    "The easiest way to get your head around latent variables at first is via an example. Consider the words \"computer\", \"laptop\", and \"PC\"; these words are most likely seen together very often, meaning they are highly correlated. We can thinking a _latent_ or _hidden_ variable that is below representing them all, and we can call that $z$. We can mathematically define $z$ as:\n",
    "\n",
    "$$z = 0.7*computer \\; + 0.5*PC \\; + 0.6*laptop$$\n",
    "\n",
    "So, we now have an idea of what a latent variable is, but what is the job of Latent Semantic Analysis? The entire goal of LSA is:\n",
    "\n",
    "1. To find the latent/hidden variables.\n",
    "2. Then, transform original data into these new variables. \n",
    "\n",
    "Ideally, after the above has been performed, the dimensionality of the new data will be much smaller than that of the original data set. It is important to note that LSA definitely helps solve the synonomy problem, by combining correlated variables. However, there are conflicting view points about whether or not it helps with polysemy. \n",
    "\n",
    "## 5.2 The Math Behind LSA\n",
    "As we just discussed, the main goal when applying LSA is to deal with synonyms. For example, \"small\" and \"little\" would each make up their own unique variable, but in reality we know that they mean the same thing, so that is redundant. We could combine them into a single variable, reducing the dimensionality of our data set by one. So, to be clear the goal of LSA is:\n",
    "\n",
    "> **Goal of LSA**: Reduce redundancy.\n",
    "\n",
    "### 5.2.1 Redundancy in Numbers\n",
    "Now, machine learning at its core is always dealing with numbers, so what exactly do I mean by redundancy from a numerical standpoint? Take a look at the the plot below:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1GK0COCtvumKXTI0nB0qHB_e696p2IE_J\" width=\"300\">\n",
    "\n",
    "We can see clearly that there is a linear relationship between the dependent and independent variable. In other words, there is a linear relationship between lean body mass and muscle strength. So, we could say that one of these variables is redundant; if we know someones lean body mass, we can accurately predict their muscle strength, and vice versa. If we want a compact representation of attributes related to someones athletic performance, then we may only need to know one of these variables, since the other can be predicted from it. This advantage becomes more apparent as our dimensionality grows; if we could go from 1 million variables down to variables, that is a 200,000x's savings of space! Saving space is good, and hence reducing redundancy is good! \n",
    "\n",
    "We are about ready to move onto the math portion, but before we do I want to preface by saying that the mathematics behind LSA involve a good deal of linear algebra, so I highly recommend brushing up (via my mathematics notes if you'd like) before continuing. With that said, LSA is essentially just the application of **Singular Value Decomposition** (SVD) to a term document matrix. \n",
    "\n",
    "Now, we can begin by gaining a brief bit of intuition behind what LSA may look like in code. As usual, we are going to begin with an input matrix `X` of shape $NxD$, where $N$ is the number of samples and $D$ is the number of features. This will be passed into scikit learns svd model, `TruncatedSVD`, call the `fit`, `transform` function, and finally receive an output matrix `Z` of shape $Nx2$, or $Nxd$, where $d << D$. \n",
    "\n",
    "```\n",
    "model = TruncatedSVD()\n",
    "model.fit(X)\n",
    "Z = model.transform(X)\n",
    "# equivalent: Z = model.fit_transform(X)\n",
    "```\n",
    "\n",
    "Since, by default, $d$ is 2 dimensional, we can make a scatter plot of all the points. Note, however, that the $d$ can be any value up to and including $D$. If it is $D$, then we have not reduced anything, but we still have changed the representation of the data in some way. \n",
    "\n",
    "At this point, we can dive into a bit of the math behind how SVD works. We know that one sample of our data is represented by a vector, which we can call $x$, which has a length of $D$. What is the simplest transformation that we can perform on this vector? Well, the simplest transformation that we can perform on a vector is a linear transformation. Simply multiple $x$ by a matrix $V$, and get a new vector $z$:\n",
    "\n",
    "$$z = V^T x$$\n",
    "\n",
    "So, SVD is all about how to find this matrix $V$. Note, the shapes of the matrices and vectors are above are:\n",
    "\n",
    "```\n",
    "shape(x) = D\n",
    "shape(V) = D x d\n",
    "shape(z) = d  # where d << D\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
