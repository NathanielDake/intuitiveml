{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Language Modeling and Neural Networks\n",
    "This post is about language modeling and its relation to neural networks. We will start with what may very well be the simplest task possible: creating a **bigram language model**. \n",
    "\n",
    "## 2.1 Bigram Language Model\n",
    "To start, in case it not clear, what is a language model? A language model is a model of the probabilities of sequences of words. In english, we refer to a sequence of words a sentence. So, for example, if we had the sentence:\n",
    "\n",
    "```\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "```\n",
    "\n",
    "A language model will allow us to calculate: \n",
    "\n",
    "$$p\\Big(\\text{The quick brown fox jumps over the lazy dog.}\\Big)$$\n",
    "\n",
    "The form that the above probability distribution takes is what makes up the model, and typically that is going to involve making some assumptions about the structure of language and how sentences are formed. \n",
    "\n",
    "### 2.1.1 What is a Model anyways?\n",
    "I want to take a moment to be very clear here and describe exactly what a model is. A model is trying to capture some real world phenomema (i.e. language, motion, finances, etc), and it will never be 100% correct. It is always going to make simplifying assumptions. The idea is that they will be correct most of the time, but some of the time they will be incorrect. For example, Newtonian Mechanics was determined to be incorrect (based on work by Einstein), but it still proves to be useful!\n",
    "\n",
    "### 2.1.2 What is a Bigram?\n",
    "A **bigram** is simply two consecutive words in a sentence. So, from our above example, the bigrams would be:\n",
    "\n",
    "```\n",
    "The quick\n",
    "quick brown\n",
    "brown fox\n",
    "fox jumps\n",
    "jump over\n",
    "over the\n",
    "the lazy\n",
    "lazy dog\n",
    "```\n",
    "\n",
    "We could also have **trigrams** and **n-grams**, which deal with three and $n$ consecutive words respectively. In terms of the bigram model, we are going to be modeling each bigram as a probability: \n",
    "\n",
    "$$Bigram \\; model: p\\big(w_t \\mid w_{t-1}\\big)$$\n",
    "\n",
    "So for example, we could have:\n",
    "\n",
    "$$p\\big(brown \\mid quick\\big) = 0.5$$\n",
    "\n",
    "$$p\\big(the \\mid the\\big) = 0$$\n",
    "\n",
    "In the above, all the statement is saying is: the probability of seeing the word `brown` given that we just saw the word `quick` is 0.5. Now, how do we actually find these probabilities? We just count! So, to find that $p\\big(brown \\mid quick\\big) = 0.5$, we would simply count up how many times $quick \\rightarrow brown$ appears in our documents, and how many times $brown$ appears, and then divide the former by the later. This will give us the **maximum likelihood probability**:\n",
    "\n",
    "$$p\\big(brown \\mid quick\\big) = \\frac{count(quick \\rightarrow brown)}{count(quick)}$$\n",
    "\n",
    "I would like to clarify what I mean when I refer to **documents**. Generally speaking, we are going to have some training data - a list of exmaple sentences - to create our model. For our purposes, we will mostly be using wikipedia, but in general documents just refer to a set of files that contains sentences. This sometimes will be called a **corpus**. \n",
    "\n",
    "### 2.1.3 What is a Language Model\n",
    "Returning to the idea of a language model, recall that we want to know the probability of an entire sentence. How can bigrams help us do that? As per our previous discussion, this is going to involve making some assumptions. Let's look at a simpler example, specifically the sentence: \n",
    "\n",
    "```\n",
    "I like dogs.\n",
    "```\n",
    "\n",
    "Our goal is to find $p\\big(I like dogs\\big)$. Well, we can apply the **rules of conditional probability** here. For a quick refresher, recall that the rules of conditional probability state that:\n",
    "\n",
    "$$p\\big(A \\mid B\\big) = \\frac{p \\big(A \\cap B \\big)}{p\\big(B\\big)}$$\n",
    "\n",
    "$$p \\big(A \\cap B \\big) = p\\big(A \\mid B\\big)  p\\big(B\\big)$$\n",
    "\n",
    "This can of course be extended to 3 variables like so:\n",
    "\n",
    "$$p \\big(A, B, C \\big) = p \\big(C \\mid A, B\\big)  p\\big(A, B\\big)$$\n",
    "\n",
    "Which, in the case of our sentence above would leave us with:\n",
    "\n",
    "$$p \\big(I, like, dogs \\big) = p \\big(dogs \\mid I, like\\big)  p\\big(I, like\\big)$$\n",
    "\n",
    "We can apply this rule of conditional probability yet again the joint probability $ p\\big(I, like\\big)$, resultin in:\n",
    "\n",
    "$$p \\big(I, like, dogs \\big) = p \\big(dogs \\mid I, like\\big)  p\\big(like \\mid I\\big) p \\big(I\\big)$$\n",
    "\n",
    "This could simply be continued if we had a longer sentence. This process encapsulates what is known as the **chain rule** of probability. So, why is the above important? Well, if we look at our resulting expression above, we can see that one of the probabilities is part of the bigram model:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<span style=\"color:#0000cc\">$p \\big(dogs \\mid I, like\\big)$</span>\n",
    "<span style=\"color:#ff0000\">$p\\big(like \\mid I\\big)$</span>\n",
    "<span style=\"color:#0000cc\">$p \\big(I\\big)$</span>\n",
    "</center>\n",
    "\n",
    "Now, the two other terms in blue are _not_ bigrams, but that is okay! We can still calculate them using maximum likelihood estimation. For the unigram $p(I)$, this is simply the number of times $I$ appears in the corpus, relative to the total corpus length:\n",
    "\n",
    "$$p\\big(I\\big) = \\frac{count(I)}{corpus \\; length }$$\n",
    "\n",
    "For the trigram $p \\big(dogs \\mid I, like\\big)$, we would need to perform the following counts:\n",
    "\n",
    "$$p \\big(dogs \\mid I, like\\big) = \\frac{count(I, like, dogs)}{count(I, like)}$$\n",
    "\n",
    "We can extend the above logic to sentences of any length. So, if we were dealing with a sentence:\n",
    "\n",
    "```\n",
    "A B C D E\n",
    "```\n",
    "\n",
    "We could model it as:\n",
    "\n",
    "$$p\\big( A, B, C, D, E\\big) = p\\big(E \\mid A, B, C, D\\big) p\\big( D \\mid A, B, C\\big) p\\big( C \\mid A, B\\big)\n",
    "p\\big( B \\mid A \\big) p\\big( A\\big)$$\n",
    "\n",
    "Note, above we are using commas to separate our words, which makes it look like a joint probability. However, we must keep in mind that we are looking at a sequence, and not simply a joint probability. With that said, what we should be taking away from the above equation is that modeling these $n$-grams will at some point become problematic. For example, if we return to our original sentence:\n",
    "\n",
    "```\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "```\n",
    "\n",
    "Perhaps this is the only sentence like this in our corpus! We know that: \n",
    "\n",
    "```\n",
    "The quick brown fox jumps over the lazy cat.\n",
    "```\n",
    "\n",
    "Is a valid and reasonable sentence. However, if it never shows up in our corpus, its maximum likelihood probability is 0. Zero is not an accurate probability in this case, since we _know_ that the sentence makes sense, and that our language model should allow for it. \n",
    "\n",
    "### 2.1.4 Add-One Smoothing\n",
    "One simple way to overcome these zero probabilities is to add a small number to each count, instead of performing vanilla maximum-likelihood counting. For instance, if we have a vocabulary size of $V$, our probability would look like:\n",
    "\n",
    "$$p_{smooth}\\big(B \\mid A\\big) = \\frac{count(A \\rightarrow B) + 1}{count(A) + V}$$\n",
    "\n",
    "We add $V$ to the denominator to ensure that our probabilities sum to one. This process ensures that even if a phrase does not appear in our corpus, it still has a small probability of occuring. \n",
    "\n",
    "### 2.1.5 The Markov Assumption\n",
    "Another thing we can do is make the **Markov Assumption**. This is that whatever you see _now_ depends only on what you saw in the previous step. Mathematically this looks like: \n",
    "\n",
    "$$p\\big( w_t \\mid w_{t-1}, w_{t-2}, ... , w_1 \\big) = p \\big(w_t \\mid w_{t-1}\\big)$$\n",
    "\n",
    "This is know as a **first order markov** because it only depends on one previous term. For example, in our previous situation when modeling `A B C D E` we ended up with:\n",
    "\n",
    "$$p\\big( A, B, C, D, E\\big) = p\\big(E \\mid A, B, C, D\\big) p\\big( D \\mid A, B, C\\big) p\\big( C \\mid A, B\\big)\n",
    "p\\big( B \\mid A \\big) p\\big( A\\big)$$\n",
    "\n",
    "If we made the markov assumption, the first term on the right would be reduced to:\n",
    "\n",
    "$$p\\big(E \\mid A, B, C, D\\big) = p\\big(E \\mid D\\big)$$\n",
    "\n",
    "The entire sentence would be reduced to:\n",
    "\n",
    "$$p\\big( A, B, C, D, E\\big) = p\\big(E \\mid  D\\big) p\\big( D \\mid  C\\big) p\\big( C \\mid B\\big)\n",
    "p\\big( B \\mid A \\big) p\\big( A\\big)$$\n",
    "\n",
    "We end up with a probability consisting entirely of bigrams and one unigram! Why is this important? Well, we need to keep in mind that the longer a sentence, the less likely it is to appear in our training data. This is because our training data makes up only a tiny fraction of the entire space of possible sentences. However, very short phrases like bigrams are going to be very common. So, while phrases such as:\n",
    "\n",
    "```\n",
    "The quick brown fox jumps over the lazy cat.\n",
    "\n",
    "The quick brown fox jumps over the lazy lizard.\n",
    "```\n",
    "\n",
    "Are not likely to appear in our corpus, phrases such as `lazy cat` and `lazy lizard` most likely do. Hence, it is easier to model the probability for `lazy lizard` than it is to model the probability for `The quick brown fox jumps over the lazy lizard.`. This in turn makes the full sentence much more probable. \n",
    "\n",
    "## 2.2 Creating a Bigram Language Model with NLTK\n",
    "We are about to create a bigram language model in code using NLTK, but before we do there are a few things to consider. First, we know that probabilities are always between 0 and 1, and that the full joint probability of our bigram model is just the multiplication of each bigram probability in the sentence:\n",
    "\n",
    "$$p\\big(w_1,...,w_T \\big) = p\\big(w_1\\big) \\prod_{t=2}^T p \\big( w_t \\mid w_{t-1}\\big)$$\n",
    "\n",
    "We also know that multiplying two numbers less than one together will always yield a smaller number. The result is that if we just keep multiplying probabilities together, we may encounter the **underflow** problem, which means that we hit the limit of numerical precision that our computer can handle, and it will just round down to 0. The solution to this is to use the **log probability** instead:\n",
    "\n",
    "$$log \\Big(p\\big(w_1,...,w_T \\big)\\big) = log \\Big(p\\big(w_1\\big)\\Big) \\sum_{t=2}^T log \\Big( p \\big( w_t \\mid w_{t-1}\\big) \\Big)$$\n",
    "\n",
    "We can use this because we know that the log function is **monotonically** increasing, so if $A > B$ then $log(A) > log(B)$. The other thing that we are going to want to do is **normalize** each sentence. Since probabilities are between 0 and 1, log probabilities are always negative. Hence, the longer our sentences, the more negative numbers we are going to add together. This means that if we compare raw log probabilities, there is always going to be a bias towards shorter sentences. Shorter sentences will always have a higher log probabilty, simply because they have fewer negative numbers to add together. For example:\n",
    "\n",
    "$$logp\\big( \\text{the the the} \\big) > logp\\big( \\text{A real, but much longer sentence than the one to left} \\big)$$\n",
    "\n",
    "To solve this, we can just compare the log probabilities, divided by the length of the sentence, $T$:\n",
    "\n",
    "$$\\frac{1}{T}logp\\big(w_1,...,w_T \\big) = \\frac{1}{T} \\Big[ logp\\big(w_1\\big)\\Big) \\sum_{t=2}^T logp \\big( w_t \\mid w_{t-1}\\big) \\Big)\\big]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Bigram Language Model in Code\n",
    "We will start with our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then write a few functions to load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "    \"\"\"Returns 57,430 sentences from the brown corpus. Each sentence is a list of individual string tokens.\"\"\"\n",
    "    return brown.sents()\n",
    "\n",
    "def get_sentences_with_word2idx():\n",
    "    \"\"\"Converts sentences from word representation to index representation.\n",
    "    \n",
    "    Assign a unique integer, starting from 0, to every word that appears in the corpus.\n",
    "    Returns a dictionary that contains a mapping from every word to its corresponding index.\"\"\"\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    \n",
    "    i = 2 \n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence: \n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    print('Vocabulary size: ', i)\n",
    "    return indexed_sentences, word2idx\n",
    "\n",
    "KEEP_WORDS = set([\n",
    "  'king', 'man', 'queen', 'woman',\n",
    "  'italy', 'rome', 'france', 'paris',\n",
    "  'london', 'britain', 'england',\n",
    "])\n",
    "\n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
    "    sentences = get_sentences()\n",
    "    indexed_sentences = []\n",
    "    \n",
    "    i = 2 \n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    \n",
    "    word_idx_count = {\n",
    "        0: float('inf'),\n",
    "        1: float('inf'),\n",
    "    }\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence: \n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "\n",
    "            # keep track of counts for later sorting\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "            \n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    \n",
    "    # ---- Restrict vocab size ----\n",
    "    # Set all the words that should be kept to infinity so that they are included when\n",
    "    # we pick the most common words\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "        \n",
    "    # Sort word counts dictionary by value, in descending order\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "#         print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "    \n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "        \n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start with our language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=1):\n",
    "    # Structure of bigram probability matrix will be:\n",
    "    # (last word, current word) -> probability\n",
    "    # Utilizing add-1 smoothing\n",
    "    bigram_probs = np.ones((V, V)) * smoothing\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                # Beginning word\n",
    "                bigram_probs[start_idx, sentence[i]] += 1\n",
    "            else:\n",
    "                # Middle word\n",
    "                bigram_probs[sentence[i-1], sentence[i]] += 1\n",
    "            if i == len(sentence) - 1:\n",
    "                # Final Word\n",
    "                # We update the bigram for last -> current\n",
    "                # AND current -> End otken\n",
    "                bigram_probs[sentence[i], end_idx] += 1\n",
    "    bigram_probs /= bigram_probs.sum(axis=1, keepdims=True)\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 10001\n",
      "REAL: when at last she could suffer the UNKNOWN no longer , nor face the girl's UNKNOWN , she said in a voice UNKNOWN : SCORE: -5.303214027073128\n",
      "FAKE: panic refer r yankees friction stained szold bake rico sought passions eleanor admiration cause aide charles keeping muzzle midst wtv transport pip's draws exert SCORE: -9.399468870608093\n",
      "Enter your own sentence:\n",
      "Hoping that this works\n",
      "SCORE: -7.237799525364752\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load in the data\n",
    "    # Note: sentences are already converted to sequences of word indexes\n",
    "    # Note: you can limit the vocab size if you run out of memory\n",
    "    sentences, word2idx = get_sentences_with_word2idx_limit_vocab(10000)\n",
    "\n",
    "    # Vocab size\n",
    "    V = len(word2idx)\n",
    "    print(\"Vocab size:\", V)\n",
    "\n",
    "    # Treat beginning of sentence and end of sentence as bigrams\n",
    "    # START -> first word\n",
    "    # last word -> END\n",
    "    start_idx = word2idx['START']\n",
    "    end_idx = word2idx['END']\n",
    "\n",
    "    # A matrix where:\n",
    "    # - row = last word\n",
    "    # - col = current word\n",
    "    # value at [row, col] = p(current word | last word)\n",
    "    bigram_probs = get_bigram_probs(sentences, V, start_idx, end_idx, smoothing=0.1)\n",
    "    \n",
    "    def get_score(sentence):\n",
    "        score = 0\n",
    "        for i in range(len(sentence)):\n",
    "            if i == 0:\n",
    "                # Beginning word\n",
    "                score += np.log(bigram_probs[start_idx, sentence[i]])\n",
    "            else:\n",
    "                # Middle word\n",
    "                score += np.log(bigram_probs[sentence[i-1], sentence[i]])\n",
    "        # Final word        \n",
    "        score += np.log(bigram_probs[sentence[-1], end_idx])\n",
    "        \n",
    "        return score / (len(sentence) + 1)\n",
    "    \n",
    "    # Map word indexes back to real words - helpful to display sentences\n",
    "    idx2word = dict((v, k) for k, v in word2idx.items())\n",
    "    def get_words(sentence):\n",
    "        return ' '.join(idx2word[i] for i in sentence)\n",
    "\n",
    "    # when we sample a fake sentence, we want to ensure not to sample start token or end token\n",
    "    sample_probs = np.ones(V)\n",
    "    sample_probs[start_idx] = 0\n",
    "    sample_probs[end_idx] = 0\n",
    "    sample_probs /= sample_probs.sum()\n",
    "    \n",
    "    # Test our model on real and fake sentences\n",
    "    while True:\n",
    "        # real sentence\n",
    "        real_idx = np.random.choice(len(sentences))\n",
    "        real = sentences[real_idx]\n",
    "\n",
    "        # fake sentence\n",
    "        fake = np.random.choice(V, size=len(real), p=sample_probs)\n",
    "\n",
    "        print(\"REAL:\", get_words(real), \"SCORE:\", get_score(real))\n",
    "        print(\"FAKE:\", get_words(fake), \"SCORE:\", get_score(fake))\n",
    "\n",
    "        # input your own sentence\n",
    "        custom = input(\"Enter your own sentence:\\n\")\n",
    "        custom = custom.lower().split()\n",
    "\n",
    "        # check that all tokens exist in word2idx (otherwise, we can't get score)\n",
    "        bad_sentence = False\n",
    "        for token in custom:\n",
    "            if token not in word2idx:\n",
    "                bad_sentence = True\n",
    "\n",
    "        if bad_sentence:\n",
    "            print(\"Sorry, you entered words that are not in the vocabulary\")\n",
    "        else:\n",
    "            # convert sentence into list of indexes\n",
    "            custom = [word2idx[token] for token in custom]\n",
    "            print(\"SCORE:\", get_score(custom))\n",
    "\n",
    "        cont = input(\"Continue? [Y/n]\")\n",
    "        if cont and cont.lower() in ('N', 'n'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Neural Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
