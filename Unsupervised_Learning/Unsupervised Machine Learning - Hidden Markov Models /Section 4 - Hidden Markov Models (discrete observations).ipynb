{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. From Markov Models to Hidden Markov Models\n",
    "We are now going to extend the basic idea of markov models to hidden markov models. We have talked about latent variables before, and they will be a very important concept as we move forward. They show up in **K-means clustering**, **Gaussian Mixture Models**, **principle components analysis**, and many other areas. With hidden markov models, it even shows up in the name, so you know that hidden (latent) variables are central to this model. \n",
    "\n",
    "The basic idea behind a latent variable is that there is something going on beyond what we can observe/measure. What we observe is generally stochastic/random, since if it were deterministic we could predict it without doing any machine learning at all. The assumption that we make when we assume there are latent or hidden variables is that there is some cause behind the scenes that is leading to the observations that we see. In hidden markov models, the hidden cause itself is stochastic-it is a random process, the markov chain. \n",
    "\n",
    "An example of this can be seen in genetics. As a human, we are just a physical manifestation of some biological code. Now that the code is readable, it is not hidden in the sense that we can't measure it, but there was a time when we couldn't. At that point, people would use HMM's to determine how genes map to actual physical attributes. \n",
    "\n",
    "Another example is speech to text. A computer isn't able to read the words you are attempting to say, but it can use an internal language model-i.e. a model of likely sequences of hidden states, to try and match those to the sounds that it hears. So, in this case what is observed are the sound signal, and the latent variables are just the sentence or phrase that you are saying. \n",
    "\n",
    "## 1.1 Markov $\\rightarrow$ Hidden Markov\n",
    "So, how do we go from markov models to hidden markov models? The simplest way to explain this is via an example. Suppose you are at a carnival and a magician has two biased coins that he is hiding behind his back. He will choose to flip one of the coins at random, and all you get to see is the result of the coin toss (H/T). So, what are the **hidden states** and what are the **observed variables**? \n",
    "\n",
    "* Since we can see the results of the coin toss, that means heads and tails are our *observed variables*. We can think of this as a vocabulary or space of possible observed values. \n",
    "* The **hidden states**, of course, are which coin the magician chose to flip. We can't see them, so they are hidden. This is called a stochastic or random process, since it is a sequence of random variables. \n",
    "\n",
    "## 1.2 Define an HMM\n",
    "How do we actually go about defining an HMM? Well, an HMM has 3 parts:\n",
    "> **$\\pi$, A, B**\n",
    "\n",
    "(Note that this is opposed to the regular markov model which just has $\\pi$ and $A$). $\\pi$ is the **initial state distribution**, or the probability of being in a state when the sequence begins. In our coin example, say the magician really likes coin 1, so the probability that he starts with coin 1 is 0.9.\n",
    "\n",
    "$$\\pi_i = 0.9$$\n",
    "\n",
    "$A$ is the state transition matrix, which tells us the probability of going from one state to another. \n",
    "\n",
    "$$A(i,j) = probability \\; of \\; going \\; to \\; state \\; j \\; from \\; state \\; i$$\n",
    "\n",
    "\n",
    "In hidden markov models, the states themselves are hidden, so $A$ corresponds from transitioning from one hidden state to another hidden state. In the coin example, suppose the magician is very figity, and the probability of transitioning from coin 1 to coin 2 is 0.9, and the probability of transitioning from 2 to 1 is 0.9. Then, the probability of staying with the same coin for either coin is 0.1.\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "    A_{11} & A_{12}\\\\\n",
    "    A_{21} & A_{22} \n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    0.1 & 0.9\\\\\n",
    "    0.9 & 0.1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The new variable here of course is $B$. This is the probability of observing some symbol given what state you are in. Note this also a matrix because it has two inputs. What state you are in, which is $j$, and what you observe, which is $k$. \n",
    "\n",
    "$$B(j,k) = probability \\; of \\;observing \\;symbol\\;k\\;while\\;you\\;are\\;in\\;state\\;j$$\n",
    "\n",
    "## 1.3 Indepence Assumptions\n",
    "In the HMM we are making more independence assumptions than just the markov assumption. Remember, the markov assumption is that the current state only depends on the previous state, but is independent of any state before the previous state. Now that we have both observed and hidden variables in our model, we have another independence assumption: \n",
    "\n",
    "> \"What we observe is only dependent on the current state\"\n",
    "\n",
    "So, the observation at time $t$, depends only on the state at time $t$, but not at any other time, state, or observation. \n",
    "\n",
    "## 1.4 What can we do with an HMM? \n",
    "So, what are we able to do with an HMM once we have one? Well, it will be similar to what we had discussed with regular markov models, with some additions. With markov models there were two main things we could do:\n",
    "> 1. **Get the probability of a sequence**. This was just the multiplication of each state transition probability, and the probability of the initial state.  \n",
    "2. **Train the model.** For this we just used maximum likelihood. That was just using frequency counts. \n",
    "\n",
    "With HMM's, we still have these two tasks, but both of these will be harder due to a more complex model. Training will most definitely be harder, because it not only requires the expectation maximization algorithm, but we will run into the limits of the numerical accuracy of the computer (limited accuracy of float). \n",
    "\n",
    "There is also one more task we will go over: *finding the most likely sequence of hidden states*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 2. HMM's are Doubly Embedded \n",
    "Let's now discuss how HMM's are doubly embedded stochastic processes. Why do we say that they are doubly embedded? Well, think of the inner most layer. This is already a markov model, which is a specific type of stochastic process. With regular markov models, that is all we need-you know the state, end of story. With hidden markov models, once we hit a state there is yet another random sample that must be drawn. Think about our magician example: once the magician chooses the coin (1 or 2) he still has to flip the coin. So, we pick a state and then we have another random variable whose value has to be observed. \n",
    "\n",
    "We can think of this as two layers:\n",
    "\n",
    "> * On the inner most layer, the state is chosen (choosing of coin). \n",
    "* On the outer layer, once the state is chosen a random variable is generated using the observation distribution for that state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "# 3. How can we chose the number of hidden states?\n",
    "The number of hidden states is a **hyperparameter**. In order to chose the number of hidden states, in general we would use *cross validation*. Well, if you think about, say we have $N$ training samples, and we then create a model with $N$ parameters. We could easily train this model to achieve 100% classication accuracy, however, this does not say anything about how the model will generalize to *unseen data*. Our goal will always be to fit to the trend, and not to the noise. If we can capture the real underlying trend, we should be able to make good predictions on new data. So, we will chose the number of hidden states that gives us the highest validation accuracy. We can use K-folds cross-validation. \n",
    "\n",
    "Generally that is all we would need to do when talking about hyperparameters. However, HMM's are a bit different. A lot of the time, the number of states in an HMM can reflect a real physical situation, or what we know about the situation we are trying to model-aka *priori knowledge*. For instance, in the magician example, we know the magician only has two coins, so we would use two states. When we are doing speech to text, we know the number of words in our vocabulary. In addition, we can separately train the hidden state transitions on pure text to give us a good initialization on the transition probabilities. Another example is biology-a codon is sequence of 3 DNA or RNA nucleotides, and these are responsible for creating amino acids which are then turned into proteins. A simple HMM may then have 3 physical states. So, we can use our knowledge of the physical system to help us determine the number of hidden states.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Forward-Backward Algorithm\n",
    "The first question that we can ask of our HMM is the simplest one: \n",
    "\n",
    ">\"What is the probability of a sequence?\"\n",
    "\n",
    "Suppose we have $M$ hidden states, and our sequence of observations is of length $T$. The idea is that we want to *marginalize* the joint probability over all possible values of the hidden states. \n",
    "\n",
    "So, we start with:\n",
    "\n",
    "$$p(x,z)$$\n",
    "\n",
    "Where both $x$ and $z$ are vectors:\n",
    "\n",
    "$$x = \\big[x(1), x(2), ..., x(T)\\big]$$\n",
    "$$z = \\big[z(1), z(2), ..., z(T)\\big]$$\n",
    "\n",
    "However, we want to be able to marginalize out $z$ and find:\n",
    "\n",
    "$$p\\big(x(1), x(2),...,x(T)\\big)$$\n",
    "\n",
    "The final equation we end up with is:\n",
    "\n",
    "$$p\\big(x(1), x(2),...,x(T)\\big) = \\sum_{z(1)=1..M,...,z(T)=1..M}\\pi\\big(z(1)\\big)p\\big(x(1)|z(1)\\big)\\prod_{t=2}^Tp\\big(z(t)|z(t-1)\\big)p\\big(x(t)|z(t)\\big)$$\n",
    "\n",
    "<br>\n",
    "Which when we break it down we see that we have **the probability of the initial state**: \n",
    "$$\\pi\\big(z(1)\\big)p\\big(x(1)|z(1)\\big)$$\n",
    "\n",
    "We have **A, the probability of going to state j from state i**:\n",
    "$$p\\big(z(t)|z(t-1)\\big)$$\n",
    "$$A(i,j) = p\\big(z(t)=j|z(t-1)=i\\big)$$\n",
    "\n",
    "And we have **B, the probability of seeing symbol k from state j**:\n",
    "$$p\\big(x(t)|z(t)\\big)$$\n",
    "$$B(j,k) = p\\big(x(t)=k|z(t)=j\\big)$$\n",
    "\n",
    "By performing our marginalization:\n",
    "\n",
    "$$\\sum_{z(1)=1..M,...,z(T)=1..M}$$\n",
    "\n",
    "We are essentially saying: \n",
    "> For the hidden variable at state 1, we want to look at *each potential value* of z. So in the case of the magician, at state 1, we would perform the calculation if coin 1 was used, z(1) and then add that the calculation if coin two was used, z(2). We would then perform this again from state 2, and all the way up through state $T$. This process of marginalization is based on the product rule of probability. \n",
    "\n",
    "Note, that when performing this marginalization we do _not_ need to do it for all values of $x(1)$, $x(2)$, and so on (see the notebook on marginalization). Why is that? Well, in our scenario $x(1)$, $x(2)$ and so on, will have been _observed_. This means that they are known quantities and the realm of probability and uncertainty no longer applies. So, when looking at our equation above:\n",
    "\n",
    "$$x = \\big[x(1), x(2), ..., x(T)\\big]$$\n",
    "\n",
    "We will know the values that $x(1)$, $x(2)$, and so on, took on during that sequence! For example, if $x$ represented heads or tails in a coin flip, we may see:\n",
    "\n",
    "$$x = \\big[x(1)=1, x(2)=0, ..., x(T)=1\\big]$$\n",
    "\n",
    "So, that means we are dealing with the marginalization case where we have a fixed variable(s) (in this case $x$) and are iterating over another variable ($Y$) that we wish to marginalize out:\n",
    "\n",
    "$$P(X=x) = \\sum_{y}p(X=x, Y=y)$$\n",
    "\n",
    "With that said, the question is, how long will this take to calculate?  Well, in the inner part we have a product which is $2T - 1$, which can be seen based on the first product:\n",
    "\n",
    "$$\\prod_{t=2}^Tp\\big(z(t)|z(t-1)\\big)p\\big(x(t)|z(t)\\big)$$\n",
    "\n",
    "Which is multipled by:\n",
    "\n",
    "$$\\pi\\big(z(1)\\big)p\\big(x(1)|z(1)\\big)$$\n",
    "\n",
    "Given us the second product. this occurs for a total of $T$ times, hence $2T$ products. We then subtract 1 from this based on where $T$ is initialized, leaving us with $2T -1$ products. How many times do we need to compute this product? This is equal to the number of possible state sequences, which is $M^T$. So in total that leaves us with $O(TM^T)$. This is exponential growth which is pretty bad, so we don't want to do this. A better way of doing this would be the forward backward algorithm. The main issue that is causing us so many problems is that we have a product inside of a sum. Normally, we can't simplify a product inside of a sum, but in this case we can factor the expression using the properties of probability to reduce the number of calculations we have to do. \n",
    "\n",
    "As a note, if you would like to see an example of the above calculation and the subsequent Forward-Backward algorithm, it can be found in the Appendix - HMM Calculations. \n",
    "\n",
    "## 4.1 Forward-Backword Algorithm Process\n",
    "So, how does the forward backward algorithm actually work? It utilizes _dynamic programming_ in order to reduce the computational complexity of our task. We need to define a variable called $\\alpha$:\n",
    "\n",
    "> _This is the forward variable, and it represents the joint probability of seeing the sequence you have observed up until now and being in a specific state at that time._\n",
    "\n",
    "$$\\alpha(t,i) = p\\big(x(1),...,x(t), z(t)=i\\big)$$\n",
    "\n",
    "We can see that there are two index's to $\\alpha$: time and $i$, which index's the state.\n",
    "\n",
    "#### 4.1.1 Step 1\n",
    "So, our first step is to calculate the initial value of $\\alpha$ (t = 1):\n",
    "\n",
    "$$\\alpha(1, i) = p \\big(x(1), z(1)=i\\big)$$\n",
    "\n",
    "Where if we recall the _Kolmogorov definition_ of conditional probability:\n",
    "\n",
    "$$P( A \\cap B ) = P(A \\mid B) P(B)$$\n",
    "\n",
    "We can extend that to our scenario:\n",
    "\n",
    "$$\\alpha(1, i) =  p\\big(z(1) = i \\big) p\\big(x(1) \\mid z(1)= i\\big)$$\n",
    "\n",
    "And we know that:\n",
    "\n",
    "$$p\\big(x(1) \\mid z(1)= i\\big) = B\\big(i, x(1)\\big)$$\n",
    "\n",
    "And that:\n",
    "\n",
    "$$p\\big(z(1) = i \\big) = \\pi_i$$\n",
    "\n",
    "Meaning we end up with:\n",
    "\n",
    "$$\\alpha(t,i) = \\pi_iB\\big(i, x(t)\\big)$$\n",
    "\n",
    "$$\\alpha(1,i) = \\pi_iB\\big(i, x(1)\\big)$$\n",
    "\n",
    "#### 4.1.2 Step 2\n",
    "The second step is called the **induction step**. This will be done for every state and every time up until $T$. \n",
    "\n",
    "$$\\alpha(t+1, j) = \\sum_{i=1}^M \\alpha(t,i) A(i,j)B(j, x(t+1))$$\n",
    "\n",
    "What this is doing is allowing us to update our forward variable, $\\alpha$. In other words, we continually update the joint probability of seeing the sequence you have observed up until now and being in a specific state at that time.\n",
    "\n",
    "#### 4.1.3 Step 3\n",
    "The final step is the termination step, where we marginalize over the hidden states at time $T$. \n",
    "\n",
    "$$p(x) = \\sum_{i=1}^M\\alpha(T,i) = \\sum_{i=1}^M p\\big(x(1),...,x(T),z(t)=i\\big)$$\n",
    "\n",
    "Notice that we already have our answer. We already know the probability of the sequence after only having done the forward step of the forward-backward algorithm. We can also show that the time complexity of this algorithm is $O(M^2T)$. \n",
    "\n",
    "## 4.2 Backward \n",
    "Now, at this point we do not need the backward algorithm (it is not needed to solve for $p(x)$, but we are going to use it later! It has two main steps, and it essentially just the reverse of the forward algorithm. \n",
    "\n",
    "To perform the backward algorithm, we will define a variable called $\\beta$, which is also indexed by time and the state. \n",
    "\n",
    "#### Initialization Step\n",
    "The initialization step is to define $\\beta$, at time $T$, to be 1 for every state:\n",
    "\n",
    "$$\\beta(T, i) = 1 $$\n",
    "\n",
    "$$\\beta(t, i) = p\\big(x(t+1), ... x(T) \\mid z(t)=i\\big)$$\n",
    "\n",
    "#### Induction Step\n",
    "The induction step is to then calculate the previous $\\beta$ for every state, similar to what we did with the forward algorithm: \n",
    "\n",
    "$$\\beta(t, i) = \\sum_{j=1}^M A(i,j)B\\big(j, x(t+1)\\big) \\beta(t+1, j)$$\n",
    "\n",
    "Again, we want to do this for all times down to 1 or 0, depending on how you index, and for every state at each time. \n",
    "\n",
    "## 4.3 Pseudocode\n",
    "\n",
    "```\n",
    "alpha = np.zeros((T, self.M))\n",
    "alpha[0] = self.pi * self.B[:, x[0]]\n",
    "for t in range(1, T):\n",
    "    alpha[t] = alpha[t-1].dot(self.A) * self.B[:, x[t]]\n",
    "P[n] = alpha[-1].sum()\n",
    "\n",
    "beta = np.zeros((T, self.m))\n",
    "beta[-1] = 1\n",
    "for t in range(T-2, -1, -1):\n",
    "    beta[t] = self.A.dot(self.B[:, x[t+1]] * beta[t+1])\n",
    "```\n",
    "\n",
    "We can see above that both $\\alpha$ and $\\beta$ are arrays of $TxM$, and notice how we have vectorized our operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4 Forward Algorithm Explanation\n",
    "The key idea behind the forward algorithm is that we are going to unroll the HMM in time. \n",
    "\n",
    "### 4.4.1 Sequence of Length 1\n",
    "First we can discuss what we do with a sequence of length 1. Remember, the goal is to determine the probability of the sequence. Well, that is just a simple probability problem. \n",
    "\n",
    "We have the following:\n",
    "\n",
    "> * $\\pi \\rightarrow$ The probability of the first state\n",
    "* $B \\rightarrow$ The probability of observing something given the state\n",
    "\n",
    "And we want to find:\n",
    "\n",
    "$$p\\big(x(1)\\big)$$\n",
    "\n",
    "So, we just marginalize over the states, $z$:\n",
    "\n",
    "$$p\\big(x(1)\\big) = p\\big(z(1)=1\\big) p\\big(x(1) \\mid z(1) =1 \\big) +...+p\\big(z(1)=M\\big) p\\big(x(1) \\mid z(1) =M \\big)$$\n",
    "\n",
    "$$p\\big(x(1)\\big) = \\pi_1 B \\big(1, x(1) \\big) + \\pi_2 B \\big(2, x(1) \\big)+ ... +\\pi_M B \\big(M, x(1) \\big)$$\n",
    "\n",
    "Visually, this looks like:\n",
    "\n",
    "<img src=\"images/forward-1.png\" width=\"350\">\n",
    "\n",
    "> * We go from the null, or _start_ position, to one of the states. That is $\\pi$. For this example, we will assume that the number of states, $M$, is 3.\n",
    "* We then go from that state to producing an observed variable. That is just:\n",
    "$\\pi$ times $B$ for the 3 states. \n",
    "\n",
    "This is our definition for the initial value of $\\alpha$:\n",
    "\n",
    "$$\\alpha(1, i) = p \\big(x(1), z(1)=i\\big)$$\n",
    "\n",
    "And we can define $\\alpha$ at rows 1, 2, and 3 respectively as:\n",
    "\n",
    "$$\\alpha(t=1, i=1) = p \\big(x(1), z(1)=1\\big) = \\pi_1 B \\big(1, x(1) \\big)$$\n",
    "\n",
    "$$\\alpha(t=1, i=2) = p \\big(x(1), z(1)=2\\big) = \\pi_2 B \\big(2, x(1) \\big)$$\n",
    "\n",
    "$$\\alpha(t=1, i=3) = p \\big(x(1), z(1)=3\\big) = \\pi_3 B \\big(3, x(1) \\big)$$\n",
    "\n",
    "### 4.4.2 Sequence of Length 2 $\\rightarrow$ Induction Step\n",
    "Now let's think about what we can do for a sequence of length 2. How would we find $p\\big( x(1), x(2)\\big)$, given we already have $p\\big(x(1)\\big)$? Remember, the observations are not directly dependent, and that each observation at a certain time, depends only on the state at that time. The states are Markov, so we can use the Markov assumption here. Let's try and find the probability of the second observation, $x(2)$, if the state is 1.\n",
    "\n",
    "The question we want to ask here is: \n",
    "\n",
    "> How can we get to state 1 at time t=2, seeing symbol $x(2)$? \n",
    "\n",
    "The answer is that we can come from any possible previous state! Visually, that looks like:\n",
    "\n",
    "<img src=\"images/forward-2.png\" width=\"350\">\n",
    "\n",
    "Since each of those transitions are independent, we can sum each of those distinct possibilities:\n",
    "\n",
    "$$\\pi_1A(1,1)B(1, x(2))+ \\pi_2A(2,1)B(1, x(2))+\\pi_3A(3,1)B(1, x(2))$$\n",
    "\n",
    "Where: \n",
    "\n",
    "> * $\\pi_1$ is the probability we start at state 1\n",
    "* $A(1,1)$ is the probabilty of transitioning from state 1 to state 1 \n",
    "* $B(1, x(2))$ is the probability of observing $x(2)$ while in state 1\n",
    "* We then add the probability that we came from state 2 and state 3\n",
    "\n",
    "Notice, if we include the $B$ for $t=1$, $B(i, x(1))$, this just gives us $\\alpha$:\n",
    "\n",
    "$$\\pi_1B \\big(1, x(1) \\big)A(1,1)B(1, x(2))+ \\pi_2B \\big(2, x(1) \\big)A(2,1)B(1, x(2))+\\pi_3B \\big(3, x(1) \\big)A(3,1)B(1, x(2))$$\n",
    "\n",
    "Hence, we can write the previous probability in terms of the previous $\\alpha$:\n",
    "\n",
    "$$\\alpha(t=0, i=1)A(1,1)B(1, x(2))+ \\alpha(t=0, i=2)A(2,1)B(1, x(2))+\\alpha(t=0, i=3)A(3,1)B(1, x(2))$$\n",
    "\n",
    "But wait-this is just the next $\\alpha$ at time $t=2$!\n",
    "\n",
    "$$\\alpha(t=2, i=1)$$\n",
    "\n",
    "That particular $\\alpha$, at time $t=2$ and state = 1, is the probability of observing $x(1)$ and observing $x(2)$ and being in the state 1 at time = 2:\n",
    "\n",
    "$$\\alpha(t=2, i=1) = p \\big(x(1), x(2), z(2)=1 \\big)$$\n",
    "\n",
    "So, we can see that $\\alpha$ is defined recursively! This particular $\\alpha$ we are showing for $t=2$ and state = 1, is the probability of observing $x(1)$ and $x(2)$, and being the state $z(2) = 1$ at $t=2$. \n",
    "\n",
    "Realize that this induction can be used for any subsequent time step. In other words, the next $\\alpha$ can always be defined in terms of the current alpha. The probability that this gives us is the probability of the observed sequence so far, and ending up in a particular state:\n",
    "\n",
    "$$\\alpha(t+1,i=1) = p \\big(x(1),...,x(t+1), z(t+1)=1\\big)$$\n",
    "\n",
    "### 4.4.3 Termination Step\n",
    "If we keep doing this process, eventually we will end up with $\\alpha(T,i)$, which is the probability of the entire sequence, and ending up in state $i$:\n",
    "\n",
    "$$\\alpha(T,i) = p \\big(x(1),...,x(T), z(T)=i\\big)$$\n",
    "\n",
    "Remember, our goal is to find just the probability of the sequence, so how can we do that? We can do the same thing that we did initially, it is just another probability problem. We marginalize over $z$, or in other words, sum the last $\\alpha$ over all $i$:\n",
    "\n",
    "$$p \\big(x(1),...,x(T)\\big) = \\sum_{i=1..M}\\alpha(T,i)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Viterbi Algorithm\n",
    "One question that we may have for our HMM, especially if our hidden states are modeled on some physical reality or an actual system, rather than just being arbitrary latent variables, is:\n",
    "\n",
    "> What is the sequence of hidden states, given the observation?\n",
    "\n",
    "For example, given a sound sample of someone speaking, what are the words that they are saying? This is what the **Viterbi Algorithm** calculates-the most probable hidden state sequence, given the observed sequence, under the current model. \n",
    "\n",
    "We will see that the viterbi algorithm works a lot like the forward algorithm we just went ever, expect that instead of just taking the sum, we just take the max. \n",
    "\n",
    "One extra thing we will need to add is _**backtracking**_, since we will need to keep track of the actual states, not just determine the final probability. \n",
    "\n",
    "## 5.1 Viterbi Algorithm Internals\n",
    "To do this we will create two new variables:\n",
    "\n",
    "> * $\\delta(t,i)$, which is indexed by time and state. This will represent the maximum probability of ending up in state $i$ at time $t$, which is a joint probability distribution over the state sequence and observed sequence. <br>\n",
    "<br>\n",
    "$$\\delta(t,i) = max \\Big \\{p\\big(z(1), z(t)=i, x(1),...,x(t) \\big) \\Big \\}$$\n",
    "<br>\n",
    "* $\\psi(t,i)$, which is also indexed by time and state. This will keep track of the actual state sequences that end up at time $t$ and in state $i$. <br>\n",
    "<br>\n",
    "$$\\psi(t,i)$$\n",
    "\n",
    "### 5.1.1 Step 1 - Initialization\n",
    "We again will have 3 steps, the first of which is initialization. It looks very much like the forward algorithm:\n",
    "\n",
    "$$\\delta(t,i) = \\pi_i B\\big(i, x(1)\\big)$$\n",
    "$$\\psi(1,i) = 0$$\n",
    "\n",
    "### 5.1.2 Step 2 - Recursion\n",
    "We then have the recursion step, which we will calculate for all times and all states, filling up the values in $\\delta$ and $\\psi$:\n",
    "\n",
    "$$\\delta(t, j) = max_{1 \\leq i \\leq M} \\big \\{ \\delta(t-1, i) A(i,j)\\big \\} B\\big(j, x(t) \\big)$$\n",
    "\n",
    "$$\\psi(t,j) = argmax_{1 \\leq i \\leq M} \\big \\{ \\delta(t-1, i) A(i,j)\\big \\}$$\n",
    "\n",
    "### 5.1.3 Step 3 - Termination Step\n",
    "Finally, we have the termination step, where we find the maximum probability:\n",
    "\n",
    "$$p^* = max_{1 \\leq i \\leq M} \\delta(T,i)$$\n",
    "\n",
    "For the best state sequence, in the final best state:\n",
    "\n",
    "$$z(T)^* argmax_{1 \\leq i \\leq M} \\delta(T, i)$$\n",
    "\n",
    "To determine the rest of the best state sequence, we just need to back track using $\\psi$. This is done for time equals $T-1$ all the way down to 1:\n",
    "\n",
    "$$z(T)^* = \\psi (t+1, z(t+1)^*)$$\n",
    "\n",
    "### 5.1.4 Pseudocode\n",
    "In pseudocode, it may look like:\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "delta = np.zeros((T, self.M))\n",
    "psi = np.zeros((T, self.M))\n",
    "delta[0] = self.pi*self.B[:, x[0]]\n",
    "for t in range(1, T):\n",
    "    for j in range(self.M):\n",
    "        delta[t,j] = np.max(delta[t-1] * self.A[:,j]) * self.B[j, x[t]]\n",
    "        psi[t,j] = np.argmax(delta[t-1] * self.A[:, j])\n",
    "        \n",
    "# Back Track\n",
    "states = np.zeros(T, dtype=np.int32)\n",
    "states[T-1] = np.argmax(delta[T-1])\n",
    "for t in range(T-2, -1, -1):\n",
    "    states[t] = psi[t+1, states[t+1]]\n",
    "return states\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 6. Viterbi Visualization\n",
    "The main difference between the viterbi algorithm and the forward algorithm, is that instead of taking the sum over all of the previous states that could have lead to the current one, we want to take the _max_. This is because we want to find the sequence that gives us the highest _observation probability_. \n",
    "\n",
    "## 6.1 First Step Visualization \n",
    "The first step is to consider just one observation, $x(1)$.\n",
    "\n",
    "<img src=\"images/forward-1.png\" width=\"350\">\n",
    "\n",
    "Since just one observation corresponds to just one state, then we just want to find the most probable single state. This is just maximum of $\\pi$ times $B$:\n",
    "\n",
    "$$max\\Big(\\pi_1B\\big(1, x(1)\\big), \\pi_2B\\big(2, x(1)\\big), \\pi_3B\\big(3, x(1)\\big) \\Big)$$\n",
    "\n",
    "This is just the probability of going to a state, and then observing what we observed from that state. Remember, we defined $\\pi B$ as $\\delta$, which is indexed by time and state:\n",
    "\n",
    "$$max \\Big(\\delta(t=1, 1), \\delta(t=1, 2), \\delta(t=1, 3)\\Big)$$\n",
    "\n",
    "## 6.2 Two Step Sequence $\\rightarrow$ Induction Step\n",
    "Let's now look at a two step sequence to get a better intuition for how our induction step works. We have the following visualization:\n",
    "\n",
    "<img src=\"images/viterbi-1.png\" width=\"350\">\n",
    "\n",
    "We will consider the following: a two step sequence, which we observe $x(1)$ and $x(2)$. Note, in our visualization $x(1)$ is not shown to allow things to state more compact. We can ask: how can we get to state 1 (the state at the top, $z_2 = 1$) at time $t=2$ from $t=1$?\n",
    "\n",
    "The answer is that we can get there from any other state! So the main idea is that we want to pick the one that gives us the maximum probability so far. How can we get that probability? That is the previous $\\delta$, which if we recall is $\\pi B$ at time $t=1$:\n",
    "\n",
    "$$\\delta(1,i)$$\n",
    "\n",
    "And then transitioning from that state to the current state (which we are currently assuming is 1):\n",
    "\n",
    "$$A(i,1)$$\n",
    "\n",
    "And then multiplying by the observation probability from this state:\n",
    "\n",
    "$$B\\big( 1, x(2)\\big)$$\n",
    "\n",
    "The _best_ path is whatever gives us:\n",
    "\n",
    "$$max \\Big(\\delta(1,i)A(i,1)B\\big( 1, x(2)\\big) \\Big)$$\n",
    "\n",
    "That max is clearly dependent on the previous state, $i$, so we are taking the max with respect to $i$. Note, the above equation is just the definition of the next $\\delta$!\n",
    "\n",
    "$$\\delta(2,1) = max \\Big(\\delta(1,i)A(i,1)B\\big( 1, x(2)\\big) \\Big)$$\n",
    "\n",
    "Hence, $\\delta$ is also defined recursively in terms of its previous value. \n",
    "\n",
    "In the end, what we we are looking for is the best state sequence, considering all the observed variables as a whole. So that means that looking at only the first observation, perhaps being in state 1 gives us the best total probability. But, if we considering both observations, being in state 2 and then transitioning to state 1 might give us the best total probability for the entire sequence. In that case, we should say that the state at time $t=1$ is 2, $z_1 = 2$.  \n",
    "\n",
    "Because we can only determine the best sequence at the very last $\\delta(T,i)$, we need to keep track of all possible state transitions that we encounter. Thus, we want to keep track of _ALL_ possible state transitions that we encountered. That is not exponential, because it doesn't consider all possible state transitions in total, just the ones that were giving us the best probabilities along the way. \n",
    "\n",
    "<img src=\"images/viterbi-2.png\" width=\"350\">\n",
    "\n",
    "### 6.3 Termination Step\n",
    "Since $\\delta(T,i)$ is the best probability for observing the whole sequence $x(1), ..., x(T)$ and ending up in state $i$, then the max of that will give us the best probability overall:\n",
    "\n",
    "$$max_i \\big \\{ \\delta(T, i)\\big \\} $$\n",
    "\n",
    "Furthermore, the argmax will give us the best last state! \n",
    "\n",
    "$$z(T)^* = argmax_i \\big \\{ \\delta(T,i)\\big \\}$$\n",
    "\n",
    "And since we were keeping track of all of the transitions, we know we got to the last state, $z(T)^*$, from the second last state, $z(T-1)^*$, and we know the third last state and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 7. Baum-Welch Algorithm\n",
    "We are now going to discuss the last of the three tasks that we can perform on an HMM, and it is the most critical:\n",
    "\n",
    "> How do we train an HMM?\n",
    "\n",
    "Similar to when we studied Gaussian Mixture Models, we have a latent variable where all of the possibilities need to be summed over, so we can't easily find the maximum likelihood solution. We need to use the expectation maximization algorithm, which is an iterative algorithm. \n",
    "\n",
    "The _**Baum-Welch**_ algorithm is named after the mathematicians who invented it, and it relies on the forward-backward algorithm that we learned about earlier. In fact, the first step to completing the Baum-Welch update, is to first compute the forward and backward variables, $\\alpha$ and $\\beta$. \n",
    "\n",
    "## 7.1 New Variable $\\rightarrow \\phi$\n",
    "Once we have those variables, we can compute a new quantity called $\\phi$. It has three indices, $t,i,j$:\n",
    "\n",
    "$$\\phi(t, i ,j) = p\\big(z(t)=i, z(t+1)=j \\mid x\\big)$$\n",
    "\n",
    "Which simply means _being in the state $i$ at time $t$, transitioning to state $j$ at time $t = 1$, given the observation sequence_. This can be seen visually below:\n",
    "\n",
    "<img src=\"images/baum-welch-1.png\" width=\"350\">\n",
    "\n",
    "The full definition of $\\phi$ is as follows:\n",
    "\n",
    "$$\\phi(t, i ,j) = \\frac{\\alpha(t,i)A(i,j)B\\big(j, x(t+1)\\big)\\beta(t+1, j)}{\\sum_{i=1}^M \\sum_{j=1}^M \\alpha(t,i)A(i,j)B\\big(j, x(t+1)\\big)\\beta(t+1, j)}$$\n",
    "\n",
    "## 7.2 Another New Variable $\\rightarrow \\gamma$\n",
    "Once we have $\\phi$ we can define another variable, $\\gamma$, which depends only on $t$ and $i$, and sum over $j$ which is all of the states:\n",
    "\n",
    "$$\\gamma(t,i) = \\sum_{j=1}^M \\phi(t,i,j)$$\n",
    "\n",
    "So, $\\gamma$ is just the $\\phi$ probability, marginalized over $j$. In other words:\n",
    "\n",
    "$$\\gamma(t,i) = p \\big(z(t) = i \\mid x\\big)$$\n",
    "\n",
    "## 7.3 Sum over time\n",
    "The key here is that when we sum these variables over all time, $\\gamma$ represents the expected number of transitions from state $i$:\n",
    "\n",
    "$$\\sum_{t=1}^{T-1}\\gamma(t,i) = E\\big( \\text{number of transitions from state i}\\big)$$\n",
    "\n",
    "And $\\phi$ represents the expected number of transitions from state $i$ to state $j$:\n",
    "\n",
    "$$\\sum_{t=1}^{T-1}\\phi(t,i,j) = E\\big( \\text{number of transitions from state i to state j}\\big)$$\n",
    "\n",
    "## 7.4 Update Equations\n",
    "So, we can define our update equations as follows; $\\pi$ is just:\n",
    "\n",
    "$$\\pi = \\gamma(1, i)$$\n",
    "\n",
    "$A(i,j)$ is just the expected number of transitions from $i$ to $j$, divided by the number of transitions from $i$:\n",
    "\n",
    "$$A(i,j) = \\frac{\\sum_{t=1}^{T-1} \\phi(t,i,j)}{\\sum_{t=1}^{T-1}\\gamma(t,i)}$$\n",
    "\n",
    "And $B(i,k)$ is just all the $\\gamma$'s if $x(t) = k$, divided by all the $\\gamma$'s:\n",
    "\n",
    "$$B(i,k) = \\frac{\\sum_{t=1}^{T-1} \\gamma(t,i) \\; if \\; x(t)=k, \\; else \\; 0}{\\sum_{t=1}^{T-1}\\gamma(t,i)}$$\n",
    "\n",
    "Keep in mind we are only considering one sequence at this point. Of course, we want to fit our model to all training sequences. We will discuss how to do that in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Baum-Welch Explanation and Intuition \n",
    "Let's now take a minute to dig into the mechanics of the Baum-Welch Algorithm. It can come across as rather confusing at first, but we can break it down into multiple steps so that it is easier to process. \n",
    "\n",
    "First and foremost, we need to keep in mind the following: so far we have been using predefined $A$, $B$, and $\\pi$ matrices. In reality we will most likely not have those to start with and will need to use a _learning process through training_ to find them. And that is where the Baum-Welch Algorithm comes into play. \n",
    "\n",
    "> The Baum-Welch algorithm is used to find the correct values for $A$, $B$, and $\\pi$.\n",
    "\n",
    "Secondly, we want to ensure that we understand the equations on a mechanical level. It is just multiplication, addition, and division. We define some new variables that represent some probabilities. The question can still be asked, _why_ do they represent those probabilities? UPDATE HERE. \n",
    "\n",
    "Thirdly, we want to be able to implement the equations in code. It is still just multiplication, addition, and division. We can code the sums using for loops, but it is better to vectorize the updates-either partially or fully if possible. \n",
    "\n",
    "Finally, we want to understand how Baum-Welch is derived. The fundamental principles are outside the scope of this course, but they depend on an algorithm called the expectation-maximization algorithm. The basic idea is that it is an iterative procedure that depends on two steps: the expecation step, and the maximization step. \n",
    "\n",
    "It may be helpful to first consider what would happen if the hidden states in the HMM were _not_ hidden. One example of this is parts of speech tagging in natural language processing. In parts of speech tagging you train on a dataset that contains sentences made up of sequences of words, and corresponding sequences of parts of speech tags. POS tags tell us the role of a word in a sentence. For example, noun, verb, adjective, and so on. You can see that based on the structure of the english language, there is a markovian probability model underlying the sequence of POS tags. When we make predictions, we would be given a new sentence with no POS tags, and then we would find the most likely hidden state sequence, and that would be the sequence of POS tags we predict. That is the Viterbi Algorithm! \n",
    "\n",
    "What is interesting about this problem, is that in the training problem the hidden states are not actually hidden. We know exactly what they are. Therefore, we can use maximum likelihood in closed form to determine all the state transitions, $A(i,j)$. For example, the probability of transitioning from noun to verb is the number of times we transition from noun to verb, divided by the number of times we had a noun:\n",
    "\n",
    "$$p(VERB \\mid NOUN) = \\frac{count(NOUN \\rightarrow VERB)}{count(NOUN)}$$\n",
    "\n",
    "We could similarly find observational probabilities. For example, the word \"milk\" can be a noun or a verb. To find the probability of the word _milk_ given NOUN, or the probability of the word _milk_ given VERB is easy, since we already have all that data in the training set. \n",
    "\n",
    "$$p(milk \\mid NOUN) = \\frac{count(milk \\; is \\; NOUN)}{count(NOUN)}$$\n",
    "\n",
    "## 8.1 HMM's in General\n",
    "Now the problem with HMM's in general is that the hidden states are _not_ known. Simply put, we don't know which Gaussian the data point came from. We need to define it in terms of probabilities.\n",
    "\n",
    "In Gaussian Mixture Models, \"which\" Gaussian is the hidden variable. If we knew which Gaussian, we wouldn't need Expectation Maximization. \n",
    "\n",
    "Hidden Markov Models are the same! Since we don't know \"which\" hidden states are the correct states, we define possible hidden states probabilistically. That is the _**expectation step**_. \n",
    "\n",
    "$$\\phi(t, i ,j) = p\\big(z(t)=i, z(t+1)=j \\mid x\\big)$$\n",
    "\n",
    "$$\\gamma(t,i) = p \\big(z(t) = i \\mid x\\big)$$\n",
    "\n",
    "The _**maximization step**_ is then finding the best $\\pi$, $A$, and $B$ given those probabilistically defined hidden states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Multiple Observations\n",
    "We are now going to discuss how to train an HMM when we have multiple observation sequences. Our training data isn't going to just be one sample, and we will want our model to accurately fit to all our training data as a whole. Usually, when we have $N$ samples of dimensionality $D$ we can put them in an $NxD$ numpy array or matrix. Now, when we are talking about sequences this becomes a problem.\n",
    "\n",
    "This can be seen by the following example; suppose we are looking at voice samples. A sample of someone saying: \n",
    "\n",
    "> \"Hello world\"\n",
    "\n",
    "Is going to be much shorter than someone saying:\n",
    "\n",
    "> \"I just saved 15% on my car insurance.\"\n",
    "\n",
    "The simplest way is to store each individual sample as an element in a python list. Inside the python list (of length $N$) we can have individual numpy arrays of any length. So, for now we can just call that $T(n)$, where $n=1..N$. Each of the $N$ sequences is going to have its own version of all the updating variables ($\\alpha, \\beta$, etc) that we have talked about earlier. So, we will have $p(n)$ to represent the probability of the sequence, and we will have $\\alpha(n)$ which will be the forward variable, and we will have $\\beta(n)$ which will represent the backward variable. At this point, it is actually more convenient to express our updates in terms of $\\alpha$ and $\\beta$, so we will forget about $\\phi$ and $\\gamma$ for now. \n",
    "\n",
    "## 9.1 Update Equations\n",
    "So, our update for $\\pi$ is just:\n",
    "\n",
    "$$\\pi_i = \\frac{1}{N} \\sum_{n=1}^N \\frac{\\alpha_n(1,i)\\beta_n(1,i)}{P(n)}$$\n",
    "\n",
    "And for $A(i,j)$:\n",
    "\n",
    "$$A(i,j) = \\frac{\\sum_{n=1}^N \\frac{1}{P(n)} \\sum_{t=1}^{T(n)-1} \\alpha_n(t,i)A(i,j)B\\big(j, x_n(t+1)\\big) \\beta_n(t+1, j)}{\\sum_{n=1}^N \\frac{1}{P(n)} \\sum_{t=1}^{T(n)-1} \\alpha_n(t,i) \\beta_n(t,i)}$$\n",
    "\n",
    "And the update for $B(j,k)$ is as follows:\n",
    "\n",
    "$$B(j,k) = \\frac{\\sum_{n=1}^N \\frac{1}{P(n)} \\sum_{t=1}^{T(n)} \\alpha_n(t,j) \\beta_n( t,j) \\; if \\; x_n(t) = k, \\; else \\; 0}{\\sum_{n=1}^N \\frac{1}{P(n)} \\sum_{t=1}^{T(n)} \\alpha_n(t,j) \\beta_n( t,j)}$$\n",
    "\n",
    "What is interesting about these equations is that each of the inner sums gets multiplied by the inverse of the probability of the observation. This tells us that under the current model, if some observation you would like to model is very improbable then we will give the updates based on that observation more weight. \n",
    "\n",
    "### 9.1.1 Pseudocode\n",
    "\n",
    "**$\\pi$ Update**\n",
    "\n",
    "```\n",
    "self.pi = np.sum((alphas[n][0] * betas[n][0]) / P[n] for n in range(N)) / N\n",
    "```\n",
    "\n",
    "**$A$ Update**\n",
    "\n",
    "```\n",
    "a_num = np.zeros((self.M, self.M))\n",
    "for n in range(N):\n",
    "    x = X[n]\n",
    "    T = len(x)\n",
    "    den1 += (alphas[n][:-1] * betas[n][:-1].sum(axis=0, keepdims=True).T / P[n])\n",
    "    a_num_n = np.zeros((self.M, self.M))\n",
    "    for i in range(self.M):\n",
    "        for j in range(self.M):\n",
    "            for t in range(T-1):\n",
    "                a_num_n[i,j] += alphas[n][t,i] * betas[n][t+1, j] * self.A[i,j] * self.B[j, x[t+1]]\n",
    "    \n",
    "    a_num += a_num_n / P[n]\n",
    "self.A = a_num / den1\n",
    "```\n",
    "\n",
    "For our $A$ update we can:\n",
    "\n",
    "* Initialize an $MxM$ matrix to represent the numerator. \n",
    "* Loop through all $N$ samples. \n",
    "* Then on the inside we loop through $i$ equals all states, $j$ equals all states, and $t$ equals all times except the last time. \n",
    "* This equation can be vectorized. \n",
    "* The denominator is already vectorized. \n",
    "\n",
    "**$B$ Update**\n",
    "\n",
    "```\n",
    "b_num = np.zeros((self.M, V))\n",
    "for n in range(N):\n",
    "    x = X[n]\n",
    "    T = len(x)\n",
    "    den2 += (alphas[n] * betas[n]).sum(axis=0, keepdims=True).T / P[n]\n",
    "    b_num_n = np.zeros((self.M, V))\n",
    "    for i in range(self.M):\n",
    "        for j in range(V):\n",
    "            for t in range(T):\n",
    "                if x[t] == j:\n",
    "                    b_num[i, j] += alphas[n][t][i] * betas[n][t][i]\n",
    "    b_num += b_num_n / P[n]\n",
    "self.B = b_num / den2\n",
    "```\n",
    "\n",
    "For our $B$ update:\n",
    "\n",
    "* Initialize a matrix for numerator, of size $MxV$\n",
    "* Only sum up the numerator if $X(t) = j$\n",
    "\n",
    "In the next section we are actually going to write some code and test it on multiple sequences. The data we are going to look at is a sequence of coin tosses (generated by an HMM). To do so we would implement the following:\n",
    "\n",
    "```\n",
    "# Define HMM variables\n",
    "symbol_map = ['H', 'T']\n",
    "pi = np.array([0.5, 0.5])\n",
    "A = np.array([[0.1, 0.9], [0.8, 0.2]])\n",
    "B = np.array([[0.6, 0.4], [0.3, 0.7]])\n",
    "M, V = B.shape\n",
    "\n",
    "# Function to generate sequences of length N\n",
    "def generate_sequence(N):\n",
    "    s = np.random.choice(range(M), p=pi) # Initial state\n",
    "    x = np.random.choice(range(V), p=B[s]) # Initial observation\n",
    "    sequence = [x]\n",
    "    for n in range(N-1):\n",
    "        s = np.random.choice(range(M), p=A[s]) # Next state\n",
    "        x = np.random.choice(range(V), p=B[s]) # Next observation\n",
    "        sequence.append(x)\n",
    "    return sequence\n",
    "```\n",
    "\n",
    "Note that just like the regular markov model, you first randomly choose the state. Once you have randomly chosen the state, you randomly choose the observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 10. Discrete Hidden Markov Models in Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_normalized(d1, d2):\n",
    "  \"\"\"Create random Markov Matrix, d1xd2.\"\"\"\n",
    "  x = np.random.random((d1, d2))\n",
    "  return x / x.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMM:\n",
    "  \"\"\"Define our HMM Class.\"\"\"\n",
    "  def __init__(self, M):\n",
    "    self.M = M\n",
    "    \n",
    "  def fit(self, X, max_iter=30): \n",
    "    \"\"\"max_iter controls how many iterations of expectation maximization we will do.\"\"\"\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # Get vocabulary size. Want to make sure input observations are numbered 0: V-1\n",
    "    V = max(max(x) for x in X) + 1\n",
    "    # Get Number of sequences\n",
    "    N = len(X)\n",
    "    \n",
    "    self.pi = np.ones(self.M) / self.M # Uniform Distribution\n",
    "    self.A = random_normalized(self.M, self.M)\n",
    "    self.B = random_normalized(self.M, V)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for line in open('data/coin_data.txt'):\n",
    "    # 1 for H, 0 for T\n",
    "    x = [1 if e == 'H' else 0 for e in line.rstrip()]\n",
    "    X.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = [max(x) for x in X]\n",
    "print(max(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
