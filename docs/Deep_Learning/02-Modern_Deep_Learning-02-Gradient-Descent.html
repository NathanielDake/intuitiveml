
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.6" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="9.-Gradient-Descent:-Full-vs.-Batch-vs.-Stochastic">9. Gradient Descent: Full vs. Batch vs. Stochastic<a class="anchor-link" href="#9.-Gradient-Descent:-Full-vs.-Batch-vs.-Stochastic">&#182;</a></h1><p>Let's take a moment to talk about the different ways that we can perform gradient descent, and their particular advantages and disadvantages. The three basic types we will go over are:</p>
<ol>
<li><strong>Full</strong></li>
<li><strong>Batch</strong></li>
<li><strong>Stochastic</strong></li>
</ol>
<h2 id="1.1-Full-Gradient-Descent">1.1 Full Gradient Descent<a class="anchor-link" href="#1.1-Full-Gradient-Descent">&#182;</a></h2><p>Up until now, most of my notebooks have covered <strong>full</strong> gradient descent:</p>
$$J = \sum_{n=1}^N t(n)logy(n)$$<p>Why is that? Well it makes sense that we would want to maximize the likelihood over our entire training set! The main disadvantage of this however is that the calculation of the gradient is now <strong>O(N)</strong>, since it depends on each sample. This means that it will struggle to be effective when working with big data.</p>
<h2 id="1.2-Stochastic-Gradient-Descent">1.2 Stochastic Gradient Descent<a class="anchor-link" href="#1.2-Stochastic-Gradient-Descent">&#182;</a></h2><p>On the other end of the spectrum we have <strong>stochastic gradient descent</strong>. This looks at one particular sample at a time, and the associated error. We are depending on the fact that all of the samples are <strong>IID (independent and identically distributed)</strong>. This means that in the long run your error will improve because all of your samples are coming from the same distribution. Now that we have reduced our calculation from $N$ operations to 1, that is a nice improvement, however, there is a disadvantage. Whereas the log likelihood always improves on every iteration of full gradient descent, sometimes the log likelihood can get worse with stochastic gradient descent. In fact, the cost function will behave pretty erratically over each iteration, but it will improve in the long run.</p>
<h2 id="1.3-Batch-Gradient-Descent">1.3 Batch Gradient Descent<a class="anchor-link" href="#1.3-Batch-Gradient-Descent">&#182;</a></h2><p>Batch gradient descent can be thought of as the happy medium between these two. In this method we split our data up into batches. For instance, say we have 10,000 examples, we could split it up into 100 batches of size 100. Then we could compute our cost function based on each batch for each iteration. In this case the cost function can still get worse, but it will be much less erratic than if we had done pure stochastic gradient descent.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="2.-Full-vs.-Batch-vs.-Stochastic-in-Code">2. Full vs. Batch vs. Stochastic in Code<a class="anchor-link" href="#2.-Full-vs.-Batch-vs.-Stochastic-in-Code">&#182;</a></h1><p>We are now going to compare each form of gradient descent in code, particularly how they progress. We start with the standard imports:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="k">import</span> <span class="n">datetime</span>

<span class="kn">from</span> <span class="nn">modern_dl_util</span> <span class="k">import</span> <span class="n">get_transformed_data</span><span class="p">,</span> <span class="n">forward</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">gradW</span><span class="p">,</span> <span class="n">gradb</span><span class="p">,</span> <span class="n">y2indicator</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will be using the transformed data (after PCA has been applied). We will also be using logistic regression instead of a full neural network, just to speed things up. So we will start by getting the transformed data, and only take the first 300 columns. We will also normalize X, get our training and test set, and create our indicator matrices.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_transformed_data</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">300</span><span class="p">]</span>

<span class="c1"># normalize X first</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>

<span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">,]</span>
<span class="n">Ytrain</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="o">-</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">Xtest</span>  <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:,]</span>
<span class="n">Ytest</span>  <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="o">-</span><span class="mi">1000</span><span class="p">:]</span>

<span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">Xtrain</span><span class="o">.</span><span class="n">shape</span>
<span class="n">Ytrain_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytrain</span><span class="p">)</span>
<span class="n">Ytest_ind</span> <span class="o">=</span> <span class="n">y2indicator</span><span class="p">(</span><span class="n">Ytest</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;reached&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Reading in and transforming data...
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>reached
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2-Full-Gradient-Descent">2.2 Full Gradient Descent<a class="anchor-link" href="#2.2-Full-Gradient-Descent">&#182;</a></h2><p>Now we will perform full gradient descent.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 1. full</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="mi">28</span>      <span class="c1"># initialize weights and bias</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">LL</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>                          <span class="c1"># set learning rate and regularization</span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>      
    <span class="n">p_y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>    <span class="c1"># forward pass          </span>
    
    <span class="c1"># weight and bias update using gradient</span>
    <span class="n">W</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">gradW</span><span class="p">(</span><span class="n">Ytrain_ind</span><span class="p">,</span> <span class="n">p_y</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">)</span> <span class="o">-</span> <span class="n">reg</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">gradb</span><span class="p">(</span><span class="n">Ytrain_ind</span><span class="p">,</span> <span class="n">p_y</span><span class="p">)</span> <span class="o">-</span> <span class="n">reg</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>


    <span class="n">p_y_test</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>    <span class="c1"># forward pass on test set</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">p_y_test</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
    <span class="n">LL</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">err</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">p_y_test</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration </span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ll</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="n">p_y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>                                    <span class="c1"># final prediction</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">p_y</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>            <span class="c1"># print accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsted time for full GD:&quot;</span><span class="p">,</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>      <span class="c1"># print current time </span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cost at iteration 0: 660.291362
Error rate: 0.15
Cost at iteration 20: 348.686029
Error rate: 0.098
Cost at iteration 40: 331.380188
Error rate: 0.095
Cost at iteration 60: 325.034368
Error rate: 0.096
Cost at iteration 80: 321.800822
Error rate: 0.095
Cost at iteration 100: 319.894373
Error rate: 0.094
Cost at iteration 120: 318.674439
Error rate: 0.094
Cost at iteration 140: 317.850281
Error rate: 0.095
Cost at iteration 160: 317.270303
Error rate: 0.095
Cost at iteration 180: 316.848130
Error rate: 0.094
Final error rate: 0.094
Elapsted time for full GD: 0:00:43.549665
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.3-Stochastic-Gradient-Descent">2.3 Stochastic Gradient Descent<a class="anchor-link" href="#2.3-Stochastic-Gradient-Descent">&#182;</a></h2><p>Now let's go over batch gradient descent. Note that we are only going to make 1 pass through the data since we are going to need to look at every sample and make an update based on every sample individually. We also are only going to go through 500 samples, since it will be very slow to go through all of them.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="mi">28</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">LL_stochastic</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="c1"># takes very long since we&#39;re computing cost for 41k samples</span>
    <span class="n">tmpX</span><span class="p">,</span> <span class="n">tmpY</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain_ind</span><span class="p">)</span>     <span class="c1"># want to shuffle through the labels</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">500</span><span class="p">)):</span> <span class="c1"># shortcut so it won&#39;t take so long...</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tmpX</span><span class="p">[</span><span class="n">n</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>   <span class="c1"># reshape x in a 2d matrix</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tmpY</span><span class="p">[</span><span class="n">n</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># reshape y</span>
        <span class="n">p_y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>       <span class="c1"># forward pass</span>

        <span class="c1"># gradient weight updates, with regularization as usual </span>
        <span class="n">W</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">gradW</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p_y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">reg</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">gradb</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p_y</span><span class="p">)</span> <span class="o">-</span> <span class="n">reg</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>

        <span class="n">p_y_test</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">p_y_test</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
        <span class="n">LL_stochastic</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
        
        <span class="c1"># only calculate error rate once every N divided by 2 samples, will go very slow</span>
        <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">err</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">p_y_test</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration </span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ll</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="n">p_y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">p_y</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsted time for SGD:&quot;</span><span class="p">,</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cost at iteration 0: 2469.713765
Error rate: 0.896
Final error rate: 0.892
Elapsted time for SGD: 0:00:00.492293
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.4-Batch-Gradient-Descent">2.4 Batch Gradient Descent<a class="anchor-link" href="#2.4-Batch-Gradient-Descent">&#182;</a></h2><p>The main question that usually comes up here is: "how do you select the batch?". That is done by selecting the rows from the iteration number, times the batch size, all the way to the iteration number times the batch size, plus the batch size</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 3. batch</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="mi">28</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">LL_batch</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">reg</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">batch_sz</span> <span class="o">=</span> <span class="mi">500</span>                  <span class="c1"># set batch size to 500</span>
<span class="n">n_batches</span> <span class="o">=</span> <span class="n">N</span> <span class="o">//</span> <span class="n">batch_sz</span>       <span class="c1"># get number of batches</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">tmpX</span><span class="p">,</span> <span class="n">tmpY</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain_ind</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>
        
        <span class="c1"># get the current batch</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tmpX</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),:]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">tmpY</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span><span class="p">:(</span><span class="n">j</span><span class="o">*</span><span class="n">batch_sz</span> <span class="o">+</span> <span class="n">batch_sz</span><span class="p">),:]</span>
        <span class="n">p_y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="c1"># gradient descent with regularization </span>
        <span class="n">W</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">gradW</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p_y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">reg</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">gradb</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p_y</span><span class="p">)</span> <span class="o">-</span> <span class="n">reg</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>

        <span class="n">p_y_test</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">p_y_test</span><span class="p">,</span> <span class="n">Ytest_ind</span><span class="p">)</span>
        <span class="n">LL_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ll</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_batches</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">err</span> <span class="o">=</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">p_y_test</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost at iteration </span><span class="si">%d</span><span class="s2">: </span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ll</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="n">p_y</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final error rate:&quot;</span><span class="p">,</span> <span class="n">error_rate</span><span class="p">(</span><span class="n">p_y</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Elapsted time for batch GD:&quot;</span><span class="p">,</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Cost at iteration 0: 2478.670826
Error rate: 0.903
Cost at iteration 1: 931.761701
Error rate: 0.185
Cost at iteration 2: 651.563247
Error rate: 0.139
Cost at iteration 3: 550.479113
Error rate: 0.125
Cost at iteration 4: 497.291386
Error rate: 0.122
Cost at iteration 5: 464.399765
Error rate: 0.118
Cost at iteration 6: 441.475866
Error rate: 0.111
Cost at iteration 7: 424.573953
Error rate: 0.107
Cost at iteration 8: 411.676856
Error rate: 0.108
Cost at iteration 9: 401.463488
Error rate: 0.105
Cost at iteration 10: 393.640566
Error rate: 0.102
Cost at iteration 11: 386.687257
Error rate: 0.101
Cost at iteration 12: 381.008868
Error rate: 0.101
Cost at iteration 13: 375.996665
Error rate: 0.1
Cost at iteration 14: 371.588356
Error rate: 0.1
Cost at iteration 15: 367.853288
Error rate: 0.097
Cost at iteration 16: 364.342495
Error rate: 0.098
Cost at iteration 17: 361.470327
Error rate: 0.098
Cost at iteration 18: 358.944314
Error rate: 0.098
Cost at iteration 19: 356.286892
Error rate: 0.098
Cost at iteration 20: 354.112926
Error rate: 0.098
Cost at iteration 21: 352.162839
Error rate: 0.097
Cost at iteration 22: 350.375008
Error rate: 0.098
Cost at iteration 23: 348.609629
Error rate: 0.097
Cost at iteration 24: 347.225688
Error rate: 0.097
Cost at iteration 25: 345.977963
Error rate: 0.097
Cost at iteration 26: 344.548905
Error rate: 0.096
Cost at iteration 27: 343.470726
Error rate: 0.095
Cost at iteration 28: 342.132727
Error rate: 0.095
Cost at iteration 29: 341.295308
Error rate: 0.095
Cost at iteration 30: 340.128598
Error rate: 0.096
Cost at iteration 31: 339.295293
Error rate: 0.096
Cost at iteration 32: 338.430341
Error rate: 0.096
Cost at iteration 33: 337.569819
Error rate: 0.094
Cost at iteration 34: 336.849058
Error rate: 0.095
Cost at iteration 35: 336.431914
Error rate: 0.095
Cost at iteration 36: 335.619563
Error rate: 0.096
Cost at iteration 37: 334.980167
Error rate: 0.096
Cost at iteration 38: 334.431145
Error rate: 0.096
Cost at iteration 39: 333.850203
Error rate: 0.095
Cost at iteration 40: 333.245296
Error rate: 0.097
Cost at iteration 41: 332.633111
Error rate: 0.096
Cost at iteration 42: 332.147808
Error rate: 0.095
Cost at iteration 43: 331.885392
Error rate: 0.095
Cost at iteration 44: 331.226036
Error rate: 0.094
Cost at iteration 45: 331.029855
Error rate: 0.095
Cost at iteration 46: 330.838744
Error rate: 0.095
Cost at iteration 47: 330.546760
Error rate: 0.095
Cost at iteration 48: 329.899811
Error rate: 0.095
Cost at iteration 49: 329.422481
Error rate: 0.095
Final error rate: 0.095
Elapsted time for batch GD: 0:00:12.933731
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Compare">Compare<a class="anchor-link" href="#Compare">&#182;</a></h2><p>Let's take a second to compare the plots of each gradient descent variation. We can see that <strong>stochastic</strong> gradient descent has hardly improved at all, so we would need many more iterations. If we then look at <strong>full</strong> gradient descent, it appears to have converged faster than <strong>batch</strong> gradient descent, but it all ran slower.</p>
<p>Now, if you zoom into the batch and stochastic curves you will see that they are not always smooth, whereas full gradient descent is.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">LL</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">LL</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">LL_stochastic</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">LL_stochastic</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;stochastic&quot;</span><span class="p">)</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">LL_batch</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">LL_batch</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
