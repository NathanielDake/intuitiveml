<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.6" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="6.-Summary">6. Summary<a class="anchor-link" href="#6.-Summary">&#182;</a></h1><p>We started off this series talking about the bias-variance trade-off. We showed that the error of any classification or regression model is a combination of <strong>bias</strong>, <strong>variance</strong>, and <strong>irreducible error</strong>. We then demonstrated that irreducible error can't be reduced, by bias and variance can! In the ideal situation both bias and variance are <strong>low</strong>. The main dilemma we saw was that as we decrease one, the other tends to increase. So, the idea that we found was that we want to find a happy medium where we are optimizing the test error. We learned than ensemble methods are a way to make the tradeoff, less of a tradeoff (i.e. attain low bias and low variance)!</p>
<p><br></p>
<h2 id="1.1-Bootstrap">1.1 Bootstrap<a class="anchor-link" href="#1.1-Bootstrap">&#182;</a></h2><p>We began our discussion of ensemble methods with the <strong>bootstrap technique</strong>. We showed that by using the bootstrap we can not only estimate confidence intervals, but we can actually reduce the variance of whatever we are trying to estimate; just by estimating it over and over again with resampled data sets. The key equation below, showed us that the variance of the bootstrap estimate was a function of the original variance, the correlation between each bootstrap sample, and the number of bootstrap samples.</p>
$$var(\bar{\theta}_B) = \frac{1 - \rho}{B}\sigma^2 + \rho \sigma^2$$<p>This equation showed us that when the correlation between each bootstrap sample is 1, we do not get any reduction in variance. But, when the correlation between each bootstrap sample is 0, then we get the maximum reduction in variance, which is a $\frac{1}{B}$ decrease.</p>
<p><br></p>
<h2 id="1.2-Bagging">1.2 Bagging<a class="anchor-link" href="#1.2-Bagging">&#182;</a></h2><p>The bootstrap technique motivated the idea of <strong>bagging</strong>. This is where we would create an ensemble of models by training them on bootstrapped samples of the training data. We deduced that by using trees, which have very low bias and very high variance, we could achieve the desired decorrelation effect. We then combined the trees to lower the variance.</p>
<p><br></p>
<h2 id="1.3-Random-Forest">1.3 Random Forest<a class="anchor-link" href="#1.3-Random-Forest">&#182;</a></h2><p>Next we looked at the random forest, which further decorrelated the ensemble of trees by sampling the features as well.</p>
<p><br></p>
<h2 id="1.4-AdaBoost">1.4 AdaBoost<a class="anchor-link" href="#1.4-AdaBoost">&#182;</a></h2><p>Next we looked at AdaBoost, which unlike bagged trees and random forest, did not aim to use low bias, high variance models. Instead, the idea behind boosting was that many individual weak learners could be combined to be a strong learner if weighted properly. We demonstrated that this is true, by showing that AdaBoost achieve the best error rate on our data.</p>
<p><br></p>
<h2 id="1.4-Mixture-of-Experts">1.4 Mixture of Experts<a class="anchor-link" href="#1.4-Mixture-of-Experts">&#182;</a></h2><p>An interesting idea is how you may even go past boosting. First, we know that a bagging classifier just outputs the sum of all of its base learner predictions (assuming we are using -1 and +1 as targets).</p>
$$\textbf{Bagging} \rightarrow F(x) = \sum_{m=1}^Mf_m(x)$$<p>Boosting extends this by adding weights to each base learner. These weights, however, are not dependent on the input. Thus, each weight has the same value no matter what x is.</p>
$$\textbf{Boosting} \rightarrow F(x) = \sum_{m=1}^M \alpha_mf_m(x)$$<p>One idea is that you can extend that by making each base learner an expert at something different. In other words, the model weight will also depend on x, and that will tell us how good the model is at classifying that x. This is known as mixture of experts.</p>
$$\textbf{Mixture of Experts} \rightarrow F(x) = \sum_{m=1}^M \alpha_m(x)f_m(x)$$
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
