
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="ipynb_website:version" content="0.9.4" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<link rel="stylesheet" type="text/css" href="../css/jt.css">
<link rel="stylesheet" type="text/css" href="../css/readable.css">
<link rel="stylesheet" type="text/css" href="../css/toc2.css">

<link href="../site_libs/jqueryui-1.11.4/jquery-ui.css">
<link rel="stylesheet" href="../site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<link rel="stylesheet" href="../site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="../site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>

<link rel="stylesheet"
      href="../site_libs/highlightjs/null.min.css"
      type="text/css" />

<script src="../site_libs/highlightjs/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>

<script src="../js/doc_toc.js"></script>
<script src="../js/docs.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
        },
        "HTML-CSS": {
            preferredFont: "TeX",
            availableFonts: ["TeX"],
            styles: {
                scale: 110,
                ".MathJax_Display": {
                    "font-size": "110%",
                }
            }
        }
    });
</script>
<script>
function filterDataFrame(id) {
    var input = document.getElementById("search_" + id);
    var filter = input.value.toUpperCase();
    var table = document.getElementById("dataframe_" + id);
    var tr = table.getElementsByTagName("tr");
    // Loop through all table rows, and hide those who don't match the search query
    for (var i = 1; i < tr.length; i++) {
        for (var j = 0; j < tr[i].cells.length; ++j) {
            var matched = false;
            if (tr[i].cells[j].innerHTML.toUpperCase().indexOf(filter) != -1) {
                tr[i].style.display = "";
                matched = true
                break;
            }
            if (!matched)
                tr[i].style.display = "none";
        }
    }
}
function sortDataFrame(id, n, dtype) {
    var table = document.getElementById("dataframe_" + id);
    var tb = table.tBodies[0]; // use `<tbody>` to ignore `<thead>` and `<tfoot>` rows
    var tr = Array.prototype.slice.call(tb.rows, 0); // put rows into array
    if (dtype === 'numeric') {
        var fn = function(a, b) { 
            return parseFloat(a.cells[n].textContent) <= parseFloat(b.cells[n].textContent) ? -1 : 1;
        }
    } else {
        var fn = function(a, b) {
            var c = a.cells[n].textContent.trim().localeCompare(b.cells[n].textContent.trim()); 
            return c > 0 ? 1 : (c < 0 ? -1 : 0) }
    }
    var isSorted = function(array, fn) {
        if (array.length < 2)
            return 1;
        var direction = fn(array[0], array[1]); 
        for (var i = 1; i < array.length - 1; ++i) {
            var d = fn(array[i], array[i+1]);
            if (d == 0)
                continue;
            else if (direction == 0)
                direction = d;
            else if (direction != d)
                return 0;
            }
        return direction;
    }
    var sorted = isSorted(tr, fn);
    if (sorted == 1 || sorted == -1) {
        // if sorted already, reverse it
        for(var i = tr.length - 1; i >= 0; --i)
            tb.appendChild(tr[i]); // append each row in order
    } else {
        tr = tr.sort(fn);
        for(var i = 0; i < tr.length; ++i)
            tb.appendChild(tr[i]); // append each row in order
    }
}
</script>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');
  // mark it active
  menuAnchor.parent().addClass('active');
  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>
<div class="container-fluid main-container">
<!-- tabsets -->
<script src="../site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>



<title>Nathaniel Dake Blog</title>

<style type = "text/css">
body {
  font-family: "sans-serif";
  padding-top: 66px;
  padding-bottom: 40px;
}
</style>
</head>

<body>
<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!-- code folding -->

<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Nathaniel Dake Blog</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
<li>
  <a href="../Deep_Learning.html">Deep Learning</a>
</li>
        
<li>
  <a href="../AI.html">AI</a>
</li>
        
<li>
  <a href="../Machine_Learning.html">Machine Learning</a>
</li>
        
<li>
  <a href="../NLP.html">NLP</a>
</li>
        
<li>
  <a href="../Mathematics.html">Mathematics</a>
</li>
        
<li>
  <a href="../Projects.html">Projects</a>
</li>
        
<li>
  <a href="../Book_Reviews.html">Book Reviews</a>
</li>
        
      </ul>
        
<ul class="nav navbar-nav navbar-right">
<li>
   <a href="https://github.com/NathanielDake/nathanieldake.github.io"> source </a>
</li>
</ul>
        
      </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Introduction-to-Natural-Language-Processing">1. Introduction to Natural Language Processing<a class="anchor-link" href="#1.-Introduction-to-Natural-Language-Processing">&#182;</a></h1><p>Natural Language Processing is certainly one of the most fascinating and exciting areas to be involved with at this point in time. It is a wonderful intersection of computer science, artificial intelligence, machine learning and linguistics. With the (somewhat) recent rise of Deep Learning, Natural Language Processing currently has a great deal of buzz surrounding it, and for good reason. The goal of this post is to do three things:</p>
<ol>
<li>Inspire the reader with the beauty of the problem of NLP</li>
<li>Explain how machine learning techniques (i.e. something as simple as Logistic Regression) can be applied to text data.</li>
<li>Prepare the reader for the next sections surrounding Deep Learning as it is applied to NLP.</li>
</ol>
<p>Before we dive in, I would like to share the poem <em>Jabberwocky</em> by Lewis Carrol, and an accompanying excerpt from the book "<em>Godel, Escher, Bach</em>", by Douglas Hofstadter.</p>
<p><img src="https://drive.google.com/uc?id=1ROLVf2p6xYyTqQ3fmeky0eSD6ZCdfJ3M" width="300"></p>
<p>And now, the corresponding excerpt, <em><strong>Translations of Jabberwocky</strong></em>.</p>
<blockquote><h3>Translations of Jabberwocky<br></h3>
<p>Douglas R. Hofstadter
Imagine native speakers of English, French, and German, all of whom have excellent command of their respective native languages, and all of whom enjoy wordplay in their own language. Would their symbol networks be similar on a local level, or on a global level? Or is it meaningful to ask such a question? The question becomes concrete when you look at the preceding translations of Lewis Carroll's famous "Jabberwocky".
<br>
<br>
[The "preceding translations" were "Jabberwocky" (English, original), by Lewis Carroll, "Le Jaseroque", (French), by Frank L. Warrin, and "Der Jammerwoch" (German), by Robert Scott. --kl]
<br>
<br>
I chose this example because it demonstrates, perhaps better than an example in ordinary prose, the problem of trying to find "the same node" in two different networks which are, on some level of analysis, extremely nonisomorphic. In ordinary language, the task of translation is more straightforward, since to each word or phrase in the original language, there can usually be found a corresponding word or phrase in the new language. By contrast, in a poem of this type, many "words" do not carry ordinary meaning, but act purely as exciters of nearby symbols. However, what is nearby in one language may be remote in another.
<br>
<br>
Thus, in the brain of a native speaker of English, "slithy" probably activates such symbols as "slimy", "slither", "slippery", "lithe", and "sly", to varying extents. Does "lubricilleux" do the corresponding thing in the brain of a Frenchman? What indeed would be "the corresponding thing"? Would it be to activate symbols which are the ordinary translations of those words? What if there is no word, real or fabricated, which will accomplish that? Or what if a word does exist, but it is very intellectual-sounding and Latinate ("lubricilleux"), rather than earthy and Anglo-Saxon ("slithy")? Perhaps "huilasse" would be better than "lubricilleux"? Or does the Latin origin of the word "lubricilleux" not make itself felt to a speaker of French in the way that it would if it were an English word ("lubricilious", perhaps)?
<br>
<br>
An interesting feature of the translation into French is the transposition into the present tense. To keep it in the past would make some unnatural turns of phrase necessary, and the present tense has a much fresher flavour in French than in the past. The translator sensed that this would be "more appropriate"--in some ill-defined yet compelling sense--and made the switch. Who can say whether remaining faithful to the English tense would have been better?
<br>
<br>
In the German version, the droll phrase "er an-zu-denken-fing" occurs; it does not correspond to any English original. It is a playful reversal of words, whose flavour vaguely resembles that of the English phrase "he out-to-ponder set", if I may hazard a reverse translation. Most likely this funny turnabout of words was inspired by the similar playful reversal in the English of one line earlier: "So rested he by the Tumtum tree". It corresponds, yet doesn't correspond.
<br>
<br>
Incidentally, why did the Tumtum tree get changed into an "arbre Té-té" in French? Figure it out for yourself.
<br>
<br>
The word "manxome" in the original, whose "x" imbues it with many rich overtones, is weakly rendered in German by "manchsam", which back-translates into English as "maniful". The French "manscant" also lacks the manifold overtones of "manxome". There is no end to the interest of this kind of translation task.
<br>
<br>
When confronted with such an example, one realizes that it is utterly impossible to make an exact translation. Yet even in this pathologically difficult case of translation, there seems to be some rough equivalence obtainable. Why is this so, if there really is no isomorphism between the brains of people who will read the different versions? The answer is that there is a kind of rough isomorphism, partly global, partly local, between the brains of all the readers of these three poems.</p>
</blockquote>
<p>Now, the purpose of sharing the above is because if you are reading these posts (and are anything like me), you may very well spend a large chunk of your time studying mathematics, computer science, machine learning, writing code, and so on. But, if you are new to NLP the appreciation for the beauty and deeper meaning surrounding language may not be on the forefront of your mind-that is understandable! But hopefully the passage and commentary above ignited some interest in the wonderfully complex and worthwhile problem of Natural Language Processing and Understanding.</p>
<h2 id="2.-Spam-Detection">2. Spam Detection<a class="anchor-link" href="#2.-Spam-Detection">&#182;</a></h2><p>Now, especially at first, I don't want to dive into phonemes, morphemes, syntactical structure, and the like. We will leave those linguistic concepts for later on. The goal here is to quickly allow someone with an understanding of basic machine learning algorithms and techniques to implement them in the domain of NLP.</p>
<p>We will see that, at least at first, a lot of NLP deals with preprocessing data, which allows us to use algorithms that we already know. The question that most definitely arises is: How do we take a bunch of documents which are basically a bunch of text, and feed them into other machine learning algorithms where the input is usually a vector of numbers?</p>
<p>Well, before we even get to that, let's take a preprocessed data set from the <a href="https://archive.ics.uci.edu/ml/datasets/Spambase">uci archive</a> and perform a simple classification on it. The data has been processed in such a way that we can consider columns 1-48 to the be the input, and column 49 to the be label (1 = spam, 0 = not spam).</p>
<p>The input columns are considered the input, and they are a <strong>word frequency measure</strong>. This measure can be calculated via:</p>
<p>$$\text{Word Frequency Measure} = \frac{\text{# of times word appears in a document}}{\text{Number of words in document}} * 100$$</p>
<p>This will result in a <strong>Document Term matrix</strong>, which is a matrix where <em>terms</em> (words that appeared in the document) go along the columns, and <em>documents</em> (emails in this case) go along the rows:</p>
<table>
<thead><tr>
<th></th>
<th>word 1</th>
<th>word 2</th>
<th>word 3</th>
<th>word 4</th>
<th>word 5</th>
<th>word 6</th>
<th>word 7</th>
<th>word 8</th>
</tr>
</thead>
<tbody>
<tr>
<td>Email 1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Email 2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Email 3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Email 4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Email 5</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="2.1-Implementation-in-Code">2.1 Implementation in Code<a class="anchor-link" href="#2.1-Implementation-in-Code">&#182;</a></h3><p>We will now use <code>Scikit Learn</code> to show that we can use <em>any</em> model on NLP data, as long as it has been preprocessed correctly. First, let's use scikit learns <code>NaiveBayes</code> classifier:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">MultinomialNB</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../../data/nlp/spambase.data&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[2]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>0.64</th>
      <th>0.64.1</th>
      <th>0.1</th>
      <th>0.32</th>
      <th>0.2</th>
      <th>0.3</th>
      <th>0.4</th>
      <th>0.5</th>
      <th>0.6</th>
      <th>...</th>
      <th>0.40</th>
      <th>0.41</th>
      <th>0.42</th>
      <th>0.778</th>
      <th>0.43</th>
      <th>0.44</th>
      <th>3.756</th>
      <th>61</th>
      <th>278</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.21</td>
      <td>0.28</td>
      <td>0.50</td>
      <td>0.0</td>
      <td>0.14</td>
      <td>0.28</td>
      <td>0.21</td>
      <td>0.07</td>
      <td>0.00</td>
      <td>0.94</td>
      <td>...</td>
      <td>0.00</td>
      <td>0.132</td>
      <td>0.0</td>
      <td>0.372</td>
      <td>0.180</td>
      <td>0.048</td>
      <td>5.114</td>
      <td>101</td>
      <td>1028</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.06</td>
      <td>0.00</td>
      <td>0.71</td>
      <td>0.0</td>
      <td>1.23</td>
      <td>0.19</td>
      <td>0.19</td>
      <td>0.12</td>
      <td>0.64</td>
      <td>0.25</td>
      <td>...</td>
      <td>0.01</td>
      <td>0.143</td>
      <td>0.0</td>
      <td>0.276</td>
      <td>0.184</td>
      <td>0.010</td>
      <td>9.821</td>
      <td>485</td>
      <td>2259</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.63</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>...</td>
      <td>0.00</td>
      <td>0.137</td>
      <td>0.0</td>
      <td>0.137</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>3.537</td>
      <td>40</td>
      <td>191</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>0.63</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>0.31</td>
      <td>0.63</td>
      <td>...</td>
      <td>0.00</td>
      <td>0.135</td>
      <td>0.0</td>
      <td>0.135</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>3.537</td>
      <td>40</td>
      <td>191</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>1.85</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.85</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>...</td>
      <td>0.00</td>
      <td>0.223</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>3.000</td>
      <td>15</td>
      <td>54</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 58 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">values</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>    <span class="c1"># randomly split data into train and test sets</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">48</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">,]</span>
<span class="n">Ytrain</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">,]</span>
<span class="n">Xtest</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:,]</span>
<span class="n">Ytest</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:,]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Classifcation Rate for NB: &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Classifcation Rate for NB:  0.87
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Excellent, a classification rate of 92%! Let's now look utilize <code>AdaBoost</code>:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">AdaBoostClassifier</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Classifcation Rate for Adaboost: &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Classifcation Rate for Adaboost:  0.94
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Great, a nice improvement, but more importantly, we have shown that we can take text data and that via correct preprocessing we are able to utilize it with standard machine learning API's. The next step is to dig into <em>how</em> basic preprocessing is performed.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="3.-Sentiment-Analysis">3. Sentiment Analysis<a class="anchor-link" href="#3.-Sentiment-Analysis">&#182;</a></h1><p>To go through the basic preprocessing steps that are frequently used when performing machine learning on text data (often referred to an NLP pipeline) we are going to want to work on the problem of <strong>sentiment analysis</strong>. Sentiment is a measure of how positive or negative something is, and we are going to build a very simple sentiment analyzer to predict the sentiment of Amazon reviews. These are reviews, so they come with 5 star ratings, and we are going to look at the electronics category in particular. These are XML files, so we will need an XML parser.</p>
<h3 id="3.1-NLP-Terminology">3.1 NLP Terminology<a class="anchor-link" href="#3.1-NLP-Terminology">&#182;</a></h3><p>Before we begin, I would just like to quickly go over some basic NLP terminology that will come up frequently throughout this post.</p>
<ul>
<li><strong>Corpus</strong>: Collection of text</li>
<li><strong>Tokens</strong>: Words and punctuation that make up the corpus. </li>
<li><strong>Type</strong>: a distinct token. Ex. "Run, Lola Run" has four tokens (comma counts as one) and 3 types.</li>
<li><strong>Vocabulary</strong>: The set of all types. </li>
<li>The google corpus (collection of text) has 1 trillion tokens, and only 13 million types. English only has 1 million dictionary words, but the google corpus includes types such as "www.facebook.com". </li>
</ul>
<h3 id="3.2-Problem-Overview">3.2 Problem Overview<a class="anchor-link" href="#3.2-Problem-Overview">&#182;</a></h3><p>Now, we are just going to be looking at the electronics category. We could use the 5 star targets to do regression, but instead we will just do classification since they are already marked "positive" and "negative". As I mentioned, we are going to be working with XML data, so we will need an XML parser, for which we will use <code>BeautifulSoup</code>. We will only look at the <code>review_text</code> attribute. To create our feature vector, we will count up the number of occurences of each word, and divided it by the total number of words. However, for that to work we will need two passes through the data:</p>
<ol>
<li>One to collect the total number of distinct words, so that we know the size of our feature vector, in other words the vocabulary size, and possibly remove stop words like "this", "is", "I", "to", etc, to decrease the vocabulary size. The goal here is to know the index of each token</li>
<li>On the second pass, we will be able to assign values to each data vector whose index corresponds to which words, and one to create data vectors </li>
</ol>
<p>Once we have that, it is simply a matter of creating a classifier like the one we did for our spam detector! Here, we will use logistic regression, so we can intepret the weights! For example, if you see a word like horrible and it has a weight of minus 1, it is associated with negative reviews. With that started, let's begin!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.3-Sentiment-Analysis-in-Code">3.3 Sentiment Analysis in Code<a class="anchor-link" href="#3.3-Sentiment-Analysis-in-Code">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="k">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="k">import</span> <span class="n">BeautifulSoup</span>

<span class="n">wordnet_lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>                                <span class="c1"># this turns words into their base form </span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../../data/nlp/stopwords.txt&#39;</span><span class="p">))</span>         <span class="c1"># grab stop words </span>

<span class="c1"># get pos reviews</span>
<span class="c1"># only want rev text</span>
<span class="n">positive_reviews</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../../data/nlp/electronics/positive.review&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="s2">&quot;lxml&quot;</span><span class="p">)</span> 
<span class="n">positive_reviews</span> <span class="o">=</span> <span class="n">positive_reviews</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;review_text&#39;</span><span class="p">)</span>                                  

<span class="n">negative_reviews</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../../data/nlp/electronics/negative.review&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="s2">&quot;lxml&quot;</span><span class="p">)</span>
<span class="n">negative_reviews</span> <span class="o">=</span> <span class="n">negative_reviews</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s1">&#39;review_text&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.3.1-Class-Imbalance">3.3.1 Class Imbalance<a class="anchor-link" href="#3.3.1-Class-Imbalance">&#182;</a></h3><p>There are more positive than negative reviews, so we are going to shuffle the positive reviews and then cut off any extra that we may have so that they are both the same size.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">positive_reviews</span><span class="p">)</span>
<span class="n">positive_reviews</span> <span class="o">=</span> <span class="n">positive_reviews</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">negative_reviews</span><span class="p">)]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.3.2-Tokenizer-function">3.3.2 Tokenizer function<a class="anchor-link" href="#3.3.2-Tokenizer-function">&#182;</a></h3><p>Lets now create a tokenizer function that can be used on our specific reviews.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">my_tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">tokenize</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>                        <span class="c1"># essentially string.split()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>                     <span class="c1"># get rid of short words</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">wordnet_lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>     <span class="c1"># get words to base form</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.3.3-Index-each-word">3.3.3 Index each word<a class="anchor-link" href="#3.3.3-Index-each-word">&#182;</a></h3><p>We now need to create an index for each of the words, so that each word has an index in the final data vector. However, to able able to do that we need to know the size of the final data vector, and to be able to know that we need to know how big the vocabulary is. Remember, the <strong>vocabulary</strong> is just the set of all types!</p>
<p>We are essentially going to look at every individual review, tokenize them, and then add those tokens 1 by 1 to the map if they do not exist yet.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">word_index_map</span> <span class="o">=</span> <span class="p">{}</span>                            <span class="c1"># our vocabulary - dictionary that will map words to dictionaries</span>
<span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>                              <span class="c1"># counter increases whenever we see a new word</span>

<span class="n">positive_tokenized</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">negative_tokenized</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># --------- loop through positive reviews ---------</span>
<span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">positive_reviews</span><span class="p">:</span>              
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">my_tokenizer</span><span class="p">(</span><span class="n">review</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>          <span class="c1"># converts single review into array of tokens (split function)</span>
    <span class="n">positive_tokenized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>                        <span class="c1"># loops through array of tokens for specific review</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_index_map</span><span class="p">:</span>                        <span class="c1"># if the token is not in the map, add it</span>
            <span class="n">word_index_map</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_index</span>          
            <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>                                 <span class="c1"># increment current index</span>
                
<span class="c1"># --------- loop through negative reviews ---------</span>
<span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="n">negative_reviews</span><span class="p">:</span>              
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">my_tokenizer</span><span class="p">(</span><span class="n">review</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>          
    <span class="n">negative_tokenized</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>                       
        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_index_map</span><span class="p">:</span>                        
            <span class="n">word_index_map</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_index</span>          
            <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>   
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And we can actually take a look at the contents of <code>word_index_map</code> by making use of the <code>random</code> module (part of the Python Standard Library):</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">word_index_map</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="mi">20</span><span class="p">)))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;tech-savvy&#39;: 5921, &#39;downloads&#39;: 2029, &#39;re-acquire&#39;: 9930, &#39;megapixels&#39;: 7066, &#39;dual-amping&#39;: 6499, &#39;unsupported&#39;: 10981, &#39;configuration&#39;: 1183, &#39;6000&#39;: 3246, &#39;obviously..&#39;: 9627, &#39;didn&#39;: 10133, &#39;eligible&#39;: 2440, &#39;lawn&#39;: 748, &#39;50-pack&#39;: 1002, &#39;yearly..&#39;: 10956, &#39;192.168.1.245&#39;: 2607, &#39;glad&#39;: 1844, &#39;occasionally&#39;: 1631, &#39;floppy&#39;: 7170, &#39;criminal&#39;: 4786, &#39;emptying&#39;: 4382}
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Size&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index_map</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Vocabulary Size 11088
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.3.4-Convert-tokens-into-vector">3.3.4 Convert tokens into vector<a class="anchor-link" href="#3.3.4-Convert-tokens-into-vector">&#182;</a></h3><p>Now that we have our tokens and vocabulary, we need to convert our tokens into a vector. Because we are going to shuffle our train and test sets again, we are going to want to put labels and vector into same array for now since it makes it easier to shuffle.</p>
<p>Note, this function operates on <strong>one</strong> review. So the +1 is creating our label, and this function is basically designed to take our input vector from an english form to a numeric vector form.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokens_to_vector</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">xy_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index_map</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>          <span class="c1"># equal to the vocab size + 1 for the label </span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>                                     <span class="c1"># loop through every token</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">word_index_map</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>                            <span class="c1"># get index from word index map</span>
        <span class="n">xy_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>                                  <span class="c1"># increment data at that index </span>
    <span class="n">xy_data</span> <span class="o">=</span> <span class="n">xy_data</span> <span class="o">/</span> <span class="n">xy_data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>                    <span class="c1"># divide entire array by total, so they add to 1</span>
    <span class="n">xy_data</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>                                  <span class="c1"># set last element to label</span>
    <span class="k">return</span> <span class="n">xy_data</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Time to actually assign these tokens to vectors.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">positive_tokenized</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">negative_tokenized</span><span class="p">)</span>               <span class="c1"># total number of examples </span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index_map</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>                       <span class="c1"># N examples x vocab size + 1 for label</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>                                                               <span class="c1"># counter to keep track of sample</span>

<span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">positive_tokenized</span><span class="p">:</span>                                   <span class="c1"># loop through postive tokenized reviews</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">tokens_to_vector</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                                <span class="c1"># passing in 1 because these are pos reviews</span>
    <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">xy</span>                                                  <span class="c1"># set data row to that of the input vector</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>                                                          <span class="c1"># increment 1</span>
    
<span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">negative_tokenized</span><span class="p">:</span>                                   
    <span class="n">xy</span> <span class="o">=</span> <span class="n">tokens_to_vector</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>                                
    <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">xy</span>                                                 
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>         
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>(2000, 11089)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our data is now 1000 rows of positively labeled reviews, followed by 1000 rows of negatively labeled reviews. We have <code>11089</code> columns, which is one more than our vocabulary size because we have a column for the label (positive or negative). Lets shuffle before getting our train and test set.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">Xtrain</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">,]</span>
<span class="n">Ytrain</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="o">-</span><span class="mi">100</span><span class="p">,]</span>
<span class="n">Xtest</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:,]</span>
<span class="n">Ytest</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:,]</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Ytrain</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification Rate: &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">Ytest</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Classification Rate:  0.7
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="3.3.5-Classification-Rate">3.3.5 Classification Rate<a class="anchor-link" href="#3.3.5-Classification-Rate">&#182;</a></h3><p>We end up with a classification rate of 0.71, which is not ideal, but it is better than random guessing.</p>
<h3 id="3.3.6-Sentiment-Analysis">3.3.6 Sentiment Analysis<a class="anchor-link" href="#3.3.6-Sentiment-Analysis">&#182;</a></h3><p>Something interesting that we can do is look at the weights of each word, to see if that word has positive or negative sentiment.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.7</span> 
<span class="n">large_magnitude_weights</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_index_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">index</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="o">&gt;</span> <span class="n">threshold</span> <span class="ow">or</span> <span class="n">weight</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">threshold</span><span class="p">:</span>
      <span class="n">large_magnitude_weights</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">weight</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sort_by_magnitude</span><span class="p">(</span><span class="n">sentiment_dict</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sentiment_dict</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  
<span class="n">large_magnitude_weights</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">sort_by_magnitude</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">large_magnitude_weights</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[(&#39;price&#39;, 2.808163204024058), (&#39;easy&#39;, 1.7646511704661152), (&#39;quality&#39;, 1.3716522244882545), (&#39;excellent&#39;, 1.319811182219224), (&#39;love&#39;, 1.237745876552362), (&#39;you&#39;, 1.155006377913112), (&#39;perfect&#39;, 1.0324004425098248), (&#39;sound&#39;, 0.9780126530219685), (&#39;highly&#39;, 0.9778749978617105), (&#39;memory&#39;, 0.9398953342479317), (&#39;little&#39;, 0.9262682823592787), (&#39;fast&#39;, 0.905207610856845), (&#39;speaker&#39;, 0.8965845758701319), (&#39;ha&#39;, 0.8111001120921802), (&#39;pretty&#39;, 0.7764302324793534), (&#39;cable&#39;, 0.7712191036378001), (&#34;&#39;ve&#34;, 0.7170298751638035), (&#39;week&#39;, -0.7194449455694366), (&#39;returned&#39;, -0.7482471935264389), (&#39;bad&#39;, -0.7542948554985326), (&#39;poor&#39;, -0.7555447694156194), (&#39;tried&#39;, -0.7892866982929136), (&#39;buy&#39;, -0.8504195601103998), (&#39;month&#39;, -0.8771148641617261), (&#39;support&#39;, -0.9163137326943319), (&#39;waste&#39;, -0.946863186564699), (&#39;item&#39;, -0.9518247418299971), (&#39;money&#39;, -1.1086664158434432), (&#39;return&#39;, -1.1512973579906935), (&#39;then&#39;, -1.2084513223482118), (&#39;doe&#39;, -1.2197007105871698), (&#39;wa&#39;, -1.6630639259918825), (&#34;n&#39;t&#34;, -2.0687949024413546)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Clearly the above list is not perfect, <em>but</em> it should give some insight on what is possible for us already. The logistic regression model was able to pick out <code>easy</code>, <code>quality</code>, and <code>excellent</code> as words that correlate to a positive response, and it was able to find <code>poor</code>, <code>returned</code>, and <code>waste</code> as words the correlate to a negative response.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="4.-NLTK-Exploration">4. NLTK Exploration<a class="anchor-link" href="#4.-NLTK-Exploration">&#182;</a></h1><p>Before we move on any further, I wanted to take a minute to go over a few of the most useful tools for the <code>nltk</code> (Natural Language Toolkit) library. This library will encapsulate many NLP tasks for us.</p>
<h3 id="4.1-Parts-of-Speech-(POS)-Tagging">4.1 Parts of Speech (POS) Tagging<a class="anchor-link" href="#4.1-Parts-of-Speech-(POS)-Tagging">&#182;</a></h3><p>Parts of speech tagging is meant to do just what it sound like: tag each word with a given part of speech within a document. For example, in the following sentence:</p>
<blockquote><p>"Bob is great."</p>
</blockquote>
<p><code>Bob</code> is a noun, <code>is</code> is a verb, and <code>great</code> is an adjective. We can utilize <code>nltk</code>'s POS tagger on that sentence and see the same result:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="s2">&quot;Bob is great&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[16]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;Bob&#39;, &#39;NNP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;great&#39;, &#39;JJ&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="s2">&quot;Machine learning is great&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[17]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;Machine&#39;, &#39;NN&#39;), (&#39;learning&#39;, &#39;NN&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;great&#39;, &#39;JJ&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The second entry in the above tuples <code>NN</code>, <code>VBZ</code>, etc, represents the determined tag of the word. For a description of each tag, check out <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">this link</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.2-Stemming-and-Lemmatization">4.2 Stemming and Lemmatization<a class="anchor-link" href="#4.2-Stemming-and-Lemmatization">&#182;</a></h3><p>Both the process of <strong>stemming</strong> and <strong>lemmatization</strong> are used in reducing words to a "base" form. This is very useful because a vocabulary can get very large, while certain words tend to have the same meaning. For example <em>dog</em> and <em>dogs</em>, and <em>jump</em> and <em>jumping</em> both have similar meanings. The main difference between stemming and lemmatization is that stemming is a bit more basic.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">porter_stemmer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">porter</span><span class="o">.</span><span class="n">PorterStemmer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">porter_stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;dogs&#39;</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>dog
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">porter_stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">&#39;wolves&#39;</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>wolv
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">stem</span><span class="o">.</span><span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s1">&#39;dogs&#39;</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>dog
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="s1">&#39;wolves&#39;</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>wolf
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Both the stemmer and lemmatizer managed to get <code>dogs</code> correct, but only the lemmatizer managed to correctly convert <code>wolves</code> to base form.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.3-Named-Entity-Recognition">4.3 Named Entity Recognition<a class="anchor-link" href="#4.3-Named-Entity-Recognition">&#182;</a></h3><p>Finally there is <strong>Named Entity</strong> recognition. Entities refer to nouns such as:</p>
<ul>
<li>"Albert Einstein" - a person</li>
<li>"Apple" - an organization</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Albert Einstein was born on March 14, 1879&quot;</span>
<span class="n">tags</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[(&#39;Albert&#39;, &#39;NNP&#39;), (&#39;Einstein&#39;, &#39;NNP&#39;), (&#39;was&#39;, &#39;VBD&#39;), (&#39;born&#39;, &#39;VBN&#39;), (&#39;on&#39;, &#39;IN&#39;), (&#39;March&#39;, &#39;NNP&#39;), (&#39;14,&#39;, &#39;CD&#39;), (&#39;1879&#39;, &#39;CD&#39;)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">ne_chunk</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[23]:</div>




<div class="output_png output_subarea output_execute_result">
<img src="data:image/png;base64,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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Steve Jobs was the CEO of Apple Corp.&quot;</span>
<span class="n">tags</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[(&#39;Steve&#39;, &#39;NNP&#39;), (&#39;Jobs&#39;, &#39;NNP&#39;), (&#39;was&#39;, &#39;VBD&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;CEO&#39;, &#39;NNP&#39;), (&#39;of&#39;, &#39;IN&#39;), (&#39;Apple&#39;, &#39;NNP&#39;), (&#39;Corp.&#39;, &#39;NNP&#39;)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">ne_chunk</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[25]:</div>




<div class="output_png output_subarea output_execute_result">
<img src="data:image/png;base64,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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="5.-Latent-Semantic-Analysis">5. Latent Semantic Analysis<a class="anchor-link" href="#5.-Latent-Semantic-Analysis">&#182;</a></h1><p>We will now take a moment to extend our semantic analysis example from before, instead now performing <strong>Latent Semantic Analysis</strong>. Latent semantic analysis is utilized to deal with the reality that we will often have <em>multiple</em> words with the <em>same</em> meaning, or on the other hand, <em>one</em> word with <em>multiple</em> meanings. These are referred to as <em>synonomy</em> and <em>polysemy</em> respectively.</p>
<p>In the case of synonyms here are a few basic examples:</p>
<ul>
<li>"Buy" and "Purchase"</li>
<li>"Big" and "Large"</li>
<li>"Quick" and "Speedy"</li>
</ul>
<p>And in the case of polysemes:</p>
<ul>
<li>"Man" (man as in human, and man as in a male opposed to a female)</li>
<li>"Milk" (can be a noun or a verb)</li>
</ul>
<p>In order to solve this problem, we will need to introduce <em>Latent Variables</em>.</p>
<h2 id="5.1-Latent-Variables">5.1 Latent Variables<a class="anchor-link" href="#5.1-Latent-Variables">&#182;</a></h2><p>The easiest way to get your head around latent variables at first is via an example. Consider the words "computer", "laptop", and "PC"; these words are most likely seen together very often, meaning they are highly correlated. We can thinking a <em>latent</em> or <em>hidden</em> variable that is below representing them all, and we can call that $z$. We can mathematically define $z$ as:</p>
<p>$$z = 0.7*computer \; + 0.5*PC \; + 0.6*laptop$$</p>
<p>So, we now have an idea of what a latent variable is, but what is the job of Latent Semantic Analysis? The entire goal of LSA is:</p>
<ol>
<li>To find the latent/hidden variables.</li>
<li>Then, transform original data into these new variables. </li>
</ol>
<p>Ideally, after the above has been performed, the dimensionality of the new data will be much smaller than that of the original data set. It is important to note that LSA definitely helps solve the synonomy problem, by combining correlated variables. However, there are conflicting view points about whether or not it helps with polysemy.</p>
<h2 id="5.2-The-Math-Behind-LSA">5.2 The Math Behind LSA<a class="anchor-link" href="#5.2-The-Math-Behind-LSA">&#182;</a></h2><p>As we just discussed, the main goal when applying LSA is to deal with synonyms. For example, "small" and "little" would each make up their own unique variable, but in reality we know that they mean the same thing, so that is redundant. We could combine them into a single variable, reducing the dimensionality of our data set by one. So, to be clear the goal of LSA is:</p>
<blockquote><p><strong>Goal of LSA</strong>: Reduce redundancy.</p>
</blockquote>
<h3 id="5.2.1-Redundancy-in-Numbers">5.2.1 Redundancy in Numbers<a class="anchor-link" href="#5.2.1-Redundancy-in-Numbers">&#182;</a></h3><p>Now, machine learning at its core is always dealing with numbers, so what exactly do I mean by redundancy from a numerical standpoint? Take a look at the the plot below:</p>
<p><img src="https://drive.google.com/uc?id=1GK0COCtvumKXTI0nB0qHB_e696p2IE_J" width="300"></p>
<p>We can see clearly that there is a linear relationship between the dependent and independent variable. In other words, there is a linear relationship between lean body mass and muscle strength. So, we could say that one of these variables is redundant; if we know someones lean body mass, we can accurately predict their muscle strength, and vice versa. If we want a compact representation of attributes related to someones athletic performance, then we may only need to know one of these variables, since the other can be predicted from it. This advantage becomes more apparent as our dimensionality grows; if we could go from 1 million variables down to variables, that is a 200,000x's savings of space! Saving space is good, and hence reducing redundancy is good!</p>
<p>Now the math behind LSA is rather complex and involves a good deal of linear algebra, and to be honest it would slightly bloat this notebook if I placed it here. Because of this, I have decided to move it to my mathematics section under linear algebra. With that said, LSA is essentially just the application of <strong>Singular Value Decomposition</strong> (SVD) to a term document matrix. I highly encourage you to go over my notebook explaining SVD and PCA before continuing, to have a better understanding of how the underlying mechanics work in the code we are about to implement.</p>
<p>Now, we can begin by gaining a brief bit of intuition behind what LSA may look like in code. As usual, we are going to begin with an input matrix <code>X</code> of shape $NxD$, where $N$ is the number of samples and $D$ is the number of features. This will be passed into scikit learns svd model, <code>TruncatedSVD</code>, call the <code>fit</code>, <code>transform</code> function, and finally receive an output matrix <code>Z</code> of shape $Nx2$, or $Nxd$, where $d &lt;&lt; D$.</p>

<pre><code>model = TruncatedSVD()
model.fit(X)
Z = model.transform(X)
# equivalent: Z = model.fit_transform(X)</code></pre>
<h2 id="5.3-LSA-in-Code">5.3 LSA in Code<a class="anchor-link" href="#5.3-LSA-in-Code">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="k">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">TruncatedSVD</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Process:">Process:<a class="anchor-link" href="#Process:">&#182;</a></h3><ul>
<li>we start by pulling in all of the titles, and all of the stop words. Our titles will look like:
<pre><code>['Philosophy of Sex and Love A Reader',
'Readings in Judaism, Christianity, and Islam',
'Microprocessors Principles and Applications',
'Bernhard Edouard Fernow: Story of North American Forestry',
'Encyclopedia of Buddhism',...]</code></pre>
</li>
<li>we then define our tokenizer which will convert our list of strings into specific tokens, which will look like:
<pre><code>[['philosophy', 'sex', 'love', 'reader'],
['reading', 'judaism', 'christianity', 'islam'],
['microprocessor', 'principle'],
['bernhard', 'edouard', 'fernow', 'story', 'north', 'american', 'forestry'],
['encyclopedia', 'buddhism'],</code></pre>
</li>
<li>we then create our input matrix. This is going to be D x N, where D is the length of the total number of terms we are using (input features, 2070) and where N is the length of all tokens (2373, the total number of titles)</li>
<li>This is essentially the transpose of how our input matrix is generally setup. Usually we have our examples along the rows, and our input features along the columns, however, in NLP it is sometimes the opposite</li>
<li>we then loop through all tokens, and create a vector for each one (essentially, if a word occurs, its value in the vector is incremented by 1)</li>
<li>the final input matrix is fed into the SVD, where the X matrix is transformed into a Z matrix of only 2 dimensions</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">wordnet_lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../../data/nlp/all_book_titles.txt&#39;</span><span class="p">)]</span>       <span class="c1"># Load all book titles in to an array</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../../data/nlp/stopwords.txt&#39;</span><span class="p">))</span>             <span class="c1"># loading stop words (irrelevant)</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">union</span><span class="p">({</span>
    <span class="s1">&#39;introduction&#39;</span><span class="p">,</span> <span class="s1">&#39;edition&#39;</span><span class="p">,</span> <span class="s1">&#39;series&#39;</span><span class="p">,</span> <span class="s1">&#39;application&#39;</span><span class="p">,</span>
    <span class="s1">&#39;approach&#39;</span><span class="p">,</span> <span class="s1">&#39;card&#39;</span><span class="p">,</span> <span class="s1">&#39;access&#39;</span><span class="p">,</span> <span class="s1">&#39;package&#39;</span><span class="p">,</span> <span class="s1">&#39;plus&#39;</span><span class="p">,</span> <span class="s1">&#39;etext&#39;</span><span class="p">,</span>
    <span class="s1">&#39;brief&#39;</span><span class="p">,</span> <span class="s1">&#39;vol&#39;</span><span class="p">,</span> <span class="s1">&#39;fundamental&#39;</span><span class="p">,</span> <span class="s1">&#39;guide&#39;</span><span class="p">,</span> <span class="s1">&#39;essential&#39;</span><span class="p">,</span> <span class="s1">&#39;printed&#39;</span><span class="p">,</span>
    <span class="s1">&#39;third&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="s1">&#39;fourth&#39;</span><span class="p">,</span> <span class="p">})</span>                                    <span class="c1"># adding additional stop words </span>
 

<span class="k">def</span> <span class="nf">my_tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">tokenize</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>                        <span class="c1"># essentially string.split()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>                     <span class="c1"># get rid of short words</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">wordnet_lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>     <span class="c1"># get words to base form</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>             <span class="c1"># remove stop words</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">isdigit</span><span class="p">()</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">t</span><span class="p">)]</span>  <span class="c1"># get rid of any token that includes a number</span>
    <span class="k">return</span> <span class="n">tokens</span>

<span class="c1"># Lets now figure out the index of each word, by going through the entire vocabularly </span>
<span class="c1"># create a word-to-index map so that we can create our word-frequency vectors later</span>
<span class="c1"># let&#39;s also save the tokenized versions so we don&#39;t have to tokenize again later</span>
<span class="n">word_index_map</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">all_tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">all_titles</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">index_word_map</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">error_count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">title</span> <span class="ow">in</span> <span class="n">titles</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">title</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">,</span> <span class="s1">&#39;ignore&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="c1"># this will throw exception if bad characters</span>
        <span class="n">all_titles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">my_tokenizer</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">all_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_index_map</span><span class="p">:</span>
                <span class="n">word_index_map</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_index</span>
                <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">index_word_map</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">error_count</span> <span class="o">+=</span> <span class="mi">1</span>  
         
<span class="c1"># now let&#39;s create our input matrices - just indicator variables for this example - works better than proportions</span>
<span class="k">def</span> <span class="nf">tokens_to_vector</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_index_map</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">word_index_map</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_tokens</span><span class="p">)</span>     <span class="c1"># nested list, has 2373 total entries, each which has several words</span>
<span class="n">D</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_index_map</span><span class="p">)</span> <span class="c1"># total number of words that we are working with (2070)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>    <span class="c1"># terms will go along rows, documents along columns</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">all_tokens</span><span class="p">:</span>
    <span class="n">X</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokens_to_vector</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">()</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Z</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">D</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">index_word_map</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Keep-in-mind...">Keep in mind...<a class="anchor-link" href="#Keep-in-mind...">&#182;</a></h2><p>What is important to remember here is that the main point of this process is to take a bunch of words (2070) be able to plot their relationship to eachother in a 2-d surface. To do this, their use in 2373 book titles are considered, and SVD is performed.</p>

</div>
</div>
</div>
<hr>
&copy; 2018 Nathaniel Dake

</div>
</div>
</body>
</html>
