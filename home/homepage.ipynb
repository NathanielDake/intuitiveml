{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1gByptP0RIr3Qh8NDTkI7Tntsfh_GKTWq\" unconfined=True>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Bio**<br>\n",
    "I am the lead Machine Learning Engineer at Carimus, Inc. We function as a consultancy which allows for me to work on a ton of challenging, diverse problems. I do everything from create models and deploy them to production, push and pull large quantities of data via a parallelized serverless approach, run experiments that directly translate into business value, wrangle data with SQL, and implement software that is able to operate in real time. There isn't a day that goes by that I am not pushed out of my comfort zone, learning, growing, and becoming a better problem solver and teammate. \n",
    "\n",
    "Prior to working at Carimus I was a Computer Engineering Research Assistant at the University of Florida, and prior to that I received my BS in Mechanical Engineering and Physics from Northeastern University. If you want to know more you can find the source code for all of the notebooks that make up this site [on my github](https://github.com/NathanielDake), and if you'd like to connect feel free to reach out via [my linkedin!](https://www.linkedin.com/in/nathanieldake/)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Popular Posts**<br>\n",
    "To get an idea of how I think about problems and break down concepts, check out some of my more popular posts.\n",
    "\n",
    "1. <a href=\"./Mathematics/06-Functions-02-Inverse-functions-exponentials-and-logarithms.html\">Inverse Functions Explored via the Exponential and Logarithm</a>\n",
    "\n",
    "2. <a href=\"./Machine_Learning/08-Bayesian_Machine_Learning-01-Bayesian-Inference.html\">Bayesian Inference</a>\n",
    "\n",
    "3. <a href=\"./Machine_Learning/08-Bayesian_Machine_Learning-02-Bayesian-AB-Testing.html\">Bayesian A/B Testing</a>\n",
    "\n",
    "4. <a href=\"./Mathematics/04-Statistics-03-statistical-inference.html\">Statistical Inference and Frequentist A/B Testing</a>\n",
    "\n",
    "5. <a href=\"./Mathematics/04-Statistics-02-History-of-Normal-Distribution.html\">History of the Gaussian Distribution</a>\n",
    "\n",
    "6. <a href=\"./Mathematics/04-Statistics-01-Introduction.html\">Introduction to Statistics</a>\n",
    "\n",
    "7. <a href=\"./Mathematics/06-Functions-01-Composition-of-functions.html\">Composition of Functions</a>\n",
    "\n",
    "8. <a href=\"./Machine_Learning/07-Dimensionality_Reduction-PCA.html\">Dimensionality Reduction & Principal Component Analysis</a>\n",
    "\n",
    "9. <a href=\"./Machine_Learning/05-Hidden_Markov_Models-04-Hidden-Markov-Models-Hidden-Markov-Models-Discrete-Observations.html\">From Markov Models to Hidden Markov Models</a>\n",
    "\n",
    "10. <a href=\"./Deep_Learning/03-Recurrent_Neural_Networks-01-The-Simple-Recurrent-Unit.html\">Recurrent Neural Networks</a>\n",
    "\n",
    "<br>  \n",
    "    \n",
    "**What is this blog about?**<br>\n",
    "During my time studying Data Science and Machine Learning, Software Development, Computer Science, Physics, and Mechanical Engineering, I have learned a lot about the best way the I learn. It is clear at this point that for a beginner, jumping right into a text book is rarely the best route to follow. The concepts, and more importantly just the language and terminology, will seem very difficult to comprehend, and most likely leave you discouraged that the material is simply outside of your grasp.\n",
    "\n",
    "This is especially true in the field of Data Science and Machine Learning, where several other technical disciplines intertwine:\n",
    "- Statistics\n",
    "- Probability\n",
    "- Computer Science\n",
    "- Calculus\n",
    "- Linear Algebra\n",
    "- Information Theory\n",
    "\n",
    "By jumping into formula heavy text books, and skipping out on the real world applications of Machine Learning (which lets be honest, they are pretty awesome), there is no incentive to continue and push forward. This is where the Top down approach was introduced. School systems generally teach via a bottom up approach - giving students the small building block thats can be combined in the end to create a grand system. Again, this leaves the learner wanting more, and often struggling to connect the dots of *why this small building is useful* and *why should I care*?\n",
    "\n",
    "The top down approach throws you right into the deep end, allowing you to work with premade algorithms and libraries, without fully understanding the math and intuitions behind the overall system. This, in my opinion, is a much better approach, but still leaves a bit to be desired. As with anything, I feel that balance is the key here. The goal should be to use real world examples to teach the mechanics of what is going on under the hood. To often concepts that are taught are treated as black boxes, and rote memorization is used to get through. This worked for a time, but in the field of Data Science and Machine Learning, it will not. There is no one size fits all - it is messy, chaotic, and unclear. And that is **my job** as a Data Scientist - to bring clarity to a problem, and help find a resolution.\n",
    "\n",
    "The goal of this blog is to build **intuitions**. Anyone can follow a basic process of predetermined steps and arrive at a solution. But we want to create intuitions of what is *actually* going on, so that if the situation was broken from its cookie cutter form we would be able to take that in stride and still make sense of based on the fundamental principles. So with that said...\n",
    "\n",
    "This blog (built from my [intuitiveml notebooks](https://github.com/NathanielDake/intuitiveml)) is designed with two main purposes in mind:\n",
    "1. Highlight my exact journey of teaching myself Machine Learning and Data Science\n",
    "2. Develop key intuitions about what is really happening. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Content**<br>\n",
    "The general content of this blog is arranged as follows:\n",
    "* **Deep Learning** \n",
    "<br> This section contains everything from the fundamentals of FeedForward Networks and Vanilla Backprop, to Modern Deep learning techniques such as adaptive learning rates and batch normalization, and finally Recurrent Neural Networks and their application to Natural Language Processing problems. Convolutional Neural Networks coming soon. \n",
    "\n",
    "* **Artificial Intelligence**\n",
    "<br> Focusing mainly on Reinforcement learning, specfically the Explore Exploit Dilemma (and bayesian techniques), Markov Decision Processes, Dynamic Programming, Temporal Difference Learning, Q-Learning, and Approximation Methods. Reinforcement learning with deep learning techniques coming soon. \n",
    "\n",
    "* **Machine Learning**\n",
    "<br> This section is the most broad by far consisting of Linear Regression, Logistic Regression, Decision Trees, Probabilistic Graphical Models, Ensemble Methods, Unsupervised Learning, and Hidden Markov Models. All of the notebooks have been made for this already and can be seen on my github, but some are still being formatted and refactored so they are post-worthy.\n",
    "\n",
    "* **Natural Language Processing**\n",
    "<br> Currently this section is empty, however, I do have the first batch of notebooks available on my github that deal with semantic and latent semantic analysis. This section is one that will be heavily expanded on, specifically applying deep learning techniques (such as recurrent neural networks) to solving NLP problems. There is also the matter of production level NLP pipelines which I have dealt with during my time working for Carimus, which will be added when I have the time.\n",
    "\n",
    "* **Mathematics**\n",
    "TODO\n",
    "\n",
    "* **Book Reviews**\n",
    "<br> This section is also going to be added to in greater detail over time, specifically to review some amazing books I have read in 2018 such as: \n",
    "\n",
    "    * **Life 3.0**: Being Human in the Age of Artificial Intelligence\n",
    "    * **The Master Algorithm**: How the Quest for the Ultimate Learning Machine Will Remake Our World\n",
    "    * **How to Create a Mind**: The Secret of Human Thought Revealed\n",
    "    * **The Golden Ticket**: P, NP, and the Search for the Impossible\n",
    "    * **Godel, Escher, Bach**: An Eternal Golden Braid\n",
    "    * **Homo Deus**: A Brief History of Tomorrow\n",
    "    * **Algorithms to Live By**: The Computer Science of Human Decisions\n",
    "    * **Deep Work**: Rules for Focused Success in a Distracted World\n",
    "    * **Outliers**: The Story Of Success\n",
    "    * **Naked Statistics**: Stripping the Dread from the Data\n",
    "    * **The Second Machine Age**: Work, Progress, and Prosperity in a Time of Brilliant Technologies\n",
    "\n",
    "<br>\n",
    "\n",
    "**An Individual Post**<br>\n",
    "I should mention that each individual post (which is built from a jupyter notebook) contains what I personally feel is crucial to understand a given topic. Code samples are always written via *python*, and are mixed with visualizations (made by me), equations, pseudocode, and whatever else is needed to ensure a clear and effective transfer of knowledge. \n",
    "\n",
    "With that said, here is one final quote to always remember (and one that I remind myself of as I put together each one of these posts). It is from the renound American Physicist Richard Feynman as he was attempting to explain Fermi-Dirac statistics:\n",
    "\n",
    "> Feynman was a truly great teacher. He prided himself on being able to devise ways to explain even the most profound ideas to beginning students. Once, I said to him, “Dick, explain to me, so that I can understand it, why spin one-half particles obey Fermi-Dirac statistics.” Sizing up his audience perfectly, Feynman said, “I’ll prepare a freshman lecture on it.” But he came back a few days later to say, “I couldn’t do it. I couldn’t reduce it to the freshman level. That means we don’t really understand it.”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
