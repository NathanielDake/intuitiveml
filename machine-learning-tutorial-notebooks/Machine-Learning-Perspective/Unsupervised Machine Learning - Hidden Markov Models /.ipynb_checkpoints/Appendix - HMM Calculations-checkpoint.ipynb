{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Hidden Markov Model Calculations\n",
    "This appendix serves as an accompaniment to hidden markov models, discrete observations. We will go over calculations concerning:\n",
    "> 1. **Probability of a Sequence**\n",
    "2. **Forward-Backward Algorithm**\n",
    "3. **Viterbi Algorithm**\n",
    "4. **Baum-Welch Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 0. General Definitions\n",
    "We will start by restating common variables that will be used throughout our calculations. \n",
    "\n",
    "#### 0.1 Hidden States and Observations\n",
    "We will let the number of hidden states be $M$:\n",
    "\n",
    "#### $$\\text{Number of hidden states} = M$$\n",
    "\n",
    "And the length of our sequence of observations be $T$:\n",
    "\n",
    "#### $$\\text{Length of sequence of observations} = T$$\n",
    "\n",
    "#### 0.2 Joint Distribution\n",
    "We know that the joint distribution containing both our observed symbols and hidden states is:\n",
    "\n",
    "#### $$p(x,z)$$\n",
    "\n",
    "Where both $x$ and $z$ are vectors:\n",
    "\n",
    "#### $$x = \\big[x(1), x(2), ..., x(T)\\big]$$\n",
    "\n",
    "#### $$z = \\big[z(1), z(2), ..., z(T)\\big]$$\n",
    "\n",
    "#### 0.3 Marginalized Distribution\n",
    "And we want to find the distribution for the sequence of observed symbols:\n",
    "\n",
    "#### $$p\\big(x(1), x(2),...,x(T)\\big)$$\n",
    "\n",
    "This is done by _marginalizing_ out $z$. \n",
    "\n",
    "#### 0.4 Initial State Distribution $\\pi$\n",
    "We have our _initial state distribution_, $\\pi$, the probability of being in a hidden state when the sequence begins:\n",
    "\n",
    "#### $$\\pi$$\n",
    "\n",
    "#### 0.5 State Transition Matrix $A$\n",
    "Then there is our _state transition matrix_, $A$, which represents the probability of going from state $i$ to state $j$:\n",
    "\n",
    "#### $$A(i, j) = \\text{probability of going from state i to state j}$$\n",
    "\n",
    "#### 0.6 Observation Matrix $B$\n",
    "Finally, we have our _observation matrix_, $B$, which represents the probability of observing symbol $k$ while in state $j$:\n",
    "\n",
    "#### $$B(j,k) = \\text{probability of observing symbol k while you are in state j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Probability of a Sequence\n",
    "We have gone over the equations utilized in determining the probability of a sequence, but we will now solidify that with an actual example. Here is the problem statement:\n",
    "\n",
    "> There is a magician who has two biased coins, _coin 1_ and _coin 2_ that he is flipping. We are trying to determine the probability of observing the following sequence of coin flips:<br>\n",
    "<br>\n",
    "$$p\\big(HHT \\big)$$<br>\n",
    "$$p\\big( x(1)=H, x(2)=H, x(3)=T \\big)$$<br>\n",
    "To start, we know that since the magician has two coins, the number of hidden states, $M$, is 2. We also know that we can either observe a flipped coin being heads or tails, so the number of possible observed symbols is also 2. We are told that the magician really likes coin 1, so the initial state distribution is:<br>\n",
    "<img src=\"images/initial-1.png\" width=\"150\">\n",
    "<br>\n",
    "He is also very figity, and tends to switch between coins very often, so his state transition matrix is:<br>\n",
    "<br>\n",
    "<img src=\"images/state-transition-1.png\" width=\"200\">\n",
    "<br>\n",
    "Finally, the coin 1 is biased towards heads and has a 0.7 probability of being heads, and a 0.3 probability of being tails. Coin 2 is biased towards tails, and has a 0.6 probability of being tails, and a 0.4 probability of being heads. Hence, the observation matrix looks like:<br>\n",
    "<img src=\"images/observation-1.png\" width=\"200\">\n",
    "<br>\n",
    "\n",
    "We now have all of the information needed to find the probability of the sequence $H,H,T$. Intuitively, we can think about it as follows: We must first take into account the probability that we start in specific state, and from that state we observed heads. We then must account for the probability that from that state we transition to another hidden state and observe heads. And finally, we must take into the account that from the second hidden state we transition to _another_ hidden state and observe tails. Let's look at each part separately.\n",
    "\n",
    "#### 1.1 The probability of the Initial State\n",
    "We can write the probability of the initial state and observing heads as:\n",
    "\n",
    "#### $$\\pi\\big(z(1)\\big)p\\big(x(1)=1|z(1)\\big)$$\n",
    "\n",
    "#### 1.2 The probability of transitioning from state 1 to state 2\n",
    "\n",
    "#### $$p\\big(z(2)|z(1)\\big) = A\\big(z(1),z(2)\\big)$$\n",
    "\n",
    "#### 1.3 The probability of observing heads from state 2\n",
    "\n",
    "#### $$p\\big(x(2)=1|z(2)\\big) = B\\big(z(2),x(2)=1\\big)$$\n",
    "\n",
    "Now, if we repeated this process for transitioning from state 2 to 3, we would end up with the following equation:\n",
    "\n",
    "#### $$p\\big(x,z \\big) = \\pi\\big(z(1)\\big)p\\big(x(1)=1|z(1)\\big) * p\\big(z(2)|z(1)\\big) * p\\big(x(2)=1|z(2)\\big) * p\\big(z(3)|z(2)\\big) * p\\big(x(3)=0|z(3)\\big)$$\n",
    "\n",
    "Which we can then update by utilizing our matrices $A$ and $B$:\n",
    "\n",
    "#### $$p\\big(x,z \\big) = \\pi\\big(z(1)\\big)B\\big(z(2),x(2)=1\\big) * A\\big(z(1),z(2)\\big) * B\\big(z(2),x(2)=1\\big) * A\\big(z(2),z(3)\\big) * B\\big(z(3),x(3)=0\\big)$$\n",
    "\n",
    "Now, at this point we can see that there are multiple values that $z$ can take on, and that in order to find $p(x)$ we must marginalize out $z$, like so:\n",
    "\n",
    "#### $$\\sum_{z_1 = 1..M,..,z_3=1..M}\\pi\\big(z(1)\\big)B\\big(z(2),x(2)=1\\big) * A\\big(z(1),z(2)\\big) * B\\big(z(2),x(2)=1\\big) * A\\big(z(2),z(3)\\big) * B\\big(z(3),x(3)=0\\big)$$\n",
    "\n",
    "We can see that since $M$ is 2, and we have $T =3$ observations, there are going to be $2^3$ different operations (separate summations) that need to be performed in order to marginalize out $z$. That would be very messy to write out, be we will calculate it in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Probability:  0.11169000000000001\n",
      "Total Operations:  40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial probability distribution and transition matrices\n",
    "pi = np.array([0.8, 0.2])\n",
    "A = np.array([[0.1, 0.9],[0.9, 0.1]])\n",
    "B = np.array([[0.7, 0.3],[0.4, 0.6]])\n",
    "x = np.array([0,0,1]) # 0 for heads, 1 for tails, based on numpy indexing\n",
    "operations_per_iteration = 5\n",
    "\n",
    "# Function to calculate the probability of the observed sequence for a given z1, z2, z3\n",
    "def sequence_probability_with_z(z1, z2, z3):\n",
    "  return pi[z1]*B[z2, x[0]] * A[z1, z2]*B[z2, x[1]] * A[z2, z3]*B[z3, x[2]]\n",
    "\n",
    "# Initial marginalized sequence probability and number of iterations performed\n",
    "marginalized_sequence_prob = 0\n",
    "iterations = 0\n",
    "\n",
    "# Calculate marginalized sequence probability: p(x1, x2, x3) -> p(H, H, T)\n",
    "Z = [0,1]\n",
    "for z1 in Z:\n",
    "  for z2 in Z:\n",
    "    for z3 in Z:\n",
    "      iterations += 1 \n",
    "      marginalized_sequence_prob += sequence_probability_with_z(z1, z2, z3)\n",
    "\n",
    "print('Sequence Probability: ', marginalized_sequence_prob)\n",
    "print('Total Operations: ', iterations * operations_per_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally arrive at our sequence probability:\n",
    "\n",
    "$$p(H,H,T) = 0.11$$\n",
    "\n",
    "#### 1.4 Complexity\n",
    "Note that in order to achieve this we needed to take the summation of 8 different calculations, consisting of $2T-1$ operations. From this, we know that the time complexity of this algorithm is $O(TM^T)$. Visually, this can be seen clearly below:\n",
    "\n",
    "<img src=\"images/computational-complexity-baseline.png\" width=\"750\">\n",
    "\n",
    "We can see via simply counting operations in the above visualization, that there are 5 internal operations that must be performed. In our case, $T=3$, and hence:\n",
    "\n",
    "$$2T-1 = 5$$\n",
    "\n",
    "The summation goes through all of our states at each time step, and since $M=2$, we have:\n",
    "\n",
    "$$M^T = 2 ^ 3 = 2*2*2 = 8$$ \n",
    "\n",
    "Hence, our total number of operations for our example is: \n",
    "\n",
    "$$2T-1*M^T = 5 * 8 = 40$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 2. Forward-Backward Algorithm\n",
    "At this point we are ready to move on to an algorithm that will help reduce the exponential complexity that we are dealing with above. This algorithm, _**the Forward-Backward**_ algorithm, works by defining a variable called $\\alpha$ that represents the joint probability of seeing the sequence you have observed up until now and being in a specific state at that time:\n",
    "\n",
    "$$\\alpha(t,i) = p \\big(x(1),...,x(t),z(t)=i\\big)$$\n",
    "\n",
    "We can see that $\\alpha$ is indexed by both time and $i$, the state.\n",
    "\n",
    "#### 2.1.1 Step 1 $\\rightarrow$ Initial Value of $\\alpha$\n",
    "The first step of the forward backward algorithm is to calculate the initial value of $\\alpha$ at $t=1$:\n",
    "\n",
    "$$\\alpha(1, i) = p \\big(x(1), z(1)=i\\big)$$\n",
    "\n",
    "$$\\alpha(1, i) =  p\\big(z(1) = i \\big) p\\big(x(1) \\mid z(1)= i\\big)$$\n",
    "\n",
    "Where, we know that:\n",
    "\n",
    "$$\\pi_i = p\\big(z(1) = i \\big)$$\n",
    "\n",
    "And also that:\n",
    "\n",
    "$$p\\big(x(1) \\mid z(1)= i\\big) = B\\big(i, x(1)\\big)$$\n",
    "\n",
    "So, we can rewrite our equation for $\\alpha(1,i)$ as:\n",
    "\n",
    "$$\\alpha(1,i) = \\pi_iB\\big(i, x(1)\\big)$$\n",
    "\n",
    "Now, in terms of our example, we know that we have $M=2$ states. Hence, we need to find $\\alpha$ for each starting state (coin 1 and coin 2):\n",
    "\n",
    "$$\\alpha(1,1) = \\alpha(1, \\text{coin 1}) = \\pi_1B\\big(z(1)=1, x(1)\\big)$$\n",
    "\n",
    "$$\\alpha(1,2) = \\alpha(1, \\text{coin 2})= \\pi_2B\\big(z(1)=2, x(1)\\big)$$\n",
    "\n",
    "#### 2.1.2 Key Points\n",
    "The first thing that I want you to take note of, is that that variable, $\\alpha$, will only be updated for _specific values_. Currently it only has _two values_; that for $t=1$ and coin 1, and $t=1$ with coin 2. From an implementation standpoint, $\\alpha$ will be a 2 dimensional matrix:\n",
    "\n",
    "```\n",
    "alpha = np.zeros((T, self.M))\n",
    "```\n",
    "\n",
    "The second key point is that the entire goal of this process is to find $p\\big(x(1)\\big)$. That may not be clear, or you may wonder why we have introduced this new variable $\\alpha$. To answer that, let's first write the equation for $p\\big(x(1)\\big)$:\n",
    "\n",
    "$$p\\big(x(1)\\big) = p\\big(z(1)=1\\big) p\\big(x(1) \\mid z(1) =1 \\big) +...+p\\big(z(1)=M\\big) p\\big(x(1) \\mid z(1) =M \\big)$$\n",
    "\n",
    "$$p\\big(x(1)\\big) = \\pi_1 B \\big(1, x(1) \\big) + \\pi_2 B \\big(2, x(1) \\big)+ ... +\\pi_M B \\big(M, x(1) \\big)$$\n",
    "\n",
    "But wait! We can see that this is our definition for $\\alpha(1,i)$! If we sum $\\alpha$ over the state $i$ when $t=1$, we end up with $p\\big(x(1)\\big)$. This leads us to the critical intuition:\n",
    "\n",
    "> $\\alpha$ is a way to successively keep track of the probability of a sequence.\n",
    "\n",
    "By creating $\\alpha$, we can keep track of the probability of a sequence up to a current point in time, while also taking advantage of certain properties of probability that allow us to reduce the total number of operations we need to perform (aka, we can reduce the time complexity). \n",
    "\n",
    "#### 2.1.3 Step 2 $\\rightarrow$ The Induction Step\n",
    "At that point, we come to the _induction step_. We are trying to find $p\\big(x(1), x(2)\\big)$ now. We know that the induction step is an updating of $\\alpha$ defined as:\n",
    "\n",
    "$$\\alpha(t+1, j) = \\sum_{i=1}^M \\alpha(t,i) A(i,j)B(j, x(t+1))$$\n",
    "\n",
    "Note, if we wanted to find $\\alpha(t+1, j)$ for _all values_ that $j$ can take on, we would simply take the summation with $j$ as the index. With that said, inuitively we can think of the induction step as follows (in relation to our magician example): At $t=1$ the magician in holding one of the coins-we are not sure which-and we observe him flip a heads. We then go to $t=2$, he shuffles the coins behind his back, and then flips one of the coins where we again observe a heads. What we want to know is, what is the probability of that happening?\n",
    "\n",
    "In order to determine that we must consider that the magician could transition from coin 1 or 2 at $t=1$, to coin 1 or 2 at $t=2$, where we then must determine the probability of observing $x(2)=heads$. This can be written as:\n",
    "\n",
    "$$\\pi_1A(1,1)B(1, x(2)=heads)+ \\pi_2A(2,1)B(1, x(2)=heads)+\\pi_1A(1,2)B(2, x(2)=heads)+ \\pi_2A(2,2)B(2, x(2)=heads)$$\n",
    "\n",
    "We can include $B$ for $t=1$ as well:\n",
    "\n",
    "$$\\pi_1B(1, x(1)=heads)A(1,1)B(1, x(2)=heads)+ \\pi_2B(2, x(1)=heads)A(2,1)B(1, x(2)=heads)+\\pi_1B(1, x(1)=heads)A(1,2)B(2, x(2)=heads)+ \\pi_2B(2, x(1)=heads)A(2,2)B(2, x(2)=heads)$$\n",
    "\n",
    "Ahah! This is just $\\alpha$ at time $t=2$! Note, this is technically the _sum_ of $\\alpha$ at $t=2$, over $j$ states. We can represent $\\alpha$ at $t=2$ for each individual state below: \n",
    "\n",
    "$$\\alpha(t=2, j) = \\sum_{i=1}^M \\alpha(t,i) A(i,j)B(j, x(t+1))$$\n",
    "\n",
    "$$\\alpha(t=2, j=1) = \\sum_{i=1}^M \\alpha(t,i) A(i,j=1)B(j=1, x(t+1))$$\n",
    "\n",
    "$$\\alpha(t=2, j=1) = \\pi_1B(1, x(1)=heads)A(1,1)B(1, x(2)=heads)+ \\pi_2B(2, x(1)=heads)A(2,1)B(1, x(2)=heads)$$\n",
    "\n",
    "$$\\alpha(t=2, j=2) = \\sum_{i=1}^M \\alpha(t,i) A(i,j=2)B(j=2, x(t+1))$$\n",
    "\n",
    "$$\\alpha(t=2, j=2) = \\pi_1B(1, x(1)=heads)A(1,2)B(2, x(2)=heads)+ \\pi_2B(2, x(1)=heads)A(2,2)B(2, x(2)=heads)$$\n",
    "\n",
    "Again, in order to find the total probability of the sequence $p\\big(x(1), x(2)\\big)$, we would want to marginalize out the hidden state, in this case indexed by $j$:\n",
    "\n",
    "$$p\\big(x(1), x(2)\\big) = \\sum_{j=1..M} \\alpha(t=2, j)$$\n",
    "\n",
    "However, we do not need to perform that step just yet, since we still have one more observed symbol to consider.\n",
    "\n",
    "#### 2.1.4 Step 2 $\\rightarrow$ The Termination Step\n",
    "We finally reach the point where we are trying to calculate $p\\big(x(1), x(2), x(3)\\big)$. Now, in order to do this we must perform the exact same set of steps that we just performed above, where we calculates $\\alpha(t=3, j)$, for each hidden state $j$. The only difference is that now we must take the summation over $\\alpha(t=3, j)$, for all $j$. This gives us our final sequence probability:\n",
    "\n",
    "$$p\\big(x(1), x(2), x(3)\\big) = \\sum_{j=1..M} \\alpha(t=3, j)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Probability:  0.11865600000000001\n",
      "Total Operations:  19\n"
     ]
    }
   ],
   "source": [
    "# Initial M states, T time steps, and alpha forward variable\n",
    "M = 2\n",
    "T = 3 \n",
    "alpha = np.zeros((T, M))\n",
    "\n",
    "# Initial value step\n",
    "operations_initial = 0\n",
    "for i in range(M):\n",
    "  alpha[0,i] = pi[i]*B[i, x[0]]\n",
    "  operations_initial += 1\n",
    "\n",
    "# Induction Step\n",
    "iterations = 0\n",
    "operations_per_iteration = 2\n",
    "for t in range(1, T):\n",
    "  for j in range(M):\n",
    "    for i in range(M):\n",
    "      alpha[t, j] += alpha[t-1, i]*A[i,j]*B[j,x[t]]\n",
    "      iterations += 1\n",
    "\n",
    "sequence_probability = alpha[-1].sum()\n",
    "print('Sequence Probability: ', sequence_probability)\n",
    "print('Total Operations: ', iterations * operations_per_iteration + operations_initial + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then vectorize the above process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Probability:  0.118656\n"
     ]
    }
   ],
   "source": [
    "# Initialize alpha forward variable\n",
    "alpha = np.zeros((T, M))\n",
    "\n",
    "# Initial Value Step\n",
    "alpha[0] = pi * B[:, x[0]]\n",
    "\n",
    "# Induction Step\n",
    "for t in range(1, T):\n",
    "  alpha[t] = alpha[t-1].dot(A) * B[:, x[t]]\n",
    "  \n",
    "\n",
    "sequence_probability = alpha[-1].sum()\n",
    "print('Sequence Probability: ', sequence_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Complexity\n",
    "Let's quickly determine our overall number of operations for the forward algorithm in our example. In order to find $\\alpha(t=1, 1)$ we need to perform 1 multiplication, and the same goes for $\\alpha(t=1, 2)$. So our initial value step requires two operations.\n",
    "\n",
    "Then, we have our induction step. Here we are trying to find $\\alpha(t=2, 1)$ and $\\alpha(t=2, 2)$. Each requires 4 operations, so a total of 8.\n",
    "\n",
    "We then hit our termination step, which will again require 8 operations, plus an additional operation to take the summation of $\\alpha$ at $t=3$ over $j$. So that is 9 operations. \n",
    "\n",
    "We end up with a total of:\n",
    "\n",
    "$$2 + 8 + 9 = 19 \\text{ operations}$$\n",
    "\n",
    "This matches what we found above in via code. From a big O standpoint, the initial 2 operations are a constant, as is the final 1 summation. That means that we are dealing with the induction steps complexity of $M^2$ at each time step (except the first); so $T-1$ time steps. Again, the $-1$ is a constant and we will be removed, leaving us with a complexity of $O(M^2T)$. Note, following this we would have expected to get $2^2*3 = 12 \\text{ operations}$, yet we came up with 19, why? This is because we were factoring in the number of operations inside of each summation, which is a constant $2$ operations. In other words, no matter how $T$ or $M$ change, it will always be $2$ operations:\n",
    "\n",
    "$$\\alpha(t,i) A(i,j)B(j, x(t+1))$$\n",
    "\n",
    "Where the first is $\\alpha*A$ and the second is $(\\alpha*A) * B$. Because these $2$ operations are a constant, they are dropped from the computational complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# 3. Viterbi Algorithm\n",
    "So far, the question that we have been asking is: _Given a sequence of observed symbols, what is the sequences probability?_ Another question that we are likely to want to ask is:\n",
    "\n",
    "> What is the sequence of hidden states, given the observation?\n",
    "\n",
    "This is where the _**Viterbi Algorithm**_ comes in; it calculates the most probable hidden states sequence, given the observed sequence, under the currenlt model. Now, we will be sure to highlight that the viterbia algorithm works in a very similar manner to the forward algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
